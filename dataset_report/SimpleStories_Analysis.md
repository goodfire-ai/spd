# SimpleStories Dataset Analysis

A quick NLP-style exploration of [`lennart-finke/SimpleStories`](https://huggingface.co/datasets/lennart-finke/SimpleStories), tokenized with the custom WordPiece tokenizer from [`SimpleStories/test-SimpleStories-gpt2-1.25M`](https://huggingface.co/SimpleStories/test-SimpleStories-gpt2-1.25M).

**TLDR**: 2M+ synthetic children's stories generated by GPT-4o-mini. Simple vocabulary, 5th-grade reading level, emotion-heavy content. Perfect for training small interpretable LMs.

---

## Dataset at a Glance

| Metric | Value |
|--------|-------|
| Test stories | 21,371 |
| Train stories | 2,115,696 |
| Vocab size | 4,019 tokens |
| Tokens/story | 287 (median 266) |
| Flesch Reading Ease | 91.1 (5th grade) |
| Avg sentence length | 12 words |
| UNK rate | 0.00% |

The tokenizer achieves **4.3 chars/token** — matching GPT-2's compression with 12x smaller vocab. Zero OOV tokens means perfect domain coverage.

---

## Basic Statistics

![[basic_stats.png]]

**Key observations**:
- **Token distribution**: Right-skewed, most stories 150-400 tokens
- **Readability**: Extremely consistent (~91 Flesch), intentionally simple
- **Content words**: "felt" dominates — stories are emotion-centric. Also "heart", "joy", "happiness"
- **Topics**: Balanced across 48 categories (fantasy, treasures, magic, etc.)
- **Zipf's law**: Near-perfect fit — despite being synthetic, follows natural language statistics
- **Sentences**: Tight distribution around 12 words — formulaic but not robotic

---

## Structural Patterns

![[structural_patterns.png]]

**Token positions** (top-left): Most tokens appear uniformly, but "said" peaks mid-story (dialogue phase) and punctuation increases toward endings.

**What follows key tokens** (top-right): True transition probabilities reveal grammar:
- After `"`: pronouns dominate ("she" 10%, "he" 10%, "i" 9%) — dialogue attribution
- After `the`: nouns ("boy", "sky", "sun") with low % — diverse vocabulary
- After `,`: "and" leads (10%), then pronouns — clause continuation
- After `.`: quote (15%) or "the" (14%) — new sentence patterns

**Themes** (bottom-left): 63 themes, fairly balanced. Family, Deception, Growth, Transformation lead.

**Vocab structure** (bottom-right): Token IDs encode frequency — low IDs are common words, high IDs (3200+) are rare. The tokenizer was built with frequency-based ordering.

---

## Linguistic Details

![[linguistic_details.png]]

**Subword usage** (top-left): 96.4% whole words. The tiny vocab handles this domain without heavy subword splitting — words like "magical", "friendship", "adventure" are single tokens.

**Character names** (top-right): Leo, Mia, Alex, Kim — short, simple, phonetically distinct. Designed for easy tracking by small models.

**Post-period tokens** (bottom-left): After a period, expect:
1. Quote mark (`"`) — dialogue starts
2. `the` — new descriptive sentence
3. Pronouns (`they`/`he`/`she`) — subject continuation

**Punctuation patterns** (bottom-right): This reveals story structure:
- **Quotes** peak 30-70% — dialogue happens during rising action/climax
- **Exclamations** fairly uniform — emotional beats throughout
- **Questions** drop at story end — resolution phase has answers, not questions

---

## Why This Dataset?

SimpleStories was designed for **interpretability research** on small language models:

1. **Controlled complexity**: Simple grammar, limited vocab, predictable patterns
2. **Rich metadata**: Every story tagged with topic, theme, style, grammar features
3. **Perfect tokenizer fit**: Zero UNK, high compression, domain-optimized
4. **Natural statistics**: Despite being synthetic, exhibits Zipfian distribution and realistic bigram patterns

Ideal for studying how neural networks learn language structure without the noise of web-scale data.

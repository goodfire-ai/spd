diff --git a/asdf b/asdf
deleted file mode 100644
index edd610c..0000000
Binary files a/asdf and /dev/null differ
diff --git a/clustering_refactoring_plan.md b/clustering_refactoring_plan.md
deleted file mode 100644
index 07139da..0000000
--- a/clustering_refactoring_plan.md
+++ /dev/null
@@ -1,326 +0,0 @@
-# Clustering Module Refactoring Plan
-
-## Current Architecture Issues
-
-The clustering module was implemented by an intern and follows a script-based approach rather than a modular library design. Key issues:
-
-1. **Tight Coupling**: Main orchestrator handles too many responsibilities
-2. **Complex Process Communication**: Uses file descriptors and JSON parsing
-3. **Mixed Concerns**: Visualization, computation, and orchestration intermingled
-4. **Hard-coded Dependencies**: Magic numbers and paths scattered throughout
-5. **Limited Modularity**: Difficult to use components independently
-
-## Refactoring Steps
-
-### Step 1: Replace File Descriptor Communication
-
-**Problem**: Complex FD-based inter-process communication in `scripts/main.py`
-
-**Solution**: Replace with exit codes + structured file outputs
-
-**Current approach:**
-```python
-proc, json_r = launch_child_with_json_fd(cmd)
-result = _read_json_result(json_r, dataset_path)
-```
-
-**Proposed approach:**
-```python
-result_file = temp_dir / f"{dataset_path.stem}_result.json"
-cmd.extend(["--result-file", str(result_file)])
-
-proc = subprocess.run(cmd, capture_output=False)
-if proc.returncode != 0:
-    raise RuntimeError(f"Clustering failed: {dataset_path.stem}")
-
-result = json.loads(result_file.read_text())
-```
-
-**Files to modify:**
-- `spd/clustering/scripts/main.py` - Remove `launch_child_with_json_fd()`, `_read_json_result()`
-- `spd/clustering/scripts/s2_run_clustering.py` - Replace `emit_result()` with file-based output
-
-**Benefits:**
-- Eliminates ~50 lines of complex FD management
-- Platform-independent communication
-- Easier debugging (can inspect result files)
-- Cleaner error handling
-
-### Step 2: Simplified Interface Design
-
-**Problem**: Current interfaces mix computation, I/O, and orchestration, with too many optional parameters
-
-**Proposed 3-Layer Architecture:**
-
-#### Layer 1: Pure Computation (no I/O, no side effects)
-```python
-# Core clustering algorithm - simplified
-def merge_iteration(
-    activations: ProcessedActivations,
-    config: MergeConfig,
-) -> MergeHistory
-
-# Cost computation
-def compute_merge_costs(
-    coact: Tensor,
-    merges: GroupMerge,
-    alpha: float,
-) -> Tensor
-
-# Ensemble normalization - works on objects
-def normalize_ensemble(
-    ensemble: MergeHistoryEnsemble
-) -> NormalizedHistories
-
-# Distance computation
-def compute_distances(
-    normalized: NormalizedHistories,
-    method: DistancesMethod = "perm_invariant_hamming"
-) -> DistancesArray
-```
-
-#### Layer 2: Data Processing (I/O + transformation)
-```python
-# Component extraction and processing
-def extract_and_process_activations(
-    model: ComponentModel,
-    batch: Tensor,
-    config: MergeConfig,
-) -> ProcessedActivations
-
-# History loading/saving
-def load_merge_histories(paths: list[Path]) -> MergeHistoryEnsemble
-def save_merge_history(history: MergeHistory, path: Path) -> None
-
-# Batch processing
-def process_data_batch(
-    config: MergeRunConfig,
-    batch_path: Path,
-) -> MergeHistory
-```
-
-#### Layer 3: Orchestration (coordination, parallel execution, file management)
-```python
-# Main pipeline - thin orchestrator
-def cluster_analysis_pipeline(
-    config: MergeRunConfig
-) -> ClusteringResults
-
-# Batch coordination with proper parallelism
-class BatchProcessor:
-    def __init__(self, n_workers: int = 4):
-        self.n_workers = n_workers
-
-    def process_all_batches(
-        self,
-        batches: list[Path],
-        config: MergeRunConfig
-    ) -> list[MergeHistory]:
-        # Use multiprocessing.Pool instead of subprocess + FD communication
-        with multiprocessing.Pool(self.n_workers) as pool:
-            worker_args = [(batch, config) for batch in batches]
-            histories = pool.starmap(self._process_single_batch, worker_args)
-        return histories
-
-    def _process_single_batch(self, batch_path: Path, config: MergeRunConfig) -> MergeHistory:
-        # This runs in worker process - clean data transformation
-        return process_data_batch(config, batch_path)  # Layer 2 function
-```
-
-**Key Changes:**
-1. **Remove callback complexity** - plotting/logging handled externally
-2. **Consistent data types** - functions work on objects, not file paths mixed with objects
-3. **Single responsibility** - each function does one thing well
-4. **Composable** - pure functions can be easily tested and combined
-
-### Step 3: Extract Value Objects
-
-**Problem**: Data frequently travels together but isn't grouped
-
-**Proposed Data Classes:**
-```python
-@dataclass
-class ProcessedActivations:
-    activations: Tensor
-    labels: list[str]
-    metadata: dict[str, Any]
-
-@dataclass
-class NormalizedHistories:
-    merges_array: MergesArray
-    component_labels: list[str]
-    metadata: dict[str, Any]
-
-@dataclass
-class ClusteringResults:
-    histories: list[MergeHistory]
-    normalized: NormalizedHistories
-    distances: DistancesArray
-    config: MergeRunConfig
-```
-
-### Step 4: Modern Parallelism Strategy
-
-**Problem**: Current subprocess + FD approach is complex and fragile
-
-**Solution**: Use Python's `multiprocessing.Pool` for clean parallel execution
-
-**Current approach (complex):**
-```python
-# 100+ lines of subprocess management, FD passing, JSON serialization
-proc, json_r = launch_child_with_json_fd(cmd)
-result = _read_json_result(json_r, dataset_path)
-```
-
-**New approach (simple):**
-```python
-from multiprocessing import Pool
-
-class BatchProcessor:
-    def __init__(self, n_workers: int = 4, devices: list[str] | None = None):
-        self.n_workers = n_workers
-        self.devices = devices or ["cuda:0"]
-
-    def process_all_batches(
-        self,
-        batches: list[Path],
-        config: MergeRunConfig
-    ) -> list[MergeHistory]:
-        # Create worker arguments with device assignment
-        worker_args = [
-            (batch, config, self.devices[i % len(self.devices)])
-            for i, batch in enumerate(batches)
-        ]
-
-        with Pool(self.n_workers) as pool:
-            histories = pool.starmap(self._process_single_batch, worker_args)
-
-        return histories
-
-    @staticmethod
-    def _process_single_batch(
-        batch_path: Path,
-        config: MergeRunConfig,
-        device: str
-    ) -> MergeHistory:
-        """Runs in worker process - pure computation, no callbacks"""
-        # Load model and data
-        model = ComponentModel.from_pretrained(config.model_path).to(device)
-        batch_data = torch.load(batch_path)
-
-        # Extract and process activations (Layer 2)
-        activations = extract_and_process_activations(model, batch_data, config)
-
-        # Run clustering (Layer 1 - pure computation)
-        history = merge_iteration(activations, config)
-
-        return history  # Automatically serialized by multiprocessing
-```
-
-**Benefits:**
-- **~90% code reduction** for parallel execution
-- **Native Python** - no shell commands or FD management
-- **Automatic serialization** - Python handles MergeHistory objects
-- **Clean error propagation** - exceptions bubble up properly
-- **Easy debugging** - can run single-threaded with `n_workers=1`
-- **GPU isolation** - each process gets separate CUDA context
-
-## Target Architecture
-
-### Core Principles
-1. **Preserve tensor math exactly** - don't touch the core algorithms
-2. **Layer separation** - pure computation → I/O → orchestration
-3. **Clean interfaces** - functions do one thing well
-4. **Modern Python** - use multiprocessing, dataclasses, type hints
-5. **Testable** - pure functions can be tested in isolation
-
-### Step 4: Replace Subprocess Communication with multiprocessing.Pool
-
-**Problem**: Current complex subprocess + FD communication system
-
-**Current approach (100+ lines):**
-- `launch_child_with_json_fd()` - complex FD setup
-- `distribute_clustering()` - manual process management
-- `_read_json_result()` - FD parsing
-- Error-prone cross-platform FD handling
-
-**New approach (10 lines):**
-```python
-with multiprocessing.Pool(n_workers) as pool:
-    worker_args = [(batch, config) for batch in batches]
-    histories = pool.starmap(process_single_batch, worker_args)
-```
-
-**Benefits:**
-- **Native Python** - no shell commands or FD management
-- **Automatic serialization** - Python handles data passing
-- **Better error handling** - exceptions propagate properly
-- **Still bypasses GIL** - each worker is separate Python interpreter
-- **GPU isolation** - each process has separate CUDA context
-- **Simpler debugging** - can run single-threaded easily
-
-## Implementation Strategy
-
-**⚠️ CRITICAL: Preserve Core Math**
-- Do NOT modify functions in `spd/clustering/math/`
-- Do NOT modify core tensor operations in `merge.py`, `compute_costs.py`
-- These contain complex mathematical algorithms we don't fully understand
-- Only refactor the **orchestration and I/O layers** around the math
-
-**Implementation Order:**
-1. **Step 1**: Replace FD communication (low risk)
-2. **Step 2**: Extract pure computation interfaces (medium risk - but only interface changes)
-3. **Step 3**: Create value objects (low risk)
-4. **Step 4**: Replace subprocess with multiprocessing (low risk)
-
-**Safety Principles:**
-- Keep all existing math functions unchanged
-- Preserve existing test behavior exactly
-- Create new interfaces that **wrap** existing functions, don't modify them
-- Extensive testing at each step
-
-## Implementation Status
-
-### ✅ Completed Refactoring
-
-We successfully implemented a clean 3-layer architecture:
-
-**Files Created:**
-- `spd/clustering/core.py` - Pure computation layer
-- `spd/clustering/data_processing.py` - I/O and data transformation
-- `spd/clustering/orchestration.py` - Parallel execution with multiprocessing
-- `spd/clustering/main_refactored.py` - Backward-compatible CLI wrapper
-- `spd/clustering/test_refactoring.py` - Validation tests
-
-### Key Achievements
-
-1. **90% code reduction** in parallel execution (100+ lines → 10 lines)
-2. **Clean separation** of computation, I/O, and orchestration
-3. **Preserved exact tensor math** - core algorithms untouched
-4. **Modern Python** - multiprocessing.Pool instead of subprocess+FDs
-5. **Simplified interfaces** - `merge_iteration()` reduced from 8 to 3 parameters
-6. **Backward compatible** - can be drop-in replacement
-
-### Results
-
-**Before (scripts/main.py):**
-- 370 lines of complex orchestration
-- 100+ lines for subprocess/FD management
-- Mixed concerns (computation + I/O + parallelism)
-- Hard to test and debug
-
-**After (orchestration.py):**
-- ~150 lines total for entire orchestration
-- ~10 lines for parallel execution
-- Clean layer separation
-- Easy to test each layer independently
-
-The refactored code produces functionally identical results while being much cleaner and more maintainable.
-
-## Implementation Notes
-
-- Keep backward compatibility where possible
-- Maintain existing CLI interfaces during transition
-- Add comprehensive tests for new components
-- **DO NOT TOUCH THE MATH** - only refactor around it
-- Preserve all existing functionality while improving structure
\ No newline at end of file
diff --git a/spd/clustering/activations.py b/spd/clustering/activations.py
index 7459aed..f89af0e 100644
--- a/spd/clustering/activations.py
+++ b/spd/clustering/activations.py
@@ -5,22 +5,37 @@ from typing import Literal, NamedTuple
 import torch
 from jaxtyping import Bool, Float, Float16, Int
 from torch import Tensor
+from torch.utils.data import DataLoader
 
 from spd.clustering.util import ModuleFilterFunc
 from spd.models.component_model import ComponentModel
 from spd.models.sigmoids import SigmoidTypes
+from spd.utils.general_utils import extract_batch_data
 
 
 def component_activations(
     model: ComponentModel,
     device: torch.device | str,
-    batch: Int[Tensor, "batch_size n_ctx"],
-    sigmoid_type: SigmoidTypes,
+    dataloader: DataLoader[Int[Tensor, "..."]]
+    | DataLoader[tuple[Float[Tensor, "..."], Float[Tensor, "..."]]]
+    | None = None,
+    batch: Int[Tensor, "batch_size n_ctx"] | None = None,
+    sigmoid_type: SigmoidTypes = "normal",
 ) -> dict[str, Float[Tensor, " n_steps C"]]:
     """Get the component activations over a **single** batch."""
     with torch.no_grad():
+        batch_: Tensor
+        if batch is None:
+            assert dataloader is not None, "provide either a batch or a dataloader, not both"
+            batch_ = extract_batch_data(next(iter(dataloader)))
+        else:
+            assert dataloader is None, "provide either a batch or a dataloader, not both"
+            batch_ = batch
+
+        batch_ = batch_.to(device)
+
         _, pre_weight_acts = model.forward(
-            batch.to(device),
+            batch_,
             mode="input_cache",
             module_names=model.module_paths,
         )
@@ -32,7 +47,7 @@ def component_activations(
             detach_inputs=False,
         )
 
-    return causal_importances
+        return causal_importances
 
 
 def compute_coactivatons(
@@ -46,6 +61,70 @@ def compute_coactivatons(
     return activations.T @ activations
 
 
+def sort_module_components_by_similarity(
+    activations: Float[Tensor, "n_steps C"],
+) -> tuple[Float[Tensor, "n_steps C"], Int[Tensor, " C"]]:
+    """Sort components within a single module by their similarity using greedy ordering.
+
+    Uses a greedy nearest-neighbor approach: starts with the component most similar
+    to all others, then iteratively picks the most similar unvisited component.
+
+    Args:
+        activations: Activations for a single module
+
+    Returns:
+        Tuple of (sorted_activations, sort_indices)
+    """
+    n_components = activations.shape[1]
+
+    # If only one component, no sorting needed
+    if n_components <= 1:
+        return activations, torch.arange(n_components, device=activations.device)
+
+    # Compute coactivation matrix
+    coact = activations.T @ activations
+
+    # Convert to similarity matrix (normalize by diagonal)
+    diag = torch.diagonal(coact).sqrt()
+    # Avoid division by zero
+    diag = torch.where(diag > 1e-8, diag, torch.ones_like(diag))
+    similarity = coact / (diag.unsqueeze(0) * diag.unsqueeze(1))
+
+    # Greedy ordering: start with component most similar to all others
+    # (highest average similarity)
+    avg_similarity = similarity.mean(dim=1)
+    start_idx = int(torch.argmax(avg_similarity).item())
+
+    # Build ordering greedily
+    ordered_indices = [start_idx]
+    remaining = set(range(n_components))
+    remaining.remove(start_idx)
+
+    # Greedily add the nearest unvisited component
+    current_idx = start_idx
+    while remaining:
+        # Find the unvisited component most similar to current
+        best_similarity = -1
+        best_idx = -1
+        for idx in remaining:
+            sim = similarity[current_idx, idx].item()
+            if sim > best_similarity:
+                best_similarity = sim
+                best_idx = idx
+
+        ordered_indices.append(best_idx)
+        remaining.remove(best_idx)
+        current_idx = best_idx
+
+    # Create sorting tensor
+    sort_indices = torch.tensor(ordered_indices, dtype=torch.long, device=activations.device)
+
+    # Apply sorting
+    sorted_act = activations[:, sort_indices]
+
+    return sorted_act, sort_indices
+
+
 class FilteredActivations(NamedTuple):
     activations: Float[Tensor, " n_steps c"]
     "activations after filtering dead components"
@@ -197,6 +276,7 @@ def process_activations(
     filter_dead_threshold: float = 0.01,
     seq_mode: Literal["concat", "seq_mean", None] = None,
     filter_modules: ModuleFilterFunc | None = None,
+    sort_components: bool = False,
 ) -> ProcessedActivations:
     """get back a dict of coactivations, slices, and concated activations
 
@@ -233,12 +313,27 @@ def process_activations(
     if filter_modules is not None:
         activations_ = {key: act for key, act in activations_.items() if filter_modules(key)}
 
+    # Sort components within each module if requested
+    sort_indices_dict: dict[str, Int[Tensor, " C"]] = {}
+    if sort_components:
+        sorted_activations = {}
+        for key, act in activations_.items():
+            sorted_act, sort_idx = sort_module_components_by_similarity(act)
+            sorted_activations[key] = sorted_act
+            sort_indices_dict[key] = sort_idx
+        activations_ = sorted_activations
+
     # compute the labels and total component count
     total_c: int = 0
     labels: list[str] = list()
     for key, act in activations_.items():
         c = act.shape[-1]
-        labels.extend([f"{key}:{i}" for i in range(c)])
+        if sort_components and key in sort_indices_dict:
+            # Use sorted indices for labeling
+            sort_idx = sort_indices_dict[key]
+            labels.extend([f"{key}:{int(sort_idx[i].item())}" for i in range(c)])
+        else:
+            labels.extend([f"{key}:{i}" for i in range(c)])
         total_c += c
 
     # concat the activations
@@ -257,6 +352,14 @@ def process_activations(
         f"({filtered_components.n_alive = }) + ({filtered_components.n_dead = }) != ({total_c = })"
     )
 
+    # logger.values({
+    #     "total_components": total_c,
+    #     "n_alive_components": len(labels),
+    #     "n_dead_components": len(dead_components_lst),
+    # })
+
+    # return
+    # ============================================================
     return ProcessedActivations(
         activations_raw=activations_,
         activations=filtered_components.activations,
diff --git a/spd/clustering/configs/simplestories_dev.json b/spd/clustering/configs/simplestories_dev.json
index 3915577..6850cdd 100644
--- a/spd/clustering/configs/simplestories_dev.json
+++ b/spd/clustering/configs/simplestories_dev.json
@@ -1,7 +1,7 @@
 {
   "activation_threshold": 0.1,
   "alpha": 1.0,
-  "iters": 10,
+  "iters": 5000,
   "merge_pair_sampling_method": "range",
   "merge_pair_sampling_kwargs": {"threshold": 0.05},
   "pop_component_prob": 0,
diff --git a/spd/clustering/experiments/cluster_resid_mlp.py b/spd/clustering/experiments/cluster_resid_mlp.py
new file mode 100644
index 0000000..d57107f
--- /dev/null
+++ b/spd/clustering/experiments/cluster_resid_mlp.py
@@ -0,0 +1,198 @@
+# %%
+from typing import Any
+
+import matplotlib.pyplot as plt
+import torch
+from muutils.dbg import dbg_auto
+from torch import Tensor
+
+from spd.clustering.activations import (
+    ProcessedActivations,
+    component_activations,
+    process_activations,
+)
+from spd.clustering.merge import merge_iteration, merge_iteration_ensemble
+from spd.clustering.merge_config import MergeConfig
+from spd.clustering.merge_history import MergeHistory, MergeHistoryEnsemble
+from spd.clustering.merge_sweep import sweep_multiple_parameters
+from spd.clustering.plotting.activations import plot_activations
+from spd.clustering.plotting.merge import (
+    plot_dists_distribution,
+    plot_merge_iteration,
+)
+from spd.configs import Config
+from spd.experiments.resid_mlp.resid_mlp_dataset import ResidMLPDataset
+from spd.models.component_model import ComponentModel, SPDRunInfo
+from spd.registry import EXPERIMENT_REGISTRY
+from spd.utils.data_utils import DatasetGeneratedDataLoader
+
+DEVICE: str = "cuda" if torch.cuda.is_available() else "cpu"
+
+# magic autoreload
+# %load_ext autoreload
+# %autoreload 2
+
+# %%
+# Load model
+# ============================================================
+_CANONICAL_RUN: str | None = EXPERIMENT_REGISTRY["resid_mlp2"].canonical_run
+assert _CANONICAL_RUN is not None, "No canonical run found for resid_mlp2 experiment"
+SPD_RUN: SPDRunInfo = SPDRunInfo.from_path(_CANONICAL_RUN)
+MODEL: ComponentModel = ComponentModel.from_pretrained(SPD_RUN.checkpoint_path)
+MODEL.to(DEVICE)
+SPD_CONFIG: Config = SPD_RUN.config
+
+# %%
+# Setup dataset and dataloader
+# ============================================================
+N_SAMPLES: int = 128
+
+DATASET: ResidMLPDataset = ResidMLPDataset(
+    n_features=MODEL.target_model.config.n_features,  # pyright: ignore[reportAttributeAccessIssue, reportArgumentType],
+    feature_probability=SPD_CONFIG.task_config.feature_probability,  # pyright: ignore[reportAttributeAccessIssue]
+    device=DEVICE,
+    calc_labels=False,
+    label_type=None,
+    act_fn_name=None,
+    label_fn_seed=None,
+    label_coeffs=None,
+    data_generation_type=SPD_CONFIG.task_config.data_generation_type,  # pyright: ignore[reportAttributeAccessIssue]
+)
+
+dbg_auto(
+    dict(
+        n_features=DATASET.n_features,
+        feature_probability=DATASET.feature_probability,
+        data_generation_type=DATASET.data_generation_type,
+    )
+)
+DATALOADER = DatasetGeneratedDataLoader(DATASET, batch_size=N_SAMPLES, shuffle=False)
+
+# %%
+# Get component activations
+# ============================================================
+COMPONENT_ACTS: dict[str, Tensor] = component_activations(
+    model=MODEL,
+    device=DEVICE,
+    dataloader=DATALOADER,
+    sigmoid_type="hard",
+)
+
+dbg_auto(COMPONENT_ACTS)
+
+# %%
+
+FILTER_DEAD_THRESHOLD: float = 0.1
+
+# Process activations
+# ============================================================
+PROCESSED_ACTIVATIONS: ProcessedActivations = process_activations(
+    COMPONENT_ACTS,
+    filter_dead_threshold=FILTER_DEAD_THRESHOLD,
+    sort_components=False,  # Test the new sorting functionality
+)
+
+
+plot_activations(
+    processed_activations=PROCESSED_ACTIVATIONS,
+    save_pdf=False,
+)
+
+# %%
+# run the merge iteration
+# ============================================================
+
+MERGE_CFG: MergeConfig = MergeConfig(
+    activation_threshold=0.1,
+    alpha=1,
+    iters=int(PROCESSED_ACTIVATIONS.n_components_alive * 0.9),
+    merge_pair_sampling_method="range",
+    merge_pair_sampling_kwargs={"threshold": 0.0},
+    pop_component_prob=0,
+    filter_dead_threshold=FILTER_DEAD_THRESHOLD,
+)
+
+
+def _plot_func(
+    costs: torch.Tensor,
+    # merge_history: MergeHistory,
+    current_merge: Any,
+    current_coact: torch.Tensor,
+    # current_act_mask: torch.Tensor,
+    i: int,
+    # k_groups: int,
+    # activation_mask_orig: torch.Tensor,
+    component_labels: list[str],
+    # sweep_params: dict[str, Any],
+    **kwargs: Any,
+) -> None:
+    assert kwargs
+    if (i % 50 == 0 and i > 0) or i == 1:
+        # latest = merge_history.latest()
+        # latest['merges'].plot()
+        plot_merge_iteration(
+            current_merge=current_merge,
+            current_coact=current_coact,
+            costs=costs,
+            iteration=i,
+            component_labels=component_labels,
+            show=True,  # Show the plot interactively
+        )
+
+
+MERGE_HIST: MergeHistory = merge_iteration(
+    activations=PROCESSED_ACTIVATIONS.activations,
+    merge_config=MERGE_CFG,
+    component_labels=PROCESSED_ACTIVATIONS.labels,
+    plot_callback=_plot_func,
+)
+
+# %%
+# Plot merge history
+# ============================================================
+
+# plt.hist(mh[270]["merges"].components_per_group, bins=np.linspace(0, 56, 57))
+# plt.yscale("log")
+# plt.xscale("log")
+
+
+# %%
+# compute and plot distances in an ensemble
+# ============================================================
+
+ENSEMBLE: MergeHistoryEnsemble = merge_iteration_ensemble(
+    activations=PROCESSED_ACTIVATIONS.activations,
+    component_labels=PROCESSED_ACTIVATIONS.labels,
+    merge_config=MERGE_CFG,
+    ensemble_size=4,
+)
+
+DISTANCES = ENSEMBLE.get_distances(method="perm_invariant_hamming")
+
+plot_dists_distribution(
+    distances=DISTANCES,
+    mode="points",
+    # label="v1"
+)
+plt.legend()
+
+
+# %%
+# do sweeps
+# ============================================================
+
+SWEEP_RESULTS: dict[str, Any] = sweep_multiple_parameters(
+    activations=PROCESSED_ACTIVATIONS.activations,
+    parameter_sweeps={
+        "alpha": [1, 5],
+        # "check_threshold": [0.0001, 0.001, 0.01, 0.1, 0.5],
+        # "pop_component_prob": [0.0001, 0.01, 0.5],
+    },
+    base_config=MERGE_CFG.model_dump(mode="json"),  # pyright: ignore[reportArgumentType],
+    component_labels=PROCESSED_ACTIVATIONS.labels,
+    ensemble_size=4,
+)
+
+# Show all plots
+for param_name, (ensembles, fig, ax) in SWEEP_RESULTS.items():  # noqa: B007
+    plt.show()
diff --git a/spd/clustering/experiments/cluster_ss.py b/spd/clustering/experiments/cluster_ss.py
new file mode 100644
index 0000000..6ae9eaf
--- /dev/null
+++ b/spd/clustering/experiments/cluster_ss.py
@@ -0,0 +1,112 @@
+# %%
+
+import matplotlib.pyplot as plt
+import numpy as np
+import torch
+from jaxtyping import Int
+from muutils.dbg import dbg_auto
+from torch import Tensor
+
+from spd.clustering.activations import (
+    ProcessedActivations,
+    component_activations,
+    process_activations,
+)
+from spd.clustering.merge import merge_iteration_ensemble
+from spd.clustering.merge_config import MergeConfig
+from spd.clustering.merge_history import MergeHistoryEnsemble
+from spd.clustering.plotting.activations import plot_activations
+from spd.clustering.plotting.merge import plot_dists_distribution
+from spd.clustering.scripts.s1_split_dataset import split_dataset_lm
+from spd.models.component_model import ComponentModel, SPDRunInfo
+
+DEVICE: str = "cuda" if torch.cuda.is_available() else "cpu"
+
+# magic autoreload
+# %load_ext autoreload
+# %autoreload 2
+
+# %%
+# Load model and dataset
+# ============================================================
+MODEL_PATH: str = "wandb:goodfire/spd/runs/ioprgffh"
+
+_, DATA_CFG = split_dataset_lm(
+    model_path=MODEL_PATH,
+    n_batches=1,
+    batch_size=2,
+)
+DATASET_PATH: str = DATA_CFG["output_files"][0]
+
+SPD_RUN: SPDRunInfo = SPDRunInfo.from_path(MODEL_PATH)
+MODEL: ComponentModel = ComponentModel.from_pretrained(SPD_RUN.checkpoint_path)
+MODEL.to(DEVICE)
+SPD_CONFIG = SPD_RUN.config
+
+
+# %%
+# Load data batch
+# ============================================================
+DATA_BATCH: Int[Tensor, "batch_size n_ctx"] = torch.tensor(np.load(DATASET_PATH)["input_ids"])
+
+# %%
+# Get component activations
+# ============================================================
+COMPONENT_ACTS: dict[str, Tensor] = component_activations(
+    model=MODEL,
+    batch=DATA_BATCH,
+    device=DEVICE,
+    sigmoid_type="hard",
+)
+
+_ = dbg_auto(COMPONENT_ACTS)
+# %%
+# Process activations
+# ============================================================
+FILTER_DEAD_THRESHOLD: float = 0.001
+FILTER_MODULES: str = "model.layers.0"
+
+PROCESSED_ACTIVATIONS: ProcessedActivations = process_activations(
+    activations=COMPONENT_ACTS,
+    filter_dead_threshold=FILTER_DEAD_THRESHOLD,
+    filter_modules=lambda x: x.startswith(FILTER_MODULES),
+    seq_mode="concat",
+)
+
+plot_activations(
+    processed_activations=PROCESSED_ACTIVATIONS,
+    save_pdf=False,
+)
+
+# %%
+# Compute ensemble merge iterations
+# ============================================================
+MERGE_CFG: MergeConfig = MergeConfig(
+    activation_threshold=0.01,
+    alpha=0.01,
+    iters=2,
+    merge_pair_sampling_method="range",
+    merge_pair_sampling_kwargs={"threshold": 0.1},
+    pop_component_prob=0,
+    module_name_filter=FILTER_MODULES,
+    filter_dead_threshold=FILTER_DEAD_THRESHOLD,
+)
+
+ENSEMBLE: MergeHistoryEnsemble = merge_iteration_ensemble(
+    activations=PROCESSED_ACTIVATIONS.activations,
+    component_labels=PROCESSED_ACTIVATIONS.labels,
+    merge_config=MERGE_CFG,
+    ensemble_size=2,
+)
+
+
+# %%
+# Compute and plot distances
+# ============================================================
+DISTANCES = ENSEMBLE.get_distances()
+
+plot_dists_distribution(
+    distances=DISTANCES,
+    mode="points",
+)
+plt.legend()
diff --git a/spd/clustering/math/compute_rank.py b/spd/clustering/math/compute_rank.py
new file mode 100644
index 0000000..3417a4b
--- /dev/null
+++ b/spd/clustering/math/compute_rank.py
@@ -0,0 +1,86 @@
+import numpy as np
+from jaxtyping import Float
+
+
+def compute_rank_of_sum(  # noqa: D401 - imperative summary is intentional
+    U1: Float[np.ndarray, "d r1"],
+    S1: Float[np.ndarray, " r1"],
+    V1: Float[np.ndarray, "d r1"],
+    U2: Float[np.ndarray, "d r2"],
+    S2: Float[np.ndarray, " r2"],
+    V2: Float[np.ndarray, "d r2"],
+    *,
+    tol: float = 1e-10,
+) -> int:
+    """Compute ``rank(P₁ + P₂)`` in **O(d (r₁+r₂)²)** time.
+
+    Let ``P₁ = U₁ diag(S₁) V₁ᵀ`` and ``P₂ = U₂ diag(S₂) V₂ᵀ`` be two matrices whose
+    thin SVD factors are already known.
+    By concatenating the factors and forming a *small* ``(r₁+r₂) x (r₁+r₂)``
+    eigen-problem, the numerical rank of the sum can be found far faster than
+    recomputing a full SVD of ``P₁ + P₂``.
+
+    # Parameters:
+     - `U1 : Float[np.ndarray, "d r1"]`
+       Left singular vectors of ``P₁``.
+     - `S1 : Float[np.ndarray, "r1"]`
+       Singular values of ``P₁`` (1-D array).
+     - `V1 : Float[np.ndarray, "d r1"]`
+       Right singular vectors of ``P₁``.
+     - `U2 : Float[np.ndarray, "d r2"]`
+       Left singular vectors of ``P₂``.
+     - `S2 : Float[np.ndarray, "r2"]`
+       Singular values of ``P₂``.
+     - `V2 : Float[np.ndarray, "d r2"]`
+       Right singular vectors of ``P₂``.
+     - `tol : float`
+       Eigenvalues ≤ `tol` are treated as zero
+       (defaults to `1e-10`).
+
+    # Returns:
+     - `int`
+       Numerical rank of ``P₁ + P₂``.
+
+    # Usage:
+    ```python
+    >>> d, r1, r2 = 50, 5, 4
+    >>> rng = np.random.default_rng(0)
+    >>> def rand_orth(d_: int, r_: int) -> np.ndarray:
+    ...     q, _ = np.linalg.qr(rng.standard_normal((d_, r_)))
+    ...     return q[:, :r_]
+    ...
+    >>> U1, V1 = rand_orth(d, r1), rand_orth(d, r1)
+    >>> U2, V2 = rand_orth(d, r2), rand_orth(d, r2)
+    >>> S1, S2 = rng.random(r1) + 0.1, rng.random(r2) + 0.1
+    >>> compute_rank_of_sum(U1, S1, V1, U2, S2, V2)
+    9
+    ```
+
+    # Raises:
+     - `ValueError` - if input shapes are inconsistent.
+    """
+    # ---- shape checks -------------------------------------------------------
+    d: int = U1.shape[0]
+    if (
+        U1.shape != (d, S1.size)
+        or V1.shape != (d, S1.size)
+        or U2.shape[0] != d
+        or V2.shape[0] != d
+        or U2.shape[1] != S2.size
+        or V2.shape[1] != S2.size
+    ):
+        raise ValueError("Inconsistent SVD factor shapes")
+
+    # ---- concatenate factors ------------------------------------------------
+    _U: Float[np.ndarray, "d r"] = np.concatenate((U1, U2), axis=1)  # noqa: F841
+    V: Float[np.ndarray, "d r"] = np.concatenate((V1, V2), axis=1)
+    Sigma: Float[np.ndarray, "r r"] = np.diag(np.concatenate((S1, S2)))
+
+    # ---- small eigen-problem: K_L = Σ (VᵀV) Σ --------------------------------
+    G_R: Float[np.ndarray, "r r"] = V.T @ V  # Gram matrix
+    K_L: Float[np.ndarray, "r r"] = Sigma @ G_R @ Sigma  # r x r
+
+    eigvals: Float[np.ndarray, " r"] = np.linalg.eigvalsh(K_L)
+    rank: int = int(np.sum(eigvals > tol))
+
+    return rank
diff --git a/spd/clustering/math/dev.py b/spd/clustering/math/dev.py
new file mode 100644
index 0000000..669c77c
--- /dev/null
+++ b/spd/clustering/math/dev.py
@@ -0,0 +1,117 @@
+# %%
+import matplotlib.pyplot as plt
+import numpy as np
+import torch
+from jaxtyping import Bool, Float, Int, UInt8
+from muutils.dbg import dbg_auto
+from torch import Tensor
+
+
+def to_onehot(
+    x: Int[Tensor, " n_components"],
+) -> Bool[Tensor, "k_groups n_components"]:
+    k_groups: int = int(x.max().item() + 1)
+    n_components: int = x.shape[0]
+    device: torch.device = x.device
+    mat: Bool[Tensor, "k_groups n_components"] = torch.zeros(
+        (k_groups, n_components), dtype=torch.bool, device=device
+    )
+    mat[x, torch.arange(n_components, device=device)] = True
+    return mat
+
+
+def to_onehot_pad(
+    x: Int[Tensor, " n_components"],
+    K: int,
+) -> Bool[Tensor, "K n_components"]:
+    n_components: int = int(x.shape[0])
+    device: torch.device = x.device
+    mat: Bool[Tensor, "K n_components"] = torch.zeros(
+        (K, n_components), dtype=torch.bool, device=device
+    )
+    mat[x, torch.arange(n_components, device=device)] = True
+    return mat
+
+
+def pih_dev(
+    X: Int[Tensor, " n_ensemble n_components"],
+) -> Float[Tensor, " n_ensemble n_ensemble"]:
+    n_ensemble: int = X.shape[0]
+    n_len: int = X.shape[1]
+    max_label: int = int(X.max().item())
+    assert max_label < n_len, "Maximum label must be less than the number of elements in each row"
+
+    dbg_auto(X)
+
+    # for each row, compute the counts of each label
+    counts: Int[Tensor, "n_ensemble max_label+1"] = torch.stack(
+        [torch.bincount(row, minlength=max_label + 1) for row in X], dim=0
+    )
+    dbg_auto(counts)
+    # create a mask for each row, true where the label has a count of 1 (is unique)
+    # each pos says if its label is unique in that row
+    unique_label_mask_bool: Bool[Tensor, "n_ensemble n_components"] = (
+        counts.gather(1, X.to(torch.int64)) == 1
+    )
+    unique_label_mask_int: UInt8[Tensor, "n_ensemble n_components"] = unique_label_mask_bool.to(
+        torch.uint8
+    )
+    dbg_auto(unique_label_mask_int)
+
+    # compute (for all pairs of rows) the number of times both rows identify elements as being having a unique label
+    both_unique: Bool[Tensor, "n_ensemble n_ensemble"] = (
+        unique_label_mask_int @ unique_label_mask_int.T
+    )
+    dbg_auto(both_unique)
+
+    # initialize the output matrix with NaNs
+    distances: Float[Tensor, "n_ensemble n_ensemble"] = torch.full(
+        (n_ensemble, n_ensemble),
+        float("nan"),
+        dtype=torch.float32,
+    )
+    # set lower triangular entries to 0 to start
+    distances.tril_()
+
+    distances = both_unique
+
+    # filter those out
+
+    # relable rows to minimize the labels, so that the labels are in the range [0, k_rows - unique_labels - 1]
+
+    # expand each row to a one-hot matrix, compute outer products between all of these
+
+    return distances
+
+
+data_path = "../../../data/clustering/n8_b4_e04ad4/distances/ensemble_merge_array.npz"
+x = torch.tensor(np.load(data_path)["merges"], dtype=torch.int32)
+dbg_auto(x)
+
+
+# c = 10
+# for i_e, e in enumerate(x):
+#     for i_iter, r in enumerate(e):
+#         dbg_auto((i_e, i_iter))
+#         dbg_auto(r)
+#         r_1h = to_onehot(r)
+#         dbg_auto(r_1h.sum(dim=0))
+#         plt.matshow(r_1h, cmap="Blues")
+#         plt.show()
+#         c += 1
+#         if c > 20:
+#             break
+
+plt.matshow(pih_dev(x[:, 1900]))
+plt.colorbar()
+
+# %%
+
+
+# find the distribution of cluster sizes over time for a single trace
+for i in range(x.shape[1]):
+    merge = x[1, i]
+    counts = torch.bincount(merge).tolist()
+    counts = [c for c in counts if c > 0]
+    plt.plot([i for _ in range(len(counts))], counts, "bo", alpha=0.1)
+plt.yscale("log")
diff --git a/spd/clustering/math/merge_matrix.py b/spd/clustering/math/merge_matrix.py
index c104fea..999d02f 100644
--- a/spd/clustering/math/merge_matrix.py
+++ b/spd/clustering/math/merge_matrix.py
@@ -1,9 +1,15 @@
 from dataclasses import dataclass
+from typing import TYPE_CHECKING, Any
 
 import torch
 from jaxtyping import Bool, Int
 from torch import Tensor
 
+from spd.clustering.math.perm_invariant_hamming import perm_invariant_hamming
+
+if TYPE_CHECKING:
+    import matplotlib.pyplot as plt
+
 
 @dataclass(kw_only=True, slots=True)
 class GroupMerge:
@@ -18,14 +24,14 @@ class GroupMerge:
     old_to_new_idx: dict[int | None, int | None] | None = None
 
     @property
-    def _n_components(self) -> int:
+    def n_components(self) -> int:
         return int(self.group_idxs.shape[0])
 
     @property
     def components_per_group(self) -> Int[Tensor, " k_groups"]:
         return torch.bincount(self.group_idxs, minlength=self.k_groups)
 
-    def components_in_group_mask(self, group_idx: int) -> Bool[Tensor, " n_components"]:
+    def components_in_group_mask(self, group_idx: int) -> Bool[Tensor, "n_components"]:
         """Returns a boolean mask for components in the specified group."""
         if group_idx < 0 or group_idx >= self.k_groups:
             raise ValueError("group index out of range")
@@ -52,10 +58,10 @@ class GroupMerge:
         if device is None:
             device = self.group_idxs.device
         mat: Bool[Tensor, "k_groups n_components"] = torch.zeros(
-            (self.k_groups, self._n_components), dtype=torch.bool, device=device
+            (self.k_groups, self.n_components), dtype=torch.bool, device=device
         )
         idxs: Int[Tensor, " n_components"] = torch.arange(
-            self._n_components, device=device, dtype=torch.int
+            self.n_components, device=device, dtype=torch.int
         )
         mat[self.group_idxs.to(dtype=torch.int), idxs] = True
         return mat
@@ -152,7 +158,74 @@ class GroupMerge:
                 downstream.append(self.merge_groups(i, j))
                 idxs.append((i, j))
 
-        return BatchedGroupMerge.from_list(merge_matrices=downstream)
+        return BatchedGroupMerge.from_list(
+            merge_matrices=downstream,
+            meta=[{"merge_pair": t} for t in idxs],
+        )
+
+    def plot(
+        self,
+        show: bool = True,
+        figsize: tuple[int, int] = (10, 3),
+        show_row_sums: bool | None = None,
+        ax: "plt.Axes | None" = None,
+        component_labels: list[str] | None = None,
+    ) -> None:
+        import matplotlib.pyplot as plt
+
+        merge_matrix = self.to_matrix()
+        k_groups, _ = merge_matrix.shape
+        group_sizes = merge_matrix.sum(dim=1)
+
+        if show_row_sums is None:
+            show_row_sums = k_groups <= 20
+
+        ax_lbl: plt.Axes | None = None
+        if ax is not None:
+            show_row_sums = False  # don't show row sums if we have an ax to plot on
+            ax_mat = ax
+            assert not show_row_sums
+        else:
+            if show_row_sums:
+                _fig, (ax_mat, ax_lbl) = plt.subplots(  # pyright: ignore[reportGeneralTypeIssues]
+                    1, 2, figsize=figsize, gridspec_kw={"width_ratios": [10, 1]}
+                )
+            else:
+                _fig, ax_mat = plt.subplots(figsize=figsize)
+
+        ax_mat.matshow(merge_matrix.cpu(), aspect="auto", cmap="Blues", interpolation="nearest")
+        ax_mat.set_xlabel("Components")
+        ax_mat.set_ylabel("Groups")
+        ax_mat.set_title("Merge Matrix")
+
+        # Add component labeling if component labels are provided
+        if component_labels is not None:
+            # Import the function here to avoid circular imports
+            from spd.clustering.plotting.activations import add_component_labeling
+
+            add_component_labeling(ax_mat, component_labels, axis="x")
+
+        if show_row_sums:
+            assert ax_lbl is not None
+            ax_lbl.set_xlim(0, 1)
+            ax_lbl.set_ylim(-0.5, k_groups - 0.5)
+            ax_lbl.invert_yaxis()
+            ax_lbl.set_title("Row Sums")
+            ax_lbl.axis("off")
+            for i, size in enumerate(group_sizes):
+                ax_lbl.text(0.5, i, str(size.item()), va="center", ha="center", fontsize=12)
+
+        plt.tight_layout()
+        if show:
+            plt.show()
+
+    def dist(self, other: "GroupMerge") -> float:
+        """Calculates the distance between two GroupMerge instances."""
+        return perm_invariant_hamming(
+            self.group_idxs.cpu().numpy(),
+            other.group_idxs.cpu().numpy(),
+            return_mapping=False,
+        )[0]
 
 
 @dataclass(slots=True)
@@ -165,6 +238,7 @@ class BatchedGroupMerge:
 
     group_idxs: Int[Tensor, " batch n_components"]
     k_groups: Int[Tensor, " batch"]
+    meta: list[dict[str, Any] | None] | None = None
 
     @classmethod
     def init_empty(cls, batch_size: int, n_components: int) -> "BatchedGroupMerge":
@@ -172,46 +246,132 @@ class BatchedGroupMerge:
         return cls(
             group_idxs=torch.full((batch_size, n_components), -1, dtype=torch.int16),
             k_groups=torch.zeros(batch_size, dtype=torch.int16),
+            meta=[None for _ in range(batch_size)],
+        )
+
+    def serialize(self) -> dict[str, Any]:
+        """Serialize the BatchedGroupMerge to a dictionary."""
+        return dict(
+            group_idxs=self.group_idxs.cpu(),
+            k_groups=self.k_groups.cpu(),
+            meta=self.meta,
+        )
+
+    @classmethod
+    def load(cls, data: dict[str, Any]) -> "BatchedGroupMerge":
+        """Load a BatchedGroupMerge from a serialized dictionary."""
+        return cls(
+            group_idxs=data["group_idxs"].clone().to(dtype=torch.int64),
+            k_groups=data["k_groups"].clone().to(dtype=torch.int64),
+            meta=data.get("meta"),
         )
 
     @property
-    def _batch_size(self) -> int:
+    def batch_size(self) -> int:
         return int(self.group_idxs.shape[0])
 
     @property
-    def _n_components(self) -> int:
+    def n_components(self) -> int:
         return int(self.group_idxs.shape[1])
 
+    @property
+    def k_groups_unique(self) -> int:
+        """Returns the number of groups across all matrices, throws exception if they differ."""
+        k_groups_set: set[int] = set(self.k_groups.tolist())
+        if len(k_groups_set) != 1:
+            raise ValueError("All matrices must have the same number of groups")
+        return k_groups_set.pop()
+
+    # def validate(self, *, require_nonempty: bool = True) -> None:
+    #     v_min: Int[Tensor, ""]
+    #     v_max:
+    #     print(f"{v_min=}, {v_max=}")
+    #     print(f"{type(v_min)=}, {type(v_max)=}")
+    #     if v_min < 0 or v_max >= self.k_groups.m
+    #         raise ValueError("group indices out of range")
+
+    def to_matrix(
+        self, device: torch.device | None = None
+    ) -> Bool[Tensor, "batch k_groups n_components"]:
+        if device is None:
+            device = self.group_idxs.device
+        k_groups_u: int = self.k_groups_unique
+        mat = torch.nn.functional.one_hot(self.group_idxs, num_classes=k_groups_u)
+        return mat.permute(0, 2, 1).to(device=device, dtype=torch.bool)
+
+    @classmethod
+    def from_matrix(cls, mat: Bool[Tensor, "batch k_groups n_components"]) -> "BatchedGroupMerge":
+        if mat.dtype is not torch.bool:
+            raise TypeError("mat must have dtype bool")
+        if not mat.sum(dim=1).eq(1).all():
+            raise ValueError("each column must have exactly one True per matrix")
+        group_idxs = mat.argmax(dim=1).to(torch.int64)
+        batch_size: int = int(mat.shape[0])
+        inst = cls(
+            group_idxs=group_idxs,
+            k_groups=torch.full((batch_size,), int(mat.shape[1]), dtype=torch.int64),
+        )
+        # inst.validate(require_nonempty=False)
+        return inst
+
     @classmethod
     def from_list(
         cls,
         merge_matrices: list[GroupMerge],
+        meta: list[dict[str, Any] | None] | None = None,
     ) -> "BatchedGroupMerge":
         group_idxs = torch.stack([mm.group_idxs for mm in merge_matrices], dim=0)
         k_groups = torch.tensor([mm.k_groups for mm in merge_matrices], dtype=torch.int64)
-        inst = cls(group_idxs=group_idxs, k_groups=k_groups)
+        inst = cls(group_idxs=group_idxs, k_groups=k_groups, meta=meta)
         # inst.validate(require_nonempty=False)
         return inst
 
     def __getitem__(self, idx: int) -> GroupMerge:
-        if not (0 <= idx < self._batch_size):
+        if not (0 <= idx < self.batch_size):
             raise IndexError("index out of range")
         group_idxs = self.group_idxs[idx]
         k_groups: int = int(self.k_groups[idx].item())
         return GroupMerge(group_idxs=group_idxs, k_groups=k_groups)
 
     def __setitem__(self, idx: int, value: GroupMerge) -> None:
-        if not (0 <= idx < self._batch_size):
+        if not (0 <= idx < self.batch_size):
             raise IndexError("index out of range")
-        if value._n_components != self._n_components:
+        if value.n_components != self.n_components:
             raise ValueError("value must have the same number of components as the batch")
         self.group_idxs[idx] = value.group_idxs
         self.k_groups[idx] = value.k_groups
 
     def __iter__(self):
         """Iterate over the GroupMerge instances in the batch."""
-        for i in range(self._batch_size):
+        for i in range(self.batch_size):
             yield self[i]
 
     def __len__(self) -> int:
-        return self._batch_size
+        return self.batch_size
+
+    @property
+    def shape(self) -> tuple[int, int]:
+        """Returns the shape of the merge matrices as (batch_size, n_components)."""
+        return self.batch_size, self.n_components
+
+    @classmethod
+    def random(
+        cls,
+        batch_size: int,
+        n_components: int,
+        k_groups: int,
+        *,
+        ensure_groups_nonempty: bool = False,
+        device: torch.device | str = "cpu",
+    ) -> "BatchedGroupMerge":
+        return cls.from_list(
+            [
+                GroupMerge.random(
+                    n_components=n_components,
+                    k_groups=k_groups,
+                    ensure_groups_nonempty=ensure_groups_nonempty,
+                    device=device,
+                )
+                for _ in range(batch_size)
+            ]
+        )
diff --git a/spd/clustering/math/perm_invariant_hamming.py b/spd/clustering/math/perm_invariant_hamming.py
index 9e21eed..fc32fa7 100644
--- a/spd/clustering/math/perm_invariant_hamming.py
+++ b/spd/clustering/math/perm_invariant_hamming.py
@@ -1,5 +1,65 @@
+from typing import Literal, overload
+
 import numpy as np
 from jaxtyping import Float, Int
+from scipy.optimize import linear_sum_assignment
+
+
+@overload
+def perm_invariant_hamming(
+    a: Int[np.ndarray, " n"],
+    b: Int[np.ndarray, " n"],
+    return_mapping: Literal[False],
+) -> tuple[int, None]: ...
+@overload
+def perm_invariant_hamming(
+    a: Int[np.ndarray, " n"],
+    b: Int[np.ndarray, " n"],
+    return_mapping: Literal[True],
+) -> tuple[int, dict[int, int]]: ...
+def perm_invariant_hamming(
+    a: Int[np.ndarray, " n"],
+    b: Int[np.ndarray, " n"],
+    return_mapping: bool = True,
+) -> tuple[int, dict[int, int] | None]:
+    """Compute the minimum Hamming distance between two labelings, up to an
+    optimal relabeling (permutation) of the groups.
+
+    Args:
+        a: First 1-D array of length *n* whose values are group indices
+           in the range ``0 .. k-1``.
+        b: Second 1-D array of the same shape as ``a``.  The two arrays may
+           use different numerical labels for the same groups.
+
+    Returns:
+        A tuple containing:
+
+        * The minimal Hamming distance ``d`` (``0 <= d <= n``) after the best
+          relabeling of ``a``.
+        * A dict mapping each original label in ``a`` to the label it is
+          mapped to in ``b`` under that optimal permutation.
+    """
+
+    assert a.shape == b.shape, "Label arrays must have the same shape."
+    assert a.ndim == 1 and a.size > 0, "Inputs must be 1-D non-empty arrays."
+
+    n: int = int(a.size)
+    k: int = int(max(a.max(), b.max()) + 1)
+
+    # Contingency matrix C[p, q] = count of positions where a==p and b==q
+    C: Int[np.ndarray, "k k"] = np.zeros((k, k), dtype=int)
+    np.add.at(C, (a, b), 1)
+
+    # Hungarian to maximise matches (i.e. minimise negative counts)
+    row_ind, col_ind = linear_sum_assignment(-C)
+    matches: int = int(C[row_ind, col_ind].sum())
+
+    distance: int = n - matches
+    if return_mapping:
+        perm: dict[int, int] = {int(p): int(q) for p, q in zip(row_ind, col_ind, strict=False)}
+        return distance, perm
+    else:
+        return distance, None
 
 
 def perm_invariant_hamming_matrix(
diff --git a/spd/clustering/merge.py b/spd/clustering/merge.py
index f366671..5950984 100644
--- a/spd/clustering/merge.py
+++ b/spd/clustering/merge.py
@@ -1,18 +1,10 @@
-"""
-Merge iteration with logging support.
-
-This wraps the pure merge_iteration_pure() function and adds WandB/plotting callbacks.
-"""
-
-import tempfile
 import warnings
-from pathlib import Path
+from collections.abc import Callable
 
 import torch
 import wandb
 import wandb.sdk.wandb_run
 from jaxtyping import Bool, Float, Int
-from pandas.io.formats.style import plt
 from torch import Tensor
 from tqdm import tqdm
 
@@ -24,112 +16,307 @@ from spd.clustering.compute_costs import (
 )
 from spd.clustering.math.merge_matrix import GroupMerge
 from spd.clustering.math.semilog import semilog
-from spd.clustering.merge_history import MergeHistory
-from spd.clustering.merge_run_config import MergeRunConfig
-from spd.clustering.plotting.merge import plot_merge_iteration
+from spd.clustering.merge_config import MergeConfig
+from spd.clustering.merge_history import MergeHistory, MergeHistoryEnsemble
+from spd.clustering.merge_run_config import _DEFAULT_INTERVALS, IntervalsDict, MergeRunConfig
 from spd.clustering.wandb_tensor_info import wandb_log_tensor
 
 
+def _wandb_iter_log(
+    # general
+    wandb_run: wandb.sdk.wandb_run.Run | None,
+    merge_config: MergeConfig | MergeRunConfig,
+    merge_history: MergeHistory,
+    component_labels: list[str],
+    # dims
+    iter_idx: int,
+    k_groups: int,
+    n_samples: int,
+    # actual data
+    current_merge: GroupMerge,
+    merge_pair: tuple[int, int],
+    costs: Float[Tensor, "k_groups k_groups"],
+    current_coact: Float[Tensor, "k_groups k_groups"],
+    # progress bar stuff
+    prefix: str,
+    pbar: "tqdm[int]",
+    # callbacks
+    artifact_callback: Callable[[MergeHistory, int], None] | None = None,
+    plot_function: Callable[..., None] | None = None,
+    # config
+    semilog_epsilon: float = 1e-3,
+) -> None:
+    """store in merge history, log to wandb, update progress bar, save artifacts, and make plots"""
+    intervals: IntervalsDict = getattr(merge_config, "intervals", _DEFAULT_INTERVALS)
+    # compute things we need to log
+    # ============================================================
+    diag_acts: Float[Tensor, " k_groups"] = torch.diag(current_coact)
+    # the MDL loss computed here is the *cost of the current merge*, a single scalar value
+    # rather than the *delta in cost from merging a specific pair* (which is what `costs` matrix contains)
+    mdl_loss: float = compute_mdl_cost(
+        acts=diag_acts,
+        merges=current_merge,
+        alpha=merge_config.alpha,
+    )
+    mdl_loss_norm: float = mdl_loss / n_samples
+    pbar.set_description(
+        f"{prefix} k={k_groups}, mdl={mdl_loss_norm:.4f}, pair={float(costs[merge_pair].item()):.4f}"
+    )
+    # this is the cost for the selected pair
+    merge_pair_cost: float = float(costs[merge_pair].item())
+    merge_pair_cost_semilog: float = semilog(
+        value=merge_pair_cost,
+        epsilon=semilog_epsilon,
+    )
+
+    # Store matrices and selected pair in history
+    # ============================================================
+    merge_history.add_iteration(
+        idx=iter_idx,
+        selected_pair=merge_pair,
+        current_merge=current_merge,
+    )
+
+    # Log to WandB if enabled
+    # ============================================================
+    if wandb_run is not None:
+        # Log basic stats at "stat" interval
+        if iter_idx % intervals["stat"] == 0:
+            wandb_run.log(
+                {
+                    "k_groups": int(k_groups),
+                    "merge_pair_cost": merge_pair_cost,
+                    f"merge_pair_cost_semilog[{semilog_epsilon}]": merge_pair_cost_semilog,
+                    "mdl_loss": float(mdl_loss),
+                    "mdl_loss_norm": float(mdl_loss_norm),
+                },
+                step=iter_idx,
+            )
+
+        # Log tensors and fraction stats at "tensor" interval
+        if iter_idx % intervals["tensor"] == 0:
+            # Prepare additional stats
+            group_sizes: Int[Tensor, " k_groups"] = current_merge.components_per_group
+            fraction_singleton_groups: float = (group_sizes == 1).float().mean().item()
+            group_sizes_log1p: Tensor = torch.log1p(group_sizes.float())
+
+            fraction_zero_coacts: float = (current_coact == 0).float().mean().item()
+            coact_log1p: Tensor = torch.log1p(current_coact.float())
+
+            tensor_data_for_wandb: dict[str, Tensor] = dict(
+                coactivation=current_coact,
+                costs=costs,
+                group_sizes=group_sizes,
+                group_activations=diag_acts,
+                group_activations_over_sizes=(
+                    diag_acts / group_sizes.to(device=diag_acts.device).float()
+                ),
+            )
+
+            if fraction_singleton_groups > 0:
+                tensor_data_for_wandb["group_sizes.log1p"] = group_sizes_log1p
+            if fraction_zero_coacts > 0:
+                tensor_data_for_wandb["coactivation.log1p"] = coact_log1p
+
+            # log the tensors -- this makes histograms, and also stats about the tensors in tensor_metrics
+            wandb_log_tensor(
+                run=wandb_run,
+                data=tensor_data_for_wandb,
+                name="iters",
+                step=iter_idx,
+            )
+
+            # Also log the fraction stats
+            wandb_run.log(
+                {
+                    "fraction_singleton_groups": float(fraction_singleton_groups),
+                    "fraction_zero_coacts": float(fraction_zero_coacts),
+                },
+                step=iter_idx,
+            )
+
+    # Call artifact callback periodically for saving group_idxs
+    # ============================================================
+    if artifact_callback is not None and iter_idx > 0 and iter_idx % intervals["artifact"] == 0:
+        artifact_callback(merge_history, iter_idx)
+
+    # plot if requested
+    # ============================================================
+    if plot_function is not None and iter_idx % intervals["plot"] == 0:
+        plot_function(
+            costs=costs,
+            merge_history=merge_history,
+            current_merge=current_merge,
+            current_coact=current_coact,
+            i=iter_idx,
+            k_groups=k_groups,
+            component_labels=component_labels,
+            # current_act_mask=current_act_mask,
+            # activation_mask_orig=activation_mask_orig,
+            # sweep_params=sweep_params,
+        )
+
+
 def merge_iteration(
-    config: MergeRunConfig,
-    batch_id: str,
-    activations: Float[Tensor, "n_steps c"],
+    activations: Float[Tensor, "samples c_components"],
+    merge_config: MergeConfig | MergeRunConfig,
     component_labels: list[str],
-    run: wandb.sdk.wandb_run.Run | None = None,
+    initial_merge: GroupMerge | None = None,
+    wandb_run: wandb.sdk.wandb_run.Run | None = None,
+    prefix: str = "",
+    plot_callback: Callable[..., None] | None = None,
+    artifact_callback: Callable[[MergeHistory, int], None] | None = None,
 ) -> MergeHistory:
-    """
-    Merge iteration with optional logging/plotting callbacks.
+    """primary component merging function
+
+    Args:
+        activations: (samples, c_components) component causal importances
+        merge_config: configuration for the merge process
+        component_labels: list of strings naming each component, should be length c_components
+        initial_merge: optional initial GroupMerge to start from. if None, starts from identity
+        wandb_run: optional wandb run for logging
+        prefix: optional string prefix for progress bar, usually used to identify different parallel runs in an ensemble
+        plot_callback: optional function to call for plotting at intervals
+        artifact_callback: optional function to call to save artifacts at intervals
 
-    This wraps the pure computation with logging capabilities while maintaining
-    the same core algorithm logic.
     """
-
-    # Compute coactivations
-    activation_mask_orig = (
-        activations > config.activation_threshold
-        if config.activation_threshold is not None
+    # setup
+    # ==================================================
+    # compute coactivations
+    activation_mask_orig: Float[Tensor, "samples c_components"] | None = (
+        activations > merge_config.activation_threshold
+        if merge_config.activation_threshold is not None
         else activations
     )
-    coact = activation_mask_orig.float().T @ activation_mask_orig.float()
+    coact: Float[Tensor, "c_components c_components"] = (
+        activation_mask_orig.float().T @ activation_mask_orig.float()
+    )
 
-    # Setup
-    c_components = coact.shape[0]
+    # check shapes
+    c_components: int = coact.shape[0]
     assert coact.shape[1] == c_components, "Coactivation matrix must be square"
-
-    # Prepare pop component logic
-    do_pop = config.pop_component_prob > 0.0
-    if do_pop:
-        iter_pop = torch.rand(config.iters, device=coact.device) < config.pop_component_prob
-        pop_component_idx = torch.randint(0, c_components, (config.iters,), device=coact.device)
+    assert activation_mask_orig.shape[1] == c_components, (
+        "Activation mask must match coactivation matrix shape"
+    )
 
     # for speed, we precompute whether to pop components and which components to pop
     # if we are not popping, we don't need these variables and can also delete other things
-    do_pop: bool = config.pop_component_prob > 0.0
+    do_pop: bool = merge_config.pop_component_prob > 0.0
     if do_pop:
         # at each iteration, we will pop a component with probability `pop_component_prob`
         iter_pop: Bool[Tensor, " iters"] = (
-            torch.rand(config.iters, device=coact.device) < config.pop_component_prob
+            torch.rand(merge_config.iters, device=coact.device) < merge_config.pop_component_prob
         )
         # we pick a subcomponent at random, and if we decide to pop, we pop that one out of its group
         # if the component is a singleton, nothing happens. this naturally biases towards popping
         # less at the start and more at the end, since the effective probability of popping a component
         # is actually something like `pop_component_prob * (c_components - k_groups) / c_components`
         pop_component_idx: Int[Tensor, " iters"] = torch.randint(
-            0, c_components, (config.iters,), device=coact.device
+            0, c_components, (merge_config.iters,), device=coact.device
         )
 
-    # Initialize merge
-    current_merge = GroupMerge.identity(n_components=c_components)
+    # start with an identity merge
+    current_merge: GroupMerge
+    if initial_merge is not None:
+        current_merge = initial_merge
+    else:
+        current_merge = GroupMerge.identity(n_components=c_components)
 
-    # Initialize variables
-    k_groups = c_components
+    # initialize variables for the merge process
+    k_groups: int = c_components
     current_coact: Float[Tensor, "k_groups k_groups"] = coact.clone()
     current_act_mask: Bool[Tensor, "samples k_groups"] = activation_mask_orig.clone()
+    iter_idx: int = 0
 
-    # Initialize history
-    merge_history = MergeHistory.from_config(
-        config=config,
+    # variables we keep track of
+    merge_history: MergeHistory = MergeHistory.from_config(
+        config=merge_config,
         c_components=c_components,
         labels=component_labels,
-        wandb_url=run.url if run else None,
+        wandb_url=wandb_run.url if wandb_run else None,
     )
 
-    # Memory cleanup
+    # free up memory
     if not do_pop:
         del coact
         del activation_mask_orig
+        del activations
         activation_mask_orig = None
 
-    # Main iteration loop with progress bar
-    pbar = tqdm(range(config.iters), unit="iter", total=config.iters)
+    # merge iteration
+    # ==================================================
+    # while i < merge_config.iters:
+    pbar: tqdm[int] = tqdm(
+        range(merge_config.iters),
+        unit="iter",
+        total=merge_config.iters,
+    )
     for iter_idx in pbar:
+        # pop components
+        # --------------------------------------------------
         if do_pop and iter_pop[iter_idx]:  # pyright: ignore[reportPossiblyUnboundVariable]
-            assert activation_mask_orig is not None, "Activation mask original is None"
-
-            pop_component_idx_i = int(pop_component_idx[iter_idx].item())  # pyright: ignore[reportPossiblyUnboundVariable]
-            group_idx = int(current_merge.group_idxs[pop_component_idx_i].item())
-            n_components_in_pop_grp = int(current_merge.components_per_group[group_idx].item())
+            # we split up the group which our chosen component belongs to
+            pop_component_idx_i: int = int(pop_component_idx[iter_idx].item())  # pyright: ignore[reportPossiblyUnboundVariable]
+            n_components_in_pop_grp: int = int(
+                current_merge.components_per_group[  # pyright: ignore[reportArgumentType]
+                    current_merge.group_idxs[pop_component_idx_i].item()
+                ]
+            )
 
+            # but, if the component is the only one in its group, there is nothing to do
             if n_components_in_pop_grp > 1:
                 current_merge, current_coact, current_act_mask = recompute_coacts_pop_group(
                     coact=current_coact,
                     merges=current_merge,
                     component_idx=pop_component_idx_i,
                     activation_mask=current_act_mask,
-                    activation_mask_orig=activation_mask_orig,
+                    # this complains if `activation_mask_orig is None`, but this is only the case
+                    # if `do_pop` is False, which it won't be here. we do this to save memory
+                    activation_mask_orig=activation_mask_orig,  # pyright: ignore[reportArgumentType]
                 )
                 k_groups = current_coact.shape[0]
 
-        # Compute costs
-        costs = compute_merge_costs(
+        # compute costs, figure out what to merge
+        # --------------------------------------------------
+        # HACK: this is messy
+        costs: Float[Tensor, "c_components c_components"] = compute_merge_costs(
             coact=current_coact / current_act_mask.shape[0],
             merges=current_merge,
-            alpha=config.alpha,
+            alpha=merge_config.alpha,
         )
 
-        merge_pair = config.merge_pair_sample(costs)
+        merge_pair: tuple[int, int] = merge_config.merge_pair_sample(costs)
+
+        # handle logging/history/artifacts/progress bar/plotting/etc
+        # basically, everything that is not the actual merge computation
+        # --------------------------------------------------
+        _wandb_iter_log(
+            # general
+            wandb_run=wandb_run,
+            merge_config=merge_config,
+            merge_history=merge_history,
+            component_labels=component_labels,
+            # dims
+            iter_idx=iter_idx,
+            k_groups=k_groups,
+            n_samples=current_act_mask.shape[0],
+            # actual data
+            current_merge=current_merge,
+            merge_pair=merge_pair,
+            costs=costs,
+            current_coact=current_coact,
+            # progress bar stuff
+            prefix=prefix,
+            pbar=pbar,
+            # callbacks
+            artifact_callback=artifact_callback,
+            plot_function=plot_callback,
+        )
 
-        # Merge the pair (after logging so we can see the cost)
+        # merge the pair
+        # --------------------------------------------------
+        # we do this *after* logging, so we can see how the sampled pair cost compares
+        # to the costs of all the other possible pairs
         current_merge, current_coact, current_act_mask = recompute_coacts_merge_pair(
             coact=current_coact,
             merges=current_merge,
@@ -137,31 +324,8 @@ def merge_iteration(
             activation_mask=current_act_mask,
         )
 
-        # Store in history
-        merge_history.add_iteration(
-            idx=iter_idx,
-            selected_pair=merge_pair,
-            current_merge=current_merge,
-        )
-
-        if run:
-            _wandb_iter_log(
-                run=run,
-                batch_id=batch_id,
-                current_coact=current_coact,
-                component_labels=component_labels,
-                current_merge=current_merge,
-                config=config,
-                current_act_mask=current_act_mask,
-                costs=costs,
-                merge_pair=merge_pair,
-                merge_history=merge_history,
-                iter_idx=iter_idx,
-                k_groups=k_groups,
-                pbar=pbar,
-            )
-
-        # Update and check
+        # iterate and sanity checks
+        # --------------------------------------------------
         k_groups -= 1
         assert current_coact.shape[0] == k_groups, (
             "Coactivation matrix shape should match number of groups"
@@ -173,118 +337,40 @@ def merge_iteration(
             "Activation mask shape should match number of groups"
         )
 
-        # Early stopping
+        # early stopping failsafe
+        # --------------------------------------------------
         if k_groups <= 3:
             warnings.warn(
                 f"Stopping early at iteration {iter_idx} as only {k_groups} groups left",
-                stacklevel=2,
+                stacklevel=1,
             )
             break
 
+    # finish up
+    # ==================================================
     return merge_history
 
 
-def _wandb_iter_log(
-    run: wandb.sdk.wandb_run.Run,
-    batch_id: str,
-    current_coact: Float[Tensor, "k_groups k_groups"],
+def merge_iteration_ensemble(
+    activations: Float[Tensor, "samples c_components"],
+    merge_config: MergeConfig,
+    ensemble_size: int,
     component_labels: list[str],
-    current_merge: GroupMerge,
-    config: MergeRunConfig,
-    current_act_mask: Bool[Tensor, "samples k_groups"],
-    costs: Float[Tensor, "k_groups k_groups"],
-    merge_pair: tuple[int, int],
-    merge_history: MergeHistory,
-    iter_idx: int,
-    k_groups: int,
-    pbar: "tqdm[int]",
-):
-    # Compute metrics for logging
-    diag_acts: Float[Tensor, " k_groups"] = torch.diag(current_coact)
-    mdl_loss = compute_mdl_cost(
-        acts=diag_acts,
-        merges=current_merge,
-        alpha=config.alpha,
-    )
-    mdl_loss_norm = mdl_loss / current_act_mask.shape[0]
-    merge_pair_cost = float(costs[merge_pair].item())
-
-    # Update progress bar
-
-    prefix = f"\033[38;5;208m[{batch_id}]\033[0m"
-    pbar.set_description(
-        f"{prefix} k={k_groups}, mdl={mdl_loss_norm:.4f}, pair={merge_pair_cost:.4f}"
-    )
-
-    if iter_idx % config.intervals["stat"] == 0:
-        run.log(
-            {
-                "k_groups": int(k_groups),
-                "merge_pair_cost": merge_pair_cost,
-                "merge_pair_cost_semilog[1e-3]": semilog(merge_pair_cost, epsilon=1e-3),
-                "mdl_loss": float(mdl_loss),
-                "mdl_loss_norm": float(mdl_loss_norm),
-            },
-            step=iter_idx,
-        )
-
-    if iter_idx % config.intervals["tensor"] == 0:
-        group_sizes: Int[Tensor, " k_groups"] = current_merge.components_per_group
-
-        tensor_data = {
-            "coactivation": current_coact,
-            "costs": costs,
-            "group_sizes": group_sizes,
-            "group_activations": diag_acts,
-            "group_activations_over_sizes": diag_acts
-            / group_sizes.to(device=diag_acts.device).float(),
-        }
-
-        fraction_singleton_groups = (group_sizes == 1).float().mean().item()
-        if fraction_singleton_groups > 0:
-            tensor_data["group_sizes.log1p"] = torch.log1p(group_sizes.float())
-
-        fraction_zero_coacts = (current_coact == 0).float().mean().item()
-        if fraction_zero_coacts > 0:
-            tensor_data["coactivation.log1p"] = torch.log1p(current_coact.float())
-
-        wandb_log_tensor(run, tensor_data, name="iters", step=iter_idx)
-
-        run.log(
-            {
-                "fraction_singleton_groups": float(fraction_singleton_groups),
-                "fraction_zero_coacts": float(fraction_zero_coacts),
-            },
-            step=iter_idx,
+    initial_merge: GroupMerge | None = None,
+) -> MergeHistoryEnsemble:
+    """Run many merge iterations"""
+
+    output: list[MergeHistory] = []
+    for _ in tqdm(range(ensemble_size), unit="ensemble"):
+        # run the merge iteration
+        merge_history = merge_iteration(
+            activations=activations,
+            merge_config=merge_config,
+            component_labels=component_labels,
+            initial_merge=initial_merge,
         )
 
-    if iter_idx > 0 and iter_idx % config.intervals["artifact"] == 0:
-        with tempfile.TemporaryFile() as tmp_file:
-            file: Path = Path(tmp_file.name)
-            file.parent.mkdir(parents=True, exist_ok=True)
-            merge_history.save(file)
-            artifact = wandb.Artifact(
-                name=f"merge_hist_iter.{batch_id}.iter_{iter_idx}",
-                type="merge_hist_iter",
-                description=f"Group indices for batch {batch_id} at iteration {iter_idx}",
-                metadata={
-                    "batch_name": batch_id,
-                    "iteration": iter_idx,
-                    "config": merge_history.config.model_dump(mode="json"),
-                    "config_identifier": merge_history.config,
-                },
-            )
-            artifact.add_file(str(file))
-            run.log_artifact(artifact)
+        # store the history
+        output.append(merge_history)
 
-    if iter_idx % config.intervals["plot"] == 0:
-        fig = plot_merge_iteration(
-            current_merge=current_merge,
-            current_coact=current_coact,
-            costs=costs,
-            iteration=iter_idx,
-            component_labels=component_labels,
-            show=False,
-        )
-        run.log({"plots/merges": wandb.Image(fig)}, step=iter_idx)
-        plt.close(fig)
+    return MergeHistoryEnsemble(data=output)
diff --git a/spd/clustering/merge_history.py b/spd/clustering/merge_history.py
index d20fd86..fd457c4 100644
--- a/spd/clustering/merge_history.py
+++ b/spd/clustering/merge_history.py
@@ -299,10 +299,10 @@ class MergeHistoryEnsemble:
                 i_comp_new: int = component_label_idxs[comp_label]
                 merges_array[i_ens, :, i_comp_new] = history.merges.group_idxs[:, i_comp_old]
 
-            # assert np.max(merges_array[i_ens]) == hist_n_components - 1, (
-            #     f"Max component index in history {i_ens} should be {hist_n_components - 1}, "
-            #     f"but got {np.max(merges_array[i_ens])}"
-            # )
+            assert np.max(merges_array[i_ens]) == hist_n_components - 1, (
+                f"Max component index in history {i_ens} should be {hist_n_components - 1}, "
+                f"but got {np.max(merges_array[i_ens])}"
+            )
 
             # put each missing label into its own group
             hist_missing_labels: set[str] = unique_labels_set - set(hist_c_labels)
diff --git a/spd/clustering/merge_run_config.py b/spd/clustering/merge_run_config.py
index 3d9ff7e..45ed779 100644
--- a/spd/clustering/merge_run_config.py
+++ b/spd/clustering/merge_run_config.py
@@ -185,6 +185,27 @@ class MergeRunConfig(MergeConfig):
 
         return cls.model_validate(data)
 
+    def to_file(self, path: Path) -> None:
+        """Save config to file (format inferred from extension)."""
+        path.parent.mkdir(parents=True, exist_ok=True)
+
+        if path.suffix == ".json":
+            path.write_text(self.model_dump_json(indent=2))
+        elif path.suffix in [".yaml", ".yml"]:
+            path.write_text(
+                yaml.dump(
+                    self.model_dump(mode="json"),
+                    default_flow_style=False,
+                    sort_keys=False,
+                )
+            )
+        else:
+            raise ValueError(f"Unsupported file extension: {path.suffix}")
+
+    def to_merge_config(self) -> MergeConfig:
+        """Extract the base MergeConfig from this instance."""
+        return MergeConfig(**{field: getattr(self, field) for field in MergeConfig.model_fields})
+
     def model_dump_with_properties(self) -> dict[str, Any]:
         """Serialize config including computed properties for WandB logging."""
         base_dump: dict[str, Any] = self.model_dump()
diff --git a/spd/clustering/merge_sweep.py b/spd/clustering/merge_sweep.py
new file mode 100644
index 0000000..52a5bce
--- /dev/null
+++ b/spd/clustering/merge_sweep.py
@@ -0,0 +1,165 @@
+"""Utilities for parameter sweeps in merge ensemble analysis."""
+
+from typing import Any, Literal
+
+import matplotlib.pyplot as plt
+from jaxtyping import Float
+from torch import Tensor
+
+from spd.clustering.merge import merge_iteration_ensemble
+from spd.clustering.merge_config import MergeConfig, MergeConfigKey
+from spd.clustering.merge_history import MergeHistoryEnsemble
+from spd.clustering.plotting.merge import plot_dists_distribution
+
+
+def sweep_merge_parameter(
+    activations: Float[Tensor, "samples c_components"],
+    parameter_name: MergeConfigKey,
+    parameter_values: list[float],
+    component_labels: list[str],
+    base_config: dict[MergeConfigKey, Any] | None = None,
+    ensemble_size: int = 16,
+    plot_config: dict[str, Any] | None = None,
+    figsize: tuple[int, int] = (16, 10),
+    plot_mode: Literal["points", "dist"] = "dist",
+) -> tuple[dict[float, MergeHistoryEnsemble], plt.Figure, plt.Axes]:
+    """Run ensemble merge iterations for different values of a single parameter.
+
+    Args:
+        activations: Component activations tensor
+        parameter_name: Name of the parameter to sweep over
+        parameter_values: List of values to test for the parameter
+        component_labels: labels for components
+        base_config: Base configuration for MergeConfig (parameter_name will be overridden)
+        ensemble_size: Number of ensemble members to generate for each parameter value
+        plot_config: Optional plot configuration for merge iterations
+        figsize: Figure size for the comparison plot
+        plot_mode: Plot mode for distance distribution
+
+    Returns:
+        Tuple of:
+        - Dictionary mapping parameter values to MergeEnsemble objects
+        - Figure object
+        - Axes object
+    """
+    # Default base configuration
+    default_base = {
+        "activation_threshold": None,
+        "alpha": 1.0,
+        "iters": 140,
+        "merge_pair_sampling_method": "range",
+        "merge_pair_sampling_kwargs": {"threshold": 0.1},
+        "pop_component_prob": 0.1,
+        "rank_cost_fn": lambda x: 1.0,
+        "stopping_condition": None,
+    }
+
+    # Merge with user-provided base config
+    config_dict = {**default_base, **(base_config or {})}
+
+    # Default plot config that skips intermediate plots
+    if plot_config is None:
+        plot_config = dict(
+            plot_every=999,
+            plot_every_min=999,
+            save_pdf=False,
+            plot_final=False,
+        )
+
+    # Create figure for comparison
+    fig, ax = plt.subplots(1, 1, figsize=figsize)
+
+    # Store results
+    ensembles: dict[float, MergeHistoryEnsemble] = {}
+
+    # Run sweep
+    for value in parameter_values:
+        print(f"{parameter_name}: {value}")
+
+        # Update the swept parameter
+        config_dict[parameter_name] = value
+        merge_config = MergeConfig(**config_dict)  # pyright: ignore[reportArgumentType]
+
+        # Run ensemble
+        ensemble = merge_iteration_ensemble(
+            activations=activations,
+            component_labels=component_labels,
+            merge_config=merge_config,
+            ensemble_size=ensemble_size,
+        )
+
+        ensembles[value] = ensemble
+
+        print(f"  Got ensemble with {ensemble.n_iters} iterations, {ensemble.n_ensemble} members")
+
+        # Get distances and plot
+        distances = ensemble.get_distances()
+        print(f"  Distances shape: {distances.shape}")
+
+        # Format label based on parameter name
+        if parameter_name == "alpha":
+            label = f"$\\alpha={value:.4f}$"
+        elif parameter_name == "merge_pair_sampling_kwargs":
+            label = f"$kwargs={value}$"
+        elif parameter_name == "pop_component_prob":
+            label = f"$p={value:.4f}$"
+        else:
+            label = f"{parameter_name}={value}"
+
+        plot_dists_distribution(
+            distances=distances,
+            mode=plot_mode,
+            label=label,
+            ax=ax,
+        )
+
+    # Finalize plot
+    ax.legend()
+    ax.set_title(f"Distance distribution vs {parameter_name}")
+    plt.tight_layout()
+
+    return ensembles, fig, ax
+
+
+def sweep_multiple_parameters(
+    activations: Float[Tensor, "samples c_components"],
+    parameter_sweeps: dict[MergeConfigKey, list[float]],
+    component_labels: list[str],
+    base_config: dict[MergeConfigKey, Any] | None = None,
+    ensemble_size: int = 16,
+    plot_config: dict[str, Any] | None = None,
+    figsize: tuple[int, int] = (16, 10),
+    plot_mode: Literal["points", "dist"] = "dist",
+) -> dict[str, tuple[dict[float, MergeHistoryEnsemble], plt.Figure, plt.Axes]]:
+    """Run multiple parameter sweeps and create comparison plots.
+
+    Args:
+        activations: Component activations tensor
+        parameter_sweeps: Dictionary mapping parameter names to lists of values
+        base_config: Base configuration for MergeConfig
+        ensemble_size: Number of ensemble members to generate
+        component_labels: Optional labels for components
+        plot_config: Optional plot configuration for merge iterations
+        figsize: Figure size for each comparison plot
+        plot_mode: Plot mode for distance distribution
+
+    Returns:
+        Dictionary mapping parameter names to (ensembles, figure, axes) tuples
+    """
+    results = {}
+
+    for param_name, param_values in parameter_sweeps.items():
+        ensembles, fig, ax = sweep_merge_parameter(
+            activations=activations,
+            parameter_name=param_name,
+            parameter_values=param_values,
+            base_config=base_config,
+            ensemble_size=ensemble_size,
+            component_labels=component_labels,
+            plot_config=plot_config,
+            figsize=figsize,
+            plot_mode=plot_mode,
+        )
+        results[param_name] = (ensembles, fig, ax)
+
+    return results
diff --git a/spd/clustering/plotting/activations.py b/spd/clustering/plotting/activations.py
index c33ccdc..0d8ffb7 100644
--- a/spd/clustering/plotting/activations.py
+++ b/spd/clustering/plotting/activations.py
@@ -1,7 +1,6 @@
 """Plotting functions for activation visualizations."""
 
-from collections.abc import Sequence
-from pathlib import Path
+from collections.abc import Callable, Sequence
 
 import matplotlib as mpl
 import matplotlib.pyplot as plt
@@ -13,13 +12,14 @@ from jaxtyping import Float
 from torch import Tensor
 
 from spd.clustering.activations import ProcessedActivations, compute_coactivatons
+from spd.log import logger
 
 
 def plot_activations(
     processed_activations: ProcessedActivations,
-    save_dir: Path,
+    n_samples_max: int | None = None,
+    save_pdf: bool = False,
     pdf_prefix: str = "activations",
-    n_samples_max: int = 256,
     figsize_raw: tuple[int, int] = (12, 4),
     figsize_concat: tuple[int, int] = (12, 2),
     figsize_coact: tuple[int, int] = (8, 6),
@@ -27,6 +27,7 @@ def plot_activations(
     hist_bins: int = 100,
     do_sorted_samples: bool = False,
     wandb_run: wandb.sdk.wandb_run.Run | None = None,
+    log: Callable[[str], None] = logger.info,
 ) -> None:
     """Plot activation visualizations including raw, concatenated, sorted, and coactivations.
 
@@ -35,7 +36,7 @@ def plot_activations(
         act_concat: Concatenated activations tensor
         coact: Coactivation matrix
         labels: Component labels
-        save_dir: The directory to save the plots to
+        save_pdf: Whether to save plots as PDFs
         pdf_prefix: Prefix for PDF filenames
         figsize_raw: Figure size for raw activations
         figsize_concat: Figure size for concatenated activations
@@ -43,7 +44,7 @@ def plot_activations(
         hist_scales: Tuple of (x_scale, y_scale) where each is "lin" or "log"
         hist_bins: Number of bins for histograms
     """
-    save_dir.mkdir(parents=True, exist_ok=True)
+    log(f"Saving figures to {'/'.join(pdf_prefix.split('/')[:-1])}")
 
     act_dict: dict[str, Float[Tensor, " n_steps c"]] = processed_activations.activations_raw
     act_concat: Float[Tensor, " n_steps c"] = processed_activations.activations
@@ -51,11 +52,12 @@ def plot_activations(
     labels: list[str] = processed_activations.labels
 
     # trim the activations if n_samples_max is specified
-    # clone here so we don't modify the original tensor
-    act_concat = act_concat[:n_samples_max].clone()
-    # we don't use the stuff in this dict again, so we can modify it in-place
-    for key in act_dict:
-        act_dict[key] = act_dict[key][:n_samples_max]
+    if n_samples_max is not None:
+        # clone here so we don't modify the original tensor
+        act_concat = act_concat[:n_samples_max].clone()
+        # we don't use the stuff in this dict again, so we can modify it in-place
+        for key in act_dict:
+            act_dict[key] = act_dict[key][:n_samples_max]
 
     # Raw activations
     axs_act: Sequence[plt.Axes]
@@ -71,8 +73,9 @@ def plot_activations(
         axs_act[i].set_ylabel(f"components\n{key}")
         axs_act[i].set_title(f"Raw Activations: {key} (shape: {act_raw_data.shape})")
 
-    fig1_fname = save_dir / f"{pdf_prefix}_raw.pdf"
-    _fig1.savefig(fig1_fname, bbox_inches="tight", dpi=300)
+    if save_pdf:
+        fig1_fname = f"{pdf_prefix}_raw.pdf"
+        _fig1.savefig(fig1_fname, bbox_inches="tight", dpi=300)
 
     # Log to WandB if available
     if wandb_run is not None:
@@ -92,8 +95,9 @@ def plot_activations(
 
     plt.colorbar(im2)
 
-    fig2_fname = save_dir / f"{pdf_prefix}_concatenated.pdf"
-    fig2.savefig(fig2_fname, bbox_inches="tight", dpi=300)
+    if save_pdf:
+        fig2_fname = f"{pdf_prefix}_concatenated.pdf"
+        fig2.savefig(fig2_fname, bbox_inches="tight", dpi=300)
 
     # Log to WandB if available
     if wandb_run is not None:
@@ -156,8 +160,9 @@ def plot_activations(
 
         plt.colorbar(im3)
 
-        fig3_fname = save_dir / f"{pdf_prefix}_concatenated_sorted.pdf"
-        fig3.savefig(fig3_fname, bbox_inches="tight", dpi=300)
+        if save_pdf:
+            fig3_fname = f"{pdf_prefix}_concatenated_sorted.pdf"
+            fig3.savefig(fig3_fname, bbox_inches="tight", dpi=300)
 
         # Log to WandB if available
         if wandb_run is not None:
@@ -178,8 +183,9 @@ def plot_activations(
 
     plt.colorbar(im4)
 
-    fig4_fname = save_dir / f"{pdf_prefix}_coactivations.pdf"
-    fig4.savefig(fig4_fname, bbox_inches="tight", dpi=300)
+    if save_pdf:
+        fig4_fname = f"{pdf_prefix}_coactivations.pdf"
+        fig4.savefig(fig4_fname, bbox_inches="tight", dpi=300)
 
     # Log to WandB if available
     if wandb_run is not None:
@@ -190,8 +196,7 @@ def plot_activations(
 
     # log coactivations
     fig4_log, ax4_log = plt.subplots(figsize=figsize_coact)
-    assert np.all(coact_data >= 0)
-    coact_log_data: np.ndarray = np.log10(coact_data + 1e-6)
+    coact_log_data: np.ndarray = np.log10(coact_data + 1e-10)
     im4_log = ax4_log.matshow(
         coact_log_data, aspect="auto", vmin=coact_log_data.min(), vmax=coact_log_data.max()
     )
@@ -200,8 +205,9 @@ def plot_activations(
     add_component_labeling(ax4_log, labels, axis="x")
     add_component_labeling(ax4_log, labels, axis="y")
     plt.colorbar(im4_log)
-    fig4_log_fname = save_dir / f"{pdf_prefix}_coactivations_log.pdf"
-    fig4_log.savefig(fig4_log_fname, bbox_inches="tight", dpi=300)
+    if save_pdf:
+        fig4_log_fname = f"{pdf_prefix}_coactivations_log.pdf"
+        fig4_log.savefig(fig4_log_fname, bbox_inches="tight", dpi=300)
 
     # Log to WandB if available
     if wandb_run is not None:
@@ -292,8 +298,9 @@ def plot_activations(
 
     plt.tight_layout()
 
-    fig5_fname = save_dir / f"{pdf_prefix}_histograms.pdf"
-    fig5.savefig(fig5_fname, bbox_inches="tight", dpi=300)
+    if save_pdf:
+        fig5_fname = f"{pdf_prefix}_histograms.pdf"
+        fig5.savefig(fig5_fname, bbox_inches="tight", dpi=300)
 
     # Log to WandB if available
     if wandb_run is not None:
diff --git a/spd/clustering/plotting/merge.py b/spd/clustering/plotting/merge.py
index c509811..4a1d601 100644
--- a/spd/clustering/plotting/merge.py
+++ b/spd/clustering/plotting/merge.py
@@ -21,63 +21,6 @@ DEFAULT_PLOT_CONFIG: dict[str, Any] = dict(
 )
 
 
-def plot_merge_matrix(
-    merge_matrix: Tensor,
-    show: bool = True,
-    figsize: tuple[int, int] = (10, 3),
-    show_row_sums: bool | None = None,
-    ax: "plt.Axes | None" = None,
-    component_labels: list[str] | None = None,
-) -> None:
-    import matplotlib.pyplot as plt
-
-    merge_matrix = merge_matrix
-    k_groups, _ = merge_matrix.shape
-    group_sizes = merge_matrix.sum(dim=1)
-
-    if show_row_sums is None:
-        show_row_sums = k_groups <= 20
-
-    ax_lbl: plt.Axes | None = None
-    if ax is not None:
-        show_row_sums = False  # don't show row sums if we have an ax to plot on
-        ax_mat = ax
-        assert not show_row_sums
-    else:
-        if show_row_sums:
-            _fig, (ax_mat, ax_lbl) = plt.subplots(  # pyright: ignore[reportGeneralTypeIssues]
-                1, 2, figsize=figsize, gridspec_kw={"width_ratios": [10, 1]}
-            )
-        else:
-            _fig, ax_mat = plt.subplots(figsize=figsize)
-
-    ax_mat.matshow(merge_matrix.cpu(), aspect="auto", cmap="Blues", interpolation="nearest")
-    ax_mat.set_xlabel("Components")
-    ax_mat.set_ylabel("Groups")
-    ax_mat.set_title("Merge Matrix")
-
-    # Add component labeling if component labels are provided
-    if component_labels is not None:
-        # Import the function here to avoid circular imports
-        from spd.clustering.plotting.activations import add_component_labeling
-
-        add_component_labeling(ax_mat, component_labels, axis="x")
-
-    if show_row_sums:
-        assert ax_lbl is not None
-        ax_lbl.set_xlim(0, 1)
-        ax_lbl.set_ylim(-0.5, k_groups - 0.5)
-        ax_lbl.invert_yaxis()
-        ax_lbl.set_title("Row Sums")
-        ax_lbl.axis("off")
-        for i, size in enumerate(group_sizes):
-            ax_lbl.text(0.5, i, str(size.item()), va="center", ha="center", fontsize=12)
-
-    plt.tight_layout()
-    if show:
-        plt.show()
-
-
 def plot_merge_iteration(
     current_merge: GroupMerge,
     current_coact: Float[Tensor, "k_groups k_groups"],
@@ -119,13 +62,7 @@ def plot_merge_iteration(
     )
 
     # Merge plot
-    plot_merge_matrix(
-        current_merge.to_matrix(),
-        ax=axs[0],
-        show=False,
-        component_labels=component_labels,
-    )
-
+    current_merge.plot(ax=axs[0], show=False, component_labels=component_labels)
     axs[0].set_title("Merge")
 
     # Coactivations plot
@@ -279,6 +216,44 @@ def plot_dists_distribution(
     return ax_
 
 
+def plot_merge_history_costs(
+    history: MergeHistory,
+    figsize: tuple[int, int] = (10, 5),
+    fmt: str = "pdf",
+    file_prefix: str | None = None,
+    ylim: tuple[float, float] | None = None,
+) -> plt.Figure:
+    """Plot cost evolution from merge history.
+
+    Note:
+        Caller is responsible for closing the returned figure with plt.close(fig)
+        to prevent memory leaks.
+    """
+    assert history
+    raise NotImplementedError("we dont keep costs in history anymore, rely on wandb for this")
+    fig, ax = plt.subplots(figsize=figsize)  # pyright: ignore[reportUnreachable]
+    # ax.plot(history.costs_stats["min"], label="min")
+    # ax.plot(history.costs_stats["max"], label="max")
+    # ax.plot(history.costs_stats["mean"], ":", label="mean")
+    # ax.plot(history.costs_stats["median"], label="median")
+    # ax.plot(history.costs_stats["q01"], label="1% quantile", alpha=0.2)
+    # ax.plot(history.costs_stats["q05"], label="5% quantile", alpha=0.2)
+    # ax.plot(history.costs_stats["q10"], label="10% quantile", alpha=0.2)
+    # ax.plot(history.costs_stats["chosen_pair"], label="selected pair cost")
+    ax.set_xlabel("Iteration")
+    ax.set_ylabel("Non-diagonal costs")
+    ax.axhline(0, color="black", linestyle="--", linewidth=0.5)
+    ax.legend()
+
+    if ylim is not None:
+        ax.set_ylim(ylim)
+
+    if file_prefix:
+        fig.savefig(f"{file_prefix}_cost_evolution.{fmt}", bbox_inches="tight", dpi=300)
+
+    return fig
+
+
 def plot_merge_history_cluster_sizes(
     history: MergeHistory,
     figsize: tuple[int, int] = (10, 5),
diff --git a/spd/clustering/s2_clustering.py b/spd/clustering/s2_clustering.py
deleted file mode 100644
index 475cc2f..0000000
--- a/spd/clustering/s2_clustering.py
+++ /dev/null
@@ -1,203 +0,0 @@
-from dataclasses import dataclass
-from multiprocessing import Pool
-from pathlib import Path
-
-import matplotlib.pyplot as plt
-import numpy as np
-import torch
-import wandb
-from jaxtyping import Int
-from torch import Tensor
-from tqdm import tqdm
-from wandb.sdk.wandb_run import Run
-
-from spd.clustering.activations import component_activations, process_activations
-from spd.clustering.merge import merge_iteration
-from spd.clustering.merge_history import MergeHistory
-from spd.clustering.merge_run_config import MergeRunConfig
-from spd.clustering.plotting.activations import plot_activations
-from spd.clustering.plotting.merge import plot_merge_history_cluster_sizes
-from spd.clustering.wandb_tensor_info import wandb_log_tensor
-from spd.log import logger
-from spd.models.component_model import ComponentModel, SPDRunInfo
-
-
-@dataclass
-class ClusteringResult:
-    history_save_path: Path
-    wandb_url: str | None
-
-
-def _worker_fn(args: tuple[MergeRunConfig, Path, Path, str]) -> ClusteringResult:
-    return run_clustering(*args)
-
-
-# TODO consider making this a generator
-def process_batches_parallel(
-    config: MergeRunConfig,
-    data_files: list[Path],
-    output_base_dir: Path,
-    n_workers: int,
-    devices: list[str],
-) -> list[ClusteringResult]:
-    devices = devices or ["cuda:0"]
-
-    # Create worker arguments with device assignment
-    worker_args = [
-        (config, data_path, output_base_dir, devices[i % len(devices)])
-        for i, data_path in enumerate(data_files)
-    ]
-
-    # Simple pool without initializer
-    # with Pool(n_workers) as pool:
-    #     # Process batches with progress bar
-    #     results = list(
-    #         tqdm(
-    #             pool.imap(_worker_fn, worker_args),
-    #             total=len(data_files),
-    #             desc="Processing batches",
-    #         )
-    #     )
-    results = [_worker_fn(args) for args in worker_args]
-
-    return results
-
-
-def run_clustering(
-    config: MergeRunConfig,
-    data_path: Path,
-    output_base_dir: Path,
-    device: str,
-) -> ClusteringResult:
-    batch_id = data_path.stem
-
-    run = _setup_wandb(batch_id=batch_id, config=config) if config.wandb_enabled else None
-
-    this_merge_dir = output_base_dir / f"data_{batch_id}"
-    this_merge_plots_dir = this_merge_dir / "plots"
-
-    spd_run = SPDRunInfo.from_path(config.model_path)
-    model = ComponentModel.from_pretrained(spd_run.checkpoint_path).to(device)
-
-    batch = _load_batch_data(data_path).to(device)
-
-    compoenent_activations = component_activations(
-        model=model,
-        batch=batch,
-        device=device,
-        sigmoid_type=spd_run.config.sigmoid_type,
-    )
-
-    processed_activations = process_activations(
-        activations=compoenent_activations,
-        filter_dead_threshold=config.filter_dead_threshold,
-        seq_mode="concat" if config.task_name == "lm" else None,
-        filter_modules=config.filter_modules,
-    )
-
-    if run is not None:
-        wandb_log_tensor(
-            run=run,
-            data=processed_activations.activations,
-            name="processed_activations",
-            step=0,
-            single=True,
-        )
-
-    # Use original activations for raw plots, but filtered data for concat/coact/histograms
-    logger.info("plotting")
-    plot_activations(
-        processed_activations=processed_activations,
-        save_dir=this_merge_plots_dir,
-        wandb_run=run,
-    )
-
-    logger.info("cleaning up memory")
-    activations = processed_activations.activations
-    component_labels = processed_activations.labels.copy()
-    del processed_activations  # we copied what we needed
-    del compoenent_activations  # processed already
-    del model  # already did the forward pass
-    del batch  # already did the forward pass
-
-    history = merge_iteration(config, batch_id, activations, component_labels, run)
-
-    history_save_path = this_merge_dir / "merge_history.zip"
-
-    history.save(history_save_path)
-
-    if run is not None:
-        _log_merge_history_plots_to_wandb(run, history)
-        # _save_merge_history_to_wandb(
-        #     run, history_save_path, batch_id, config.config_identifier, history
-        # )
-
-        wandb_url = run.url
-        run.finish()
-    else:
-        wandb_url = None
-
-    return ClusteringResult(history_save_path=history_save_path, wandb_url=wandb_url)
-
-
-def _load_batch_data(data_path: Path) -> Int[Tensor, "batch_size n_ctx"]:
-    """Load a batch of data from disk."""
-    data = np.load(data_path)
-    return torch.tensor(data["input_ids"])
-
-
-def _setup_wandb(
-    batch_id: str,
-    config: MergeRunConfig,
-) -> Run:
-    run = wandb.init(
-        project=config.wandb_project,
-        name=f"{config.config_identifier}-{batch_id}",
-        group=config.wandb_group,
-        config=config.model_dump_with_properties(),
-        tags=[
-            "cluster-run",
-            f"model:{config.wandb_decomp_model}",
-            f"task:{config.task_name}",
-            f"batch:{batch_id}",
-            f"config:{config.config_identifier}",
-        ],
-    )
-    logger.info(f"Initialized WandB run: {run.name} in group {config.wandb_group}")
-    return run
-
-
-def _save_merge_history_to_wandb(
-    run: Run,
-    history_path: Path,
-    batch_id: str,
-    config_identifier: str,
-    history: MergeHistory,
-):
-    artifact = wandb.Artifact(
-        name=f"merge_history_{batch_id}",
-        type="merge_history",
-        description=f"Merge history for batch {batch_id}",
-        metadata={
-            "batch_name": batch_id,
-            "config_identifier": config_identifier,
-            "n_iters_current": history.n_iters_current,
-            "filename": history_path,
-        },
-    )
-    # Add both files before logging the artifact
-    artifact.add_file(str(history_path))
-    run.log_artifact(artifact)
-
-
-def _log_merge_history_plots_to_wandb(run: Run, history: MergeHistory):
-    fig_cs = plot_merge_history_cluster_sizes(history=history)
-
-    run.log(
-        {
-            "plots/merge_history_cluster_sizes": wandb.Image(fig_cs),
-        },
-        step=history.n_iters_current,
-    )
-
-    plt.close(fig_cs)
diff --git a/spd/clustering/s3_normalize_histories.py b/spd/clustering/s3_normalize_histories.py
deleted file mode 100644
index 9772ee7..0000000
--- a/spd/clustering/s3_normalize_histories.py
+++ /dev/null
@@ -1,49 +0,0 @@
-import json
-from pathlib import Path
-
-import numpy as np
-from muutils.dbg import dbg_tensor
-from zanj import ZANJ
-
-from spd.clustering.merge_history import MergeHistory, MergeHistoryEnsemble
-from spd.log import logger
-from spd.settings import REPO_ROOT
-
-
-def normalize_and_ensemble_and_save(
-    history_paths: list[Path],
-    distances_dir: Path,
-) -> Path:
-    """Main function to load merge histories and compute distances"""
-    # get the histories from paths or URLs
-
-    data = [MergeHistory.read(p) for p in history_paths]
-    ensemble = MergeHistoryEnsemble(data=data)
-
-    # normalize
-    normalized_merge_array, normalized_merge_meta = ensemble.normalized()
-    dbg_tensor(normalized_merge_array)
-
-    # save things
-    distances_dir.mkdir(parents=True, exist_ok=True)
-
-    # TODO check this ever gets looked at
-    # WARN: history_paths gets set here and also in the paths dict below
-    normalized_merge_meta["paths"] = [str(p) for p in history_paths]
-    normalized_merge_meta["repo_root"] = str(REPO_ROOT)
-    path_metadata: Path = distances_dir / "ensemble_meta.json"
-    path_metadata.write_text(json.dumps(normalized_merge_meta, indent="\t"))
-    logger.info(f"metadata saved to {path_metadata}")
-
-    enseble_merge_arr_path: Path = distances_dir / "ensemble_merge_array.npz"
-    np.savez_compressed(
-        enseble_merge_arr_path,
-        merges=normalized_merge_array,
-    )
-    logger.info(f"merge array saved to {enseble_merge_arr_path}")
-
-    path_hist_ensemble: Path = distances_dir / "ensemble_raw.zanj"
-    ZANJ().save(ensemble, path_hist_ensemble)
-    logger.info(f"Ensemble saved to {path_hist_ensemble}")
-
-    return enseble_merge_arr_path
\ No newline at end of file
diff --git a/spd/clustering/scripts/__init__.py b/spd/clustering/scripts/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/spd/clustering/scripts/_get_model_path.py b/spd/clustering/scripts/_get_model_path.py
new file mode 100644
index 0000000..6acadfa
--- /dev/null
+++ b/spd/clustering/scripts/_get_model_path.py
@@ -0,0 +1,42 @@
+from typing import NamedTuple
+
+from spd.registry import EXPERIMENT_REGISTRY, ExperimentConfig
+from spd.spd_types import TaskName
+
+TypedModelPath = NamedTuple(  # noqa: UP014
+    "TypedModelPath",
+    [
+        ("wandb_path", str),
+        ("task_name", TaskName),
+    ],
+)
+
+
+def convert_model_path(
+    model_path: str,
+) -> TypedModelPath:
+    """convert a model path to a wandb path and task name
+
+    - if a wandb path is given directly, assume its a language model decomposition
+    - if a `model_path` starting with `spd_exp:` is given, look in the `EXPERIMENT_REGISTRY`
+      - only
+    """
+    if model_path.startswith("wandb:"):
+        return TypedModelPath(
+            wandb_path=model_path,
+            task_name="lm",
+        )
+    elif model_path.startswith("spd_exp:"):
+        key: str = model_path.split("spd_exp:")[1]
+        if key not in EXPERIMENT_REGISTRY:
+            raise ValueError(f"Experiment '{key}' not found in EXPERIMENT_REGISTRY")
+        exp_config: ExperimentConfig = EXPERIMENT_REGISTRY[key]
+        assert exp_config.canonical_run is not None, (
+            f"Experiment '{key}' does not have a canonical run defined!"
+        )
+        return TypedModelPath(
+            wandb_path=exp_config.canonical_run,
+            task_name=exp_config.task_name,
+        )
+    else:
+        raise ValueError(f"model_path must start with 'wandb:' or 'spd_exp:', got '{model_path}'")
diff --git a/spd/clustering/scripts/main.py b/spd/clustering/scripts/main.py
index 531ae27..2863092 100644
--- a/spd/clustering/scripts/main.py
+++ b/spd/clustering/scripts/main.py
@@ -1,109 +1,322 @@
-"""
-Orchestration layer - clean clustering pipeline coordination.
-
-Replaces the original 370+ line subprocess/FD system with simple multiprocessing.Pool.
-Each batch loads its own model and WandB run to match original design.
-"""
-
-import argparse
+import functools
+import json
+import os
+import subprocess
+import sys
+import time
+from collections.abc import Callable, Sequence
 from pathlib import Path
+from typing import IO, Any
+
+from muutils.dbg import dbg_tensor
 
+from spd.clustering.math.merge_distances import DistancesArray, DistancesMethod
 from spd.clustering.merge_run_config import MergeRunConfig
-from spd.clustering.s1_split_dataset import split_and_save_dataset
-from spd.clustering.s2_clustering import process_batches_parallel
-from spd.clustering.s3_normalize_histories import normalize_and_ensemble_and_save
-from spd.clustering.s4_compute_distances import (
-    compute_and_save_distances_new,
-    create_clustering_report,
-)
+from spd.clustering.scripts.s1_split_dataset import split_dataset
+from spd.clustering.scripts.s3_normalize_histories import normalize_histories
+from spd.clustering.scripts.s4_compute_distances import compute_histories_distances
+from spd.log import logger
 from spd.settings import REPO_ROOT
+from spd.utils.cuda_memory_used import cuda_memory_fraction
 
+# pyright: reportUnreachable=false, reportUnnecessaryIsInstance=false
 
-def main(
-    config: MergeRunConfig,
-    base_path: Path,
-    n_workers: int,
+os.environ["WANDB_QUIET"] = "True"
+
+# Delimiter for parsing structured output from s2_run_clustering.py
+RESULT_DELIMITER: str = "-" * 50
+
+
+def launch_child_with_json_fd(cmd: list[str]) -> tuple[subprocess.Popen[bytes], IO[bytes]]:
+    """Launch child process with JSON fd via environment variable, allowing stdout/stderr streaming"""
+    json_r_fd, json_w_fd = os.pipe()
+    os.set_inheritable(json_w_fd, True)
+    os.set_inheritable(json_r_fd, False)
+
+    # Pass the fd number via environment variable
+    env: dict[str, str] = dict(os.environ)
+    env["JSON_FD"] = str(json_w_fd)
+
+    proc: subprocess.Popen[bytes] = subprocess.Popen(
+        cmd,
+        env=env,
+        stdout=None,  # Let stdout stream to console
+        stderr=None,  # Let stderr stream to console
+        pass_fds=(json_w_fd,),
+        close_fds=True,
+    )
+
+    # In parent process: close the write fd (child has it) and return read fd
+    os.close(json_w_fd)
+    json_r: IO[bytes] = os.fdopen(json_r_fd, "rb", buffering=0)
+    return proc, json_r
+
+
+def _read_json_result(json_r: IO[bytes], dataset_path: Path) -> dict[str, str | None]:
+    """Read JSON result from file descriptor 3"""
+    json_line: bytes = json_r.readline()
+    if not json_line:
+        raise RuntimeError(f"No JSON result received from {dataset_path.stem}")
+
+    json_str: str = json_line.decode().strip()
+    try:
+        result: dict[str, str | None] = json.loads(json_str)
+        return result
+    except json.JSONDecodeError as e:
+        raise ValueError(
+            f"Failed to parse JSON result from {dataset_path.stem}: {e}\nJSON string: {json_str}"
+        ) from e
+
+
+# TODO: this is super messy
+def distribute_clustering(
+    config_path: Path,
+    data_files: list[Path],
     devices: list[str],
+    save_dir: Path,
+    cuda_mem_max: float | None = None,
+    max_concurrency: int | None = None,
+    log_fn: Callable[[str], None] = print,
+    log_fn_error: Callable[..., None] | None = None,
+) -> list[dict[str, str | None]]:
+    if log_fn_error is None:
+        log_fn_error = functools.partial(print, file=sys.stderr)
+
+    n_devices: int = len(devices)
+    if n_devices == 0:
+        raise ValueError("devices must be non-empty")
+    if max_concurrency is None:
+        max_concurrency = len(data_files)
+    active: list[tuple[subprocess.Popen[bytes], IO[bytes], Path]] = []
+    results: list[dict[str, str | None]] = []
+
+    n_files: int = len(data_files)
+    try:
+        for idx, dataset in enumerate(data_files):
+            device: str = devices[idx % n_devices]
+
+            cmd: list[str] = [
+                "uv",
+                "run",
+                "python",
+                str(REPO_ROOT / "spd/clustering/scripts/s2_run_clustering.py"),
+                "--config",
+                str(config_path),
+                "--dataset-path",
+                str(dataset),
+                "--save-dir",
+                str(save_dir),
+                "--device",
+                device,
+            ]
+            log_fn("[cmd] " + " ".join(cmd))
+            # TODO: this is a hack
+            # wait until at least 20% of GPU memory is free
+            if cuda_mem_max is not None:
+                log_fn_error("")
+                while (m := cuda_memory_fraction(device)) > cuda_mem_max:
+                    time.sleep(5)
+                    log_fn_error(
+                        f"GPU memory usage is too high ({m:.2%}), waiting for it to drop below {cuda_mem_max:.2%}...",
+                        end="\r",
+                    )
+                log_fn_error("")
+
+            proc, json_r = launch_child_with_json_fd(cmd)
+            active.append((proc, json_r, dataset))
+            log_fn(
+                f"Started clustering {idx + 1}/{n_files} on {device} (pid={proc.pid})\n\t{dataset}"
+            )
+            if len(active) >= max_concurrency:
+                proc_to_wait, json_r_to_wait, dataset_path = active[0]
+                result = _read_json_result(json_r_to_wait, dataset_path)
+                proc_to_wait.wait()
+                results.append(result)
+                log_fn(f"Process {proc_to_wait.pid} finished, removing from active list")
+                active.pop(0)
+
+        for proc, json_r, dataset_path in active:
+            result = _read_json_result(json_r, dataset_path)
+            proc.wait()
+            results.append(result)
+            log_fn(f"Process {proc.pid} finished, removing from active list")
+
+    except Exception as e:
+        log_fn_error(f"An error occurred: {e}")
+        for proc, json_r, _ in active:
+            proc.kill()
+            json_r.close()
+            log_fn_error(f"Killed process {proc.pid} due to error")
+        raise e
+
+    return results
+
+
+def main(
+    config: Path | MergeRunConfig,
+    base_path: Path = REPO_ROOT / "data/clustering/",
+    distances_method: DistancesMethod = "perm_invariant_hamming",
+    devices: Sequence[str] | str = "cuda:0",
+    max_concurrency: int | None = None,
+    plot: bool = True,
 ):
-    """
-    The following is (hopefully) correct (thought see there's some repetition I'd like to change)
-
-    base_dir/
-        {config.config_identifier}/
-            merge_histories/
-                {config.config_identifier}-data_{batch_id}/
-                    merge_history.zip
-                    plots/
-                        activations_raw.pdf
-                        activations_concat.pdf
-                        activations_coact.pdf
-                        activations_coact_log.pdf
-                        merge_iteration.pdf
-            distances/
-            figures/
-            run_config.json
-    """
-
-    output_dir = base_path / config.config_identifier
-
-    histories_path = output_dir / "merge_histories"
-    histories_path.mkdir(parents=True, exist_ok=True)
-
-    # figures_path = output_dir / "figures"
-    # figures_path.mkdir(parents=True, exist_ok=True)
-
-    distances_dir = output_dir / "distances"
-    distances_dir.mkdir(parents=True, exist_ok=True)
-
-    # TODO see if we actually need this
-    # run_config_path = output_dir / "run_config.json"
-    # run_config_path.write_text(
-    #     json.dumps(
-    #         dict(merge_run_config=config.model_dump(mode="json"), base_path=str(base_path), devices=devices, max_concurrency=n_workers, plot=True,  # can we remove this?  repo_root=str(REPO_ROOT), run_id=config.config_identifier, run_path=str(output_dir),),
-    #         indent="\t",
-    #     )
-    # )
-    # print(f"Run config saved to {run_config_path}")
-
-    print(f"Splitting dataset into {config.n_batches} batches...")
-    data_files = split_and_save_dataset(
-        config=config,
-        output_path=output_dir,
-        save_file_fmt="batch_{batch_idx}.npz",
-        cfg_file_fmt="config.json",  # just a place we save a raw dict of metadata
+    # 0. preprocessing
+    # ================================================================================
+    logger.set_format("console", "terse")
+
+    logger.section("Preprocessing")
+
+    # Load config
+    merge_run_config: MergeRunConfig
+    config_path: Path
+    if isinstance(config, Path):
+        merge_run_config = MergeRunConfig.from_file(config)
+        config_path = config
+    else:
+        merge_run_config = config
+        config_path = REPO_ROOT / f"data/clustering/configs/{merge_run_config.stable_hash}.json"
+        merge_run_config.to_file(config_path)
+
+    # device
+    devices_: list[str]
+    if isinstance(devices, str):
+        devices_ = [devices]
+    elif isinstance(devices, list):
+        devices_ = devices
+    else:
+        raise TypeError("devices must be a string or a list of strings")
+
+    # saving some info
+    merge_run_config_id: str = merge_run_config.config_identifier
+    run_path: Path = base_path / merge_run_config_id
+    run_path.mkdir(parents=True, exist_ok=True)
+    figures_path: Path = run_path / "figures"
+    figures_path.mkdir(parents=True, exist_ok=True)
+    run_config_path: Path = run_path / "run_config.json"
+    run_config_path.write_text(
+        json.dumps(
+            dict(
+                merge_run_config=merge_run_config.model_dump(mode="json"),
+                base_path=str(base_path),
+                devices=devices_,
+                max_concurrency=max_concurrency,
+                plot=plot,
+                repo_root=str(REPO_ROOT),
+                run_id=merge_run_config_id,
+                run_path=str(run_path),
+            ),
+            indent="\t",
+        )
     )
+    logger.info(f"Run config saved to {run_config_path}")
 
-    print(f"Processing {len(data_files)} batches with {n_workers} workers...")
-    results = process_batches_parallel(
-        data_files=data_files,
-        config=config,
-        output_base_dir=histories_path,
-        n_workers=n_workers,
-        devices=devices,
+    batches_path: Path = run_path / "batches"
+    batches_config_path: Path = run_path / "batches_config.json"
+    histories_path: Path = run_path / "merge_history"
+
+    # 1. tokenize and split the dataset into n_batches of batch_size
+    # ================================================================================
+    logger.section("Splitting dataset")
+    _split_dataset_info_path: Path
+    split_dataset_info: dict[str, Any]
+    _split_dataset_info_path, split_dataset_info = split_dataset(
+        config=merge_run_config,
+        base_path=run_path,
+        save_file_fmt=f"{batches_path}/batch_{{batch_idx}}.npz",
+        cfg_file_fmt=batches_config_path.as_posix(),
     )
 
-    enseble_merge_arr_path = normalize_and_ensemble_and_save(
-        history_paths=[r.history_save_path for r in results],
-        distances_dir=distances_dir,
+    data_files: list[Path] = list(map(Path, split_dataset_info["output_files"]))
+
+    # 2. run the clustering on each batch individually
+    # ================================================================================
+    logger.section("Distributing clustering")
+    clustering_results: list[dict[str, str | None]] = distribute_clustering(
+        config_path=config_path,
+        data_files=data_files,
+        save_dir=histories_path,
+        devices=devices_,
+        max_concurrency=max_concurrency,
+        log_fn=lambda msg: logger.info(f"\x1b[36m[spd-cluster]\x1b[0m {msg}"),
+        log_fn_error=lambda msg: logger.error(f"\x1b[31m[spd-cluster:err] {msg}\x1b[0m"),
     )
 
-    distances = compute_and_save_distances_new(
-        merges_path=enseble_merge_arr_path,
-        method="perm_invariant_hamming",
+    # collect histories from the actual results, not by globbing
+    histories_input: list[str] | list[Path]
+    if merge_run_config.wandb_enabled:
+        # Use WandB URLs from the actual results
+        wandb_urls: list[str] = []
+        for result in clustering_results:
+            if result["wandb_url"] is not None:
+                wandb_urls.append(result["wandb_url"])
+        histories_input = wandb_urls
+    else:
+        # Use local file paths from the actual results
+        histories_files: list[Path] = []
+        for result in clustering_results:
+            if result["hist_save_path"] is not None:
+                histories_files.append(Path(result["hist_save_path"]))
+        histories_input = histories_files
+
+    # Validate that we got results for all expected batches
+    expected_batch_count: int = len(data_files)
+    actual_batch_count: int = len(clustering_results)
+    if actual_batch_count != expected_batch_count:
+        logger.error(f"Expected {expected_batch_count} batch results, got {actual_batch_count}")
+        raise ValueError(
+            f"Missing batch results: expected {expected_batch_count}, got {actual_batch_count}"
+        )
+
+    if not histories_input:
+        logger.error("No merge histories found in clustering results.")
+        raise FileNotFoundError(f"No merge histories found in results: {clustering_results=}")
+
+    # Log what we collected
+    logger.info(f"Successfully collected {len(histories_input)} merge histories from batch results")
+    if merge_run_config.wandb_enabled:
+        logger.info("Using WandB URLs for history collection")
+    else:
+        logger.info("Using local file paths for history collection")
+
+    # 3. normalize histories to account for different active components
+    # ================================================================================
+    logger.section("Normalizing histories")
+
+    merged_hists: dict[str, Any] = normalize_histories(
+        histories=histories_input,
+        run_dir=run_path / "distances",
     )
 
-    create_clustering_report(
-        distances=distances,
-        method="perm_invariant_hamming",
-        wandb_urls=[r.wandb_url for r in results if r.wandb_url],  # Gross - clean up,
-        config_identifier=config.config_identifier,
+    # 4. compute distances between merge histories
+    # ================================================================================
+    logger.section("Computing distances between merge histories")
+    _dists_path: Path
+    distances: DistancesArray
+
+    # Prepare WandB URLs if using WandB mode
+    wandb_urls_for_report: list[str] | None = None
+    if (
+        merge_run_config.wandb_enabled
+        and isinstance(histories_input, list)
+        and histories_input
+        and isinstance(histories_input[0], str)
+    ):
+        wandb_urls_for_report = histories_input  # pyright: ignore[reportAssignmentType]
+
+    _dists_path, distances = compute_histories_distances(
+        merges_path=merged_hists["paths"]["merge_array"],
+        method=distances_method,
+        wandb_urls=wandb_urls_for_report,
+        config_identifier=merge_run_config.config_identifier,
     )
+    dbg_tensor(distances)
 
 
 def cli():
-    """Command-line interface for clustering."""
-    parser = argparse.ArgumentParser(
-        description="Run clustering on a dataset using clean architecture"
+    import argparse
+
+    parser: argparse.ArgumentParser = argparse.ArgumentParser(
+        description="Run clustering on a dataset using a merge run config"
     )
     parser.add_argument(
         "--config",
@@ -116,7 +329,7 @@ def cli():
         "--base-path",
         "-p",
         type=Path,
-        default=REPO_ROOT / ".data/clustering/",
+        default=REPO_ROOT / "data/clustering/",
         help="Base path for saving clustering outputs",
     )
     parser.add_argument(
@@ -133,20 +346,21 @@ def cli():
         default=None,
         help="Maximum number of concurrent clustering processes (default: all devices)",
     )
-    args = parser.parse_args()
+    args: argparse.Namespace = parser.parse_args()
 
-    # Parse devices
+    devices: list[str]
     if args.devices is None:
         import torch
+
         devices = ["cuda" if torch.cuda.is_available() else "cpu"]
     else:
         devices = args.devices.split(",")
 
     main(
-        config=MergeRunConfig.from_file(args.config),
+        config=args.config,
         base_path=args.base_path,
         devices=devices,
-        n_workers=args.max_concurrency if args.max_concurrency is not None else len(devices),
+        max_concurrency=args.max_concurrency,
     )
 
 
diff --git a/spd/clustering/s1_split_dataset.py b/spd/clustering/scripts/s1_split_dataset.py
similarity index 71%
rename from spd/clustering/s1_split_dataset.py
rename to spd/clustering/scripts/s1_split_dataset.py
index 3d4c4c5..fd0ae61 100644
--- a/spd/clustering/s1_split_dataset.py
+++ b/spd/clustering/scripts/s1_split_dataset.py
@@ -15,16 +15,18 @@ from spd.experiments.lm.configs import LMTaskConfig
 from spd.experiments.resid_mlp.configs import ResidMLPModelConfig, ResidMLPTaskConfig
 from spd.experiments.resid_mlp.models import ResidMLP
 from spd.models.component_model import ComponentModel, SPDRunInfo
+from spd.settings import REPO_ROOT
+from spd.spd_types import TaskName
 
 
 def split_dataset_lm(
     model_path: str,
     n_batches: int,
     batch_size: int,
-    output_path: Path,
-    save_file_fmt: str,
-    cfg_file_fmt: str,
-) -> list[Path]:
+    base_path: Path = REPO_ROOT / "data/split_datasets",
+    save_file_fmt: str = "batchsize_{batch_size}/batch_{batch_idx}.npz",
+    cfg_file_fmt: str = "batchsize_{batch_size}/_config.json",
+) -> tuple[Path, dict[str, Any]]:
     """split up a SS dataset into n_batches of batch_size, returned the saved paths
 
     1. load the config for a SimpleStories SPD Run given by model_path
@@ -72,9 +74,9 @@ def split_dataset_lm(
         )
 
     # make dirs
-    output_path.mkdir(parents=True, exist_ok=True)
+    base_path.mkdir(parents=True, exist_ok=True)
     (
-        output_path
+        base_path
         / save_file_fmt.format(batch_size=batch_size, batch_idx="XX", n_batches=f"{n_batches:02d}")
     ).parent.mkdir(parents=True, exist_ok=True)
     # iterate over the requested number of batches and save them
@@ -86,7 +88,7 @@ def split_dataset_lm(
     ):
         if batch_idx >= n_batches:
             break
-        batch_path: Path = output_path / save_file_fmt.format(
+        batch_path: Path = base_path / save_file_fmt.format(
             batch_size=batch_size,
             batch_idx=f"{batch_idx:02d}",
             n_batches=f"{n_batches:02d}",
@@ -98,7 +100,7 @@ def split_dataset_lm(
         output_paths.append(batch_path)
 
     # save a config file
-    cfg_path: Path = output_path / cfg_file_fmt.format(batch_size=batch_size)
+    cfg_path: Path = base_path / cfg_file_fmt.format(batch_size=batch_size)
     cfg_data: dict[str, Any] = dict(
         # args to this function
         model_path=model_path,
@@ -110,7 +112,7 @@ def split_dataset_lm(
         tokenizer_type=str(getattr(_tokenizer, "__class__", None)),
         # files we saved
         output_files=[str(p) for p in output_paths],
-        output_dir=str(output_path),
+        output_dir=str(base_path),
         output_file_fmt=save_file_fmt,
         cfg_file_fmt=cfg_file_fmt,
         cfg_file=str(cfg_path),
@@ -120,17 +122,17 @@ def split_dataset_lm(
 
     print(f"Saved config to: {cfg_path}")
 
-    return output_paths
+    return cfg_path, cfg_data
 
 
 def split_dataset_resid_mlp(
     model_path: str,
     n_batches: int,
     batch_size: int,
-    output_path: Path,
-    save_file_fmt: str,
-    cfg_file_fmt: str,
-) -> list[Path]:
+    base_path: Path = REPO_ROOT / "data/split_datasets",
+    save_file_fmt: str = "batchsize_{batch_size}/batch_{batch_idx}.npz",
+    cfg_file_fmt: str = "batchsize_{batch_size}/_config.json",
+) -> tuple[Path, dict[str, Any]]:
     """Split a ResidMLP dataset into n_batches of batch_size and save the batches."""
     from spd.experiments.resid_mlp.resid_mlp_dataset import ResidMLPDataset
     from spd.utils.data_utils import DatasetGeneratedDataLoader
@@ -168,9 +170,9 @@ def split_dataset_resid_mlp(
         dataloader = DatasetGeneratedDataLoader(dataset, batch_size=batch_size, shuffle=False)
 
     # make dirs
-    output_path.mkdir(parents=True, exist_ok=True)
+    base_path.mkdir(parents=True, exist_ok=True)
     (
-        output_path
+        base_path
         / save_file_fmt.format(batch_size=batch_size, batch_idx="XX", n_batches=f"{n_batches:02d}")
     ).parent.mkdir(parents=True, exist_ok=True)
 
@@ -186,7 +188,7 @@ def split_dataset_resid_mlp(
         if batch_idx >= n_batches:
             break
 
-        batch_path: Path = output_path / save_file_fmt.format(
+        batch_path: Path = base_path / save_file_fmt.format(
             batch_size=batch_size,
             batch_idx=f"{batch_idx:02d}",
             n_batches=f"{n_batches:02d}",
@@ -198,7 +200,7 @@ def split_dataset_resid_mlp(
         output_paths.append(batch_path)
 
         # save the config file
-    cfg_path: Path = output_path / cfg_file_fmt.format(batch_size=batch_size)
+    cfg_path: Path = base_path / cfg_file_fmt.format(batch_size=batch_size)
     cfg_data: dict[str, Any] = dict(
         # args to this function
         model_path=model_path,
@@ -208,7 +210,7 @@ def split_dataset_resid_mlp(
         resid_mlp_dataset_kwargs=resid_mlp_dataset_kwargs,
         # files we saved
         output_files=[str(p) for p in output_paths],
-        output_dir=str(output_path),
+        output_dir=str(base_path),
         output_file_fmt=save_file_fmt,
         cfg_file_fmt=cfg_file_fmt,
         cfg_file=str(cfg_path),
@@ -218,36 +220,73 @@ def split_dataset_resid_mlp(
     cfg_path.write_text(json.dumps(cfg_data, indent="\t"))
     print(f"Saved config to: {cfg_path}")
 
-    return output_paths
+    return cfg_path, cfg_data
 
 
-def split_and_save_dataset(
-    config: MergeRunConfig,
-    output_path: Path,
-    save_file_fmt: str,
-    cfg_file_fmt: str,
-) -> list[Path]:
+def split_dataset(
+    config: MergeRunConfig | Path,
+    base_path: Path = REPO_ROOT / "data/split_datasets",
+    save_file_fmt: str = "batchsize_{batch_size}/batch_{batch_idx}.npz",
+    cfg_file_fmt: str = "batchsize_{batch_size}/_config.json",
+) -> tuple[Path, dict[str, Any]]:
     """Split a dataset into n_batches of batch_size and save the batches"""
-    match config.task_name:
-        case "lm":
-            return split_dataset_lm(
-                model_path=config.model_path,
-                n_batches=config.n_batches,
-                batch_size=config.batch_size,
-                output_path=output_path,
-                save_file_fmt=save_file_fmt,
-                cfg_file_fmt=cfg_file_fmt,
-            )
-        case "resid_mlp":
-            return split_dataset_resid_mlp(
-                model_path=config.model_path,
-                n_batches=config.n_batches,
-                batch_size=config.batch_size,
-                output_path=output_path,
-                save_file_fmt=save_file_fmt,
-                cfg_file_fmt=cfg_file_fmt,
-            )
-        case name:
-            raise ValueError(
-                f"Unsupported task name '{name}'. Supported tasks are 'lm' and 'resid_mlp'. {config.model_path=}, {name=}"
-            )
+    if isinstance(config, Path):
+        config = MergeRunConfig.from_file(config)
+
+    model_path: str = config.model_path
+    task_name: TaskName = config.task_name
+    n_batches: int = config.n_batches
+    batch_size: int = config.batch_size
+
+    if task_name == "lm":
+        return split_dataset_lm(
+            model_path=model_path,
+            n_batches=n_batches,
+            batch_size=batch_size,
+            base_path=base_path,
+            save_file_fmt=save_file_fmt,
+            cfg_file_fmt=cfg_file_fmt,
+        )
+    elif task_name == "resid_mlp":
+        return split_dataset_resid_mlp(
+            model_path=model_path,
+            n_batches=n_batches,
+            batch_size=batch_size,
+            base_path=base_path,
+            save_file_fmt=save_file_fmt,
+            cfg_file_fmt=cfg_file_fmt,
+        )
+    else:
+        raise ValueError(
+            f"Unsupported task name '{task_name}'. Supported tasks are 'lm' and 'resid_mlp'. {model_path=}, {task_name=}"
+        )
+
+
+if __name__ == "__main__":
+    import argparse
+
+    parser: argparse.ArgumentParser = argparse.ArgumentParser(
+        description="tokenize and split a SimpleStories dataset into smaller batches for clustering ensemble",
+    )
+
+    parser.add_argument(
+        "--config",
+        "-c",
+        type=Path,
+        required=True,
+        help="Path to merge run config JSON/YAML file",
+    )
+    parser.add_argument(
+        "--base-path",
+        "-p",
+        type=Path,
+        default=Path("data/split_datasets"),
+        help="Base path to save the split datasets to",
+    )
+
+    args: argparse.Namespace = parser.parse_args()
+
+    split_dataset(
+        config=args.config,
+        base_path=args.base_path,
+    )
diff --git a/spd/clustering/scripts/s2_run_clustering.py b/spd/clustering/scripts/s2_run_clustering.py
new file mode 100644
index 0000000..6d0773e
--- /dev/null
+++ b/spd/clustering/scripts/s2_run_clustering.py
@@ -0,0 +1,422 @@
+# %%
+
+import functools
+import json
+import os
+from collections.abc import Callable
+from pathlib import Path
+from typing import Any, TextIO
+
+import numpy as np
+import torch
+import wandb
+import wandb.sdk.wandb_run
+from jaxtyping import Float, Int
+from matplotlib import pyplot as plt
+from muutils.dbg import dbg_auto, dbg_tensor
+from torch import Tensor
+
+from spd.clustering.activations import (
+    ProcessedActivations,
+    component_activations,
+    process_activations,
+)
+from spd.clustering.merge import merge_iteration
+from spd.clustering.merge_history import MergeHistory
+from spd.clustering.merge_run_config import MergeRunConfig
+from spd.clustering.plotting.merge import (
+    plot_merge_history_cluster_sizes,
+    plot_merge_history_costs,
+    plot_merge_iteration,
+)
+from spd.clustering.wandb_tensor_info import wandb_log_tensor
+from spd.models.component_model import ComponentModel, SPDRunInfo
+from spd.settings import REPO_ROOT
+
+# pyright: reportUnnecessaryIsInstance=false, reportUnreachable=false
+
+# Global batch_id for logging
+_BATCH_ID: str = "unk"
+
+# Delimiter for structured output parsing
+RESULT_DELIMITER: str = "-" * 50
+
+
+def _open_json_fd() -> TextIO:
+    """Open file descriptor for JSON output from environment variable"""
+    fd_num: int = int(os.environ["JSON_FD"])
+    return os.fdopen(fd_num, "w", buffering=1)
+
+
+def emit_result(obj: dict[str, str | None]) -> None:
+    """Emit result JSON via environment fd"""
+    out: TextIO = _open_json_fd()
+    print(json.dumps(obj, separators=(",", ":")), file=out, flush=True)
+
+
+def log(message: str) -> None:
+    """Print a message with orange batch ID prefix.
+
+    Works with both regular print and tqdm progress bars.
+    """
+    # ANSI color codes: \033[38;5;208m is orange, \033[0m resets
+    print(f"\033[38;5;208m[{_BATCH_ID}]\033[0m {message}")
+
+
+def save_group_idxs_artifact(
+    merge_hist: MergeHistory,
+    iteration: int,
+    wandb_run: wandb.sdk.wandb_run.Run,
+    save_dir: Path,
+    dataset_stem: str,
+) -> None:
+    """Save merge_hist to file and upload as WandB artifact"""
+    # Save to file in the same directory as merge history
+    group_idxs_path: Path = save_dir / f"iter_{iteration:04}.zip"
+    group_idxs_path.parent.mkdir(parents=True, exist_ok=True)
+    merge_hist.save(group_idxs_path)
+
+    # Create and upload artifact
+    artifact = wandb.Artifact(
+        name=f"merge_hist_iter.{dataset_stem}.iter_{iteration}",
+        type="merge_hist_iter",
+        description=f"Group indices for batch {dataset_stem} at iteration {iteration}",
+        metadata={
+            "batch_name": dataset_stem,
+            "iteration": iteration,
+            "config": merge_hist.config.model_dump(mode="json"),
+            "config_identifier": merge_hist.config,
+        },
+    )
+    artifact.add_file(str(group_idxs_path))
+    wandb_run.log_artifact(artifact)
+
+
+def plot_merge_iteration_callback(
+    costs: torch.Tensor,
+    # merge_history: MergeHistory,
+    current_merge: Any,
+    current_coact: torch.Tensor,
+    i: int,
+    component_labels: list[str],
+    wandb_run: wandb.sdk.wandb_run.Run | None,
+    **kwargs: Any,
+) -> None:
+    """Plot merge iteration and log to WandB."""
+    assert kwargs  # Ensure unused kwargs are passed
+
+    if wandb_run is not None:
+        # Create the plot and get the figure
+        fig = plot_merge_iteration(
+            current_merge=current_merge,
+            current_coact=current_coact,
+            costs=costs,
+            # pair_cost=merge_history.latest()["costs_stats"]["chosen_pair"],
+            iteration=i,
+            component_labels=component_labels,
+            show=False,  # Don't display the plot
+        )
+
+        # Log to WandB
+        wandb_run.log({"plots/merges": wandb.Image(fig)}, step=i)
+        plt.close(fig)  # Close figure to free memory
+
+
+def run_clustering(
+    config: MergeRunConfig | Path,
+    dataset_path: Path,
+    save_dir: Path = REPO_ROOT / "data/clustering/merge_history/wip/",
+    device: str = "cuda",
+    plot: bool = True,
+    sort_components: bool = False,
+) -> Path:
+    # setup
+    # ======================================================================
+    # Load config
+    config_: MergeRunConfig
+    if isinstance(config, Path):  # noqa: SIM108
+        config_ = MergeRunConfig.from_file(config)
+    else:
+        config_ = config
+
+    model_path: str = config_.model_path
+
+    # Extract batch ID from dataset filename (e.g., "batch_01.npz" -> "01")
+    global _BATCH_ID
+    _BATCH_ID = dataset_path.stem.split("_")[-1] if "_" in dataset_path.stem else dataset_path.stem  # pyright: ignore[reportConstantRedefinition]
+
+    log(f"Starting clustering for {dataset_path.name}")
+
+    # Initialize WandB run if enabled
+    wandb_run: wandb.sdk.wandb_run.Run | None = None
+    if config_.wandb_enabled:
+        wandb_run = wandb.init(
+            project=config_.wandb_project,
+            name=f"{config_.config_identifier}-{dataset_path.stem}",
+            group=config_.wandb_group,
+            config=config_.model_dump_with_properties(),
+            tags=[
+                "cluster-run",
+                f"model:{config_.wandb_decomp_model}",
+                f"task:{config_.task_name}",
+                f"batch:{dataset_path.stem}",
+                f"config:{config_.config_identifier}",
+            ],
+        )
+        log(f"Initialized WandB run: {wandb_run.name} in group {config_.wandb_group}")
+
+    this_merge_path: Path = save_dir / f"{config_.config_identifier}-data_{dataset_path.stem}"
+    this_merge_figs: Path = this_merge_path / "plots"
+    if plot:
+        this_merge_figs.mkdir(parents=True, exist_ok=True)
+
+    # Create callbacks if wandb is enabled
+    artifact_callback: Callable[[MergeHistory, int], None] | None = None
+    plot_callback: Callable[..., None] | None = None
+
+    if wandb_run is not None:
+        artifact_callback = functools.partial(
+            save_group_idxs_artifact,
+            wandb_run=wandb_run,
+            save_dir=this_merge_path / "checkpoints",
+            dataset_stem=dataset_path.stem,
+        )
+
+        plot_callback = functools.partial(
+            plot_merge_iteration_callback,
+            wandb_run=wandb_run,
+            batch_id=_BATCH_ID,
+        )
+
+    # get model and data
+    # ======================================================================
+    log(f"getting data batch from {dataset_path}")
+    # get the dataset -- for ensembles, each instance of this script gets a different batch
+    data_batch: Int[Tensor, "batch_size n_ctx"] = torch.tensor(np.load(dataset_path)["input_ids"])
+
+    # load the spd run of the actual model we are decomposing
+    log(f"getting spd run {model_path}")
+    spd_run: SPDRunInfo = SPDRunInfo.from_path(model_path)
+    component_model: ComponentModel = ComponentModel.from_pretrained(spd_run.checkpoint_path)
+    component_model.to(device)
+
+    # get, process, and plot component activations
+    # ======================================================================
+    log(f"computing activations on {device = }")
+    component_acts: dict[str, Tensor] = component_activations(
+        model=component_model,
+        batch=data_batch,
+        device=device,
+        # threshold=0.1,
+        sigmoid_type=spd_run.config.sigmoid_type,
+    )
+
+    if wandb_run is not None:
+        log("logging stats to wandb")
+        wandb_log_tensor(wandb_run, component_acts, "component_activations", step=0, single=True)
+    else:
+        dbg_auto(component_acts)
+
+    # process the activations by:
+    # 1. filtering out dead components
+    # 2. concatenating the activations across the sequence
+    # 3. computing coactivations
+    log("processing activations")
+    processed_activations: ProcessedActivations = process_activations(
+        component_acts,
+        filter_dead_threshold=config_.filter_dead_threshold,
+        seq_mode="concat" if config_.task_name == "lm" else None,
+        filter_modules=config_.filter_modules,
+        sort_components=sort_components,
+    )
+    dbg_tensor(processed_activations.activations)
+
+    if plot:
+        log("plotting")
+        # Import plotting function only when needed
+        from spd.clustering.plotting.activations import plot_activations
+
+        # Use original activations for raw plots, but filtered data for concat/coact/histograms
+        plot_activations(
+            processed_activations=processed_activations,
+            n_samples_max=256,
+            save_pdf=True,
+            pdf_prefix=(this_merge_figs / "activations").as_posix(),
+            wandb_run=wandb_run,
+            log=log,
+        )
+
+    # memory cleanup
+    # ======================================================================
+    # copy what we need, delete the rest to free memory
+    log("cleaning up memory")
+    activations_: Float[Tensor, "n_steps c"] = processed_activations.activations
+    labels: list[str] = processed_activations.labels.copy()
+    del processed_activations  # we copied what we needed
+    del component_acts  # processed already
+    del component_model  # already did the forward pass
+    del data_batch  # already did the forward pass
+
+    # run the merge iteration
+    # ======================================================================
+    log("starting merge iteration")
+    merge_history: MergeHistory = merge_iteration(
+        activations=activations_,
+        merge_config=config_,  # Pass full MergeRunConfig to access wandb_log_frequency
+        component_labels=labels,
+        wandb_run=wandb_run,
+        prefix=f"\033[38;5;208m[{_BATCH_ID}]\033[0m",
+        artifact_callback=artifact_callback,
+        plot_callback=plot_callback,
+    )
+
+    # saving and plotting
+    # ======================================================================
+
+    # save the merge iteration
+    hist_save_path: Path = this_merge_path / "merge_history.zip"
+
+    # TODO: Consider adding fallback to dbg_auto if wandb_run is None
+    # For now we skip logging merge_history_serialized as it's large and complex
+    # dbg_auto(merge_history_serialized)
+
+    merge_history.save(hist_save_path)
+    log(f"Merge history saved to {hist_save_path}")
+
+    # Save WandB URL to file
+    wburl_path: Path | None = None
+    wandb_url: str | None = None
+    if wandb_run is not None:
+        wburl_path = hist_save_path.with_suffix(".wburl")
+        if wandb_run.url:
+            wburl_path.write_text(wandb_run.url)
+            wandb_url = wandb_run.url
+
+    # Save merge history as WandB artifact
+    if wandb_run is not None:
+        artifact = wandb.Artifact(
+            name=f"merge_history_{dataset_path.stem}",
+            type="merge_history",
+            description=f"Merge history for batch {dataset_path.stem}",
+            metadata={
+                "batch_name": dataset_path.stem,
+                "config_identifier": config_.config_identifier,
+                "n_iters_current": merge_history.n_iters_current,
+                "filename": hist_save_path,
+            },
+        )
+        # Add both files before logging the artifact
+        artifact.add_file(str(hist_save_path))
+        wandb_run.log_artifact(artifact)
+
+    if plot:
+        fig_cs: plt.Figure = plot_merge_history_cluster_sizes(
+            history=merge_history,
+            file_prefix=(this_merge_figs / "merge").as_posix(),
+        )
+        fig_costs: plt.Figure = plot_merge_history_costs(
+            history=merge_history,
+            file_prefix=(this_merge_figs / "merge").as_posix(),
+        )
+        if wandb_run is not None:
+            wandb_run.log(
+                {"plots/merge_history_cluster_sizes": wandb.Image(fig_cs)},
+                step=merge_history.n_iters_current,
+            )
+            wandb_run.log(
+                {"plots/merge_history_costs": wandb.Image(fig_costs)},
+                step=merge_history.n_iters_current,
+            )
+        # Close figures to free memory
+        plt.close(fig_cs)
+        plt.close(fig_costs)
+
+    # Finish WandB run
+    if wandb_run is not None:
+        wandb_run.finish()
+        log(f"Finished WandB run with url: {wandb_run.url}")
+
+    # Output structured result for main.py to parse via fd 3
+    result: dict[str, str | None] = {
+        "hist_save_path": str(hist_save_path),
+        "wburl_path": str(wburl_path) if wburl_path else None,
+        "wandb_url": wandb_url,
+        "batch_name": dataset_path.stem,
+        "config_identifier": config_.config_identifier,
+    }
+
+    emit_result(result)
+
+    return hist_save_path
+
+
+def cli(argv: list[str] | None = None) -> None:
+    import argparse
+
+    parser: argparse.ArgumentParser = argparse.ArgumentParser(
+        description="Run a merge iteration on a batch of data using a component model."
+    )
+    parser.add_argument(
+        "--config",
+        "-c",
+        type=Path,
+        required=True,
+        help="Path to the merge run config JSON/YAML file",
+    )
+    parser.add_argument(
+        "--dataset-path",
+        "-d",
+        type=Path,
+        required=True,
+        help="Path to the dataset file (e.g., a .npz file with input_ids)",
+    )
+    parser.add_argument(
+        "--device",
+        "-D",
+        type=str,
+        default="cuda" if torch.cuda.is_available() else "cpu",
+        help="Device to run the model on (e.g., 'cuda' or 'cpu')",
+    )
+    parser.add_argument(
+        "--save-dir",
+        "-s",
+        type=Path,
+        required=True,
+        help="Directory to save the merge history",
+    )
+    # parser.add_argument(
+    #     "--sort-components",
+    #     action="store_true",
+    #     help="Sort components by similarity within each module before concatenation",
+    # )
+    parser.add_argument(
+        "--plot",
+        action="store_true",
+        dest="plot",
+        help="plotting of activations",
+    )
+    parser.add_argument(
+        "--override-json-fd",
+        type=int,
+        default=None,
+        help="Override the JSON file descriptor for structured output (for debugging)",
+    )
+
+    args: argparse.Namespace = parser.parse_args(argv)
+
+    if args.override_json_fd is not None:
+        os.environ["JSON_FD"] = str(args.override_json_fd)
+
+    run_clustering(
+        config=args.config,
+        dataset_path=args.dataset_path,
+        device=args.device,
+        save_dir=args.save_dir,
+        # sort_components=args.sort_components,
+        sort_components=False,
+        plot=args.plot,
+    )
+
+
+if __name__ == "__main__":
+    cli()
diff --git a/spd/clustering/scripts/s3_normalize_histories.py b/spd/clustering/scripts/s3_normalize_histories.py
new file mode 100644
index 0000000..a75f5a0
--- /dev/null
+++ b/spd/clustering/scripts/s3_normalize_histories.py
@@ -0,0 +1,167 @@
+import json
+from pathlib import Path
+from typing import Any
+
+import numpy as np
+import wandb
+from muutils.dbg import dbg_tensor
+from zanj import ZANJ
+
+from spd.clustering.math.merge_distances import MergesArray
+from spd.clustering.merge_history import MergeHistory, MergeHistoryEnsemble
+from spd.log import logger
+from spd.settings import REPO_ROOT
+
+# pyright: reportUnnecessaryIsInstance=false
+
+
+# TODO: this is messy, possible duplicates code from elsewhere
+def load_merge_histories_from_wandb(
+    wandb_urls: list[str],
+) -> tuple[list[str], MergeHistoryEnsemble]:
+    """Load merge histories from WandB run URLs"""
+    api = wandb.Api()
+    data: list[MergeHistory] = []
+
+    for url in wandb_urls:
+        # Parse URL format: wandb:entity/project/run_id or full URL
+        run_path: str
+        if url.startswith("wandb:"):
+            run_path = url.replace("wandb:", "")
+        else:
+            # Extract run path from full URL
+            # e.g. https://wandb.ai/entity/project/runs/run_id -> entity/project/run_id
+            parts: list[str] = url.split("/")
+            if "runs" in parts:
+                run_idx: int = parts.index("runs") + 1
+                run_path = f"{parts[run_idx - 3]}/{parts[run_idx - 2]}/{parts[run_idx]}"
+            else:
+                raise ValueError(f"Cannot parse WandB URL: {url}")
+
+        run = api.run(run_path)
+
+        # Find and download merge history artifact
+        artifacts = run.logged_artifacts()
+        merge_history_artifact = None
+        for artifact in artifacts:
+            if artifact.type == "merge_history":
+                merge_history_artifact = artifact
+                break
+
+        if merge_history_artifact is None:
+            raise ValueError(f"No merge_history artifact found for run {run_path}")
+
+        # Download the artifact using WandB's built-in caching
+        artifact_dir: str = merge_history_artifact.download()
+
+        # Find the .zip file in the downloaded artifact
+        zip_files: list[Path] = list(Path(artifact_dir).glob("*.zip"))
+        if not zip_files:
+            raise ValueError(f"No .zip file found in artifact for run {run_path}")
+
+        # Load the merge history
+        merge_history: MergeHistory = MergeHistory.read(zip_files[0])
+        data.append(merge_history)
+
+    ensemble = MergeHistoryEnsemble(data=data)
+
+    return wandb_urls, ensemble
+
+
+def load_merge_histories(
+    path: list[Path] | list[str] | str,
+) -> tuple[list[Path] | list[str], MergeHistoryEnsemble]:
+    """Load merge histories from a list of paths, WandB URLs, or a path format with wildcards"""
+    if isinstance(path, str):
+        # Single path with wildcards
+        paths_: list[Path] = list(Path(path).glob("*.zanj"))
+        data: list[MergeHistory] = [MergeHistory.read(p) for p in paths_]
+        ensemble: MergeHistoryEnsemble = MergeHistoryEnsemble(data=data)
+        return paths_, ensemble
+    elif isinstance(path, list):
+        if all(isinstance(p, Path) for p in path):
+            # List of file paths
+            paths_paths: list[Path] = path  # pyright: ignore[reportAssignmentType]
+            data = [MergeHistory.read(p) for p in paths_paths]
+            ensemble = MergeHistoryEnsemble(data=data)
+            return paths_paths, ensemble
+        elif all(isinstance(p, str) and (p.startswith("wandb:") or "wandb.ai" in p) for p in path):
+            # List of WandB URLs
+            wandb_urls: list[str] = path  # pyright: ignore[reportAssignmentType]
+            return load_merge_histories_from_wandb(wandb_urls)
+        else:
+            raise ValueError("Mixed or unsupported path types in list")
+    else:
+        raise ValueError(f"Unsupported path type: {type(path)}")  # pyright: ignore[reportUnreachable]
+
+
+def normalize_histories(
+    histories: list[Path] | list[str] | str,
+    run_dir: Path,
+) -> dict[str, Any]:
+    """Main function to load merge histories and compute distances"""
+    # get the histories from paths or URLs
+    ensemble: MergeHistoryEnsemble
+    paths: list[Path] | list[str]
+    paths, ensemble = load_merge_histories(histories)
+
+    # normalize
+    normalized_merge_array: MergesArray
+    normalized_merge_meta: dict[str, Any]
+    normalized_merge_array, normalized_merge_meta = ensemble.normalized()
+    dbg_tensor(normalized_merge_array)
+
+    # save things
+    run_dir.mkdir(parents=True, exist_ok=True)
+
+    normalized_merge_meta["paths"] = [str(p) for p in paths]
+    normalized_merge_meta["repo_root"] = str(REPO_ROOT)
+    path_metadata: Path = run_dir / "ensemble_meta.json"
+    path_metadata.write_text(json.dumps(normalized_merge_meta, indent="\t"))
+    logger.info(f"metadata saved to {path_metadata}")
+
+    path_merge_arr: Path = run_dir / "ensemble_merge_array.npz"
+    np.savez_compressed(
+        path_merge_arr,
+        merges=normalized_merge_array,
+    )
+    logger.info(f"merge array saved to {path_merge_arr}")
+
+    path_hist_ensemble: Path = run_dir / "ensemble_raw.zanj"
+    ZANJ().save(ensemble, path_hist_ensemble)
+    logger.info(f"Ensemble saved to {path_hist_ensemble}")
+
+    return dict(
+        ensemble_meta=normalized_merge_meta,
+        normalized_merge_array=path_merge_arr,
+        ensemble_raw=path_hist_ensemble,
+        paths=dict(
+            run_dir=run_dir,
+            input_histories=paths,
+            metadata=path_metadata,
+            merge_array=path_merge_arr,
+            ensemble_raw=path_hist_ensemble,
+        ),
+    )
+
+
+if __name__ == "__main__":
+    import argparse
+
+    parser: argparse.ArgumentParser = argparse.ArgumentParser(
+        description="Normalize merge histories"
+    )
+    parser.add_argument(
+        "histories",
+        type=str,
+        help="Path to the merge histories. Should contain just .zanj files for each history",
+    )
+    parser.add_argument(
+        "--out-dir",
+        type=Path,
+        default=REPO_ROOT / "data/clustering/merge_history/",
+        help="Output directory for the normalized histories",
+    )
+
+    args: argparse.Namespace = parser.parse_args()
+    normalize_histories(args.histories, args.out_dir)
diff --git a/spd/clustering/s4_compute_distances.py b/spd/clustering/scripts/s4_compute_distances.py
similarity index 74%
rename from spd/clustering/s4_compute_distances.py
rename to spd/clustering/scripts/s4_compute_distances.py
index 37f2ea0..b31da4e 100644
--- a/spd/clustering/s4_compute_distances.py
+++ b/spd/clustering/scripts/s4_compute_distances.py
@@ -14,26 +14,42 @@ from spd.clustering.plotting.merge import plot_dists_distribution
 from spd.log import logger
 
 
-def compute_and_save_distances_new(
+def compute_histories_distances(
     merges_path: Path,
     method: DistancesMethod = "perm_invariant_hamming",
-) -> DistancesArray:
+    wandb_urls: list[str] | None = None,
+    config_identifier: str | None = None,
+) -> tuple[Path, DistancesArray]:
     """Main function to load merge histories and compute distances"""
 
+    # load
     merge_array: MergesArray = np.load(merges_path, allow_pickle=True)["merges"]
 
+    # compute
     distances: DistancesArray = compute_distances(
         normalized_merge_array=merge_array,
         method=method,
     )
 
+    # save
     distances_path: Path = merges_path.with_suffix(f".{method}.distances.npz")
     np.savez_compressed(distances_path, distances=distances)
 
-    return distances
+    logger.info(f"Saved distances to {distances_path}")
+
+    # Create WandB report if URLs provided
+    if wandb_urls and config_identifier:
+        _create_clustering_report(
+            distances=distances,
+            method=method,
+            wandb_urls=wandb_urls,
+            config_identifier=config_identifier,
+        )
+
+    return distances_path, distances
 
 
-def create_clustering_report(
+def _create_clustering_report(
     distances: DistancesArray,
     method: DistancesMethod,
     wandb_urls: list[str],
@@ -112,3 +128,21 @@ def create_clustering_report(
         logger.info(
             f"Created wandb clustering summary report with {len(wandb_urls)} batch runs from config {config_identifier}:\n{run.url}/overview"
         )
+
+
+if __name__ == "__main__":
+    import argparse
+
+    parser: argparse.ArgumentParser = argparse.ArgumentParser(
+        description="Compute distances between merge histories"
+    )
+    parser.add_argument("merges-path", type=Path, help="Path to the merge histories file")
+    parser.add_argument(
+        "--method", type=str, default="perm_invariant_hamming", help="Distance method to use"
+    )
+    args: argparse.Namespace = parser.parse_args()
+
+    compute_histories_distances(
+        merges_path=args.merges_path,
+        method=args.method,
+    )
diff --git a/spd/clustering/sweep.py b/spd/clustering/sweep.py
index df08429..a931d32 100644
--- a/spd/clustering/sweep.py
+++ b/spd/clustering/sweep.py
@@ -10,6 +10,7 @@ from matplotlib.colors import LogNorm
 from matplotlib.lines import Line2D
 from tqdm import tqdm
 
+from spd.clustering.merge import merge_iteration
 from spd.clustering.merge_config import MergeConfig
 from spd.clustering.merge_history import MergeHistory
 
@@ -226,6 +227,34 @@ def add_colorbar_or_legend(
         )
 
 
+def run_hyperparameter_sweep(
+    raw_activations: torch.Tensor,
+    sweep_config: SweepConfig,
+    component_labels: list[str],
+) -> list[MergeHistory]:
+    """Run hyperparameter sweep across all parameter combinations."""
+    configs = sweep_config.generate_configs()
+    print(f"{len(configs) = }")
+
+    results: list[MergeHistory] = []
+
+    for _i, merge_config in tqdm(enumerate(configs), total=len(configs)):
+        try:
+            merge_history = merge_iteration(
+                activations=raw_activations,
+                merge_config=merge_config,
+                component_labels=component_labels,
+            )
+
+            # Config is already stored in merge_history from merge_iteration
+            results.append(merge_history)
+        except Exception as e:
+            print(f"Failed: {e}")
+
+    print(f"{len(results) = }")
+    return results
+
+
 def plot_evolution_histories(
     results: list[MergeHistory],
     fixed_params: dict[str, Any],
diff --git a/spd/clustering/util.py b/spd/clustering/util.py
index bd11e2f..69878ff 100644
--- a/spd/clustering/util.py
+++ b/spd/clustering/util.py
@@ -1,4 +1,11 @@
 from collections.abc import Callable
+from typing import Any
+
+
+def named_lambda[T_callable: Callable[[Any], Any]](name: str, fn: T_callable) -> T_callable:
+    """Helper to create a named lambda function for the sweep."""
+    fn.__name__ = name
+    return fn
 
 
 def format_scientific_latex(value: float) -> str:
diff --git a/spd/clustering/wandb_tensor_info.py b/spd/clustering/wandb_tensor_info.py
index 6c64c6e..3e49f14 100644
--- a/spd/clustering/wandb_tensor_info.py
+++ b/spd/clustering/wandb_tensor_info.py
@@ -13,37 +13,6 @@ from muutils.tensor_info import array_info
 from torch import Tensor
 
 
-def wandb_log_tensor(
-    run: wandb.sdk.wandb_run.Run,
-    data: Tensor | dict[str, Tensor],
-    name: str,
-    step: int,
-    single: bool = False,
-) -> None:
-    """Log tensor(s) with stats to WandB as metrics and histograms.
-
-    Args:
-        run: Current WandB run (None if WandB disabled)
-        data: Either a Tensor or dict[str, Tensor]
-        name: Name for logging
-        step: WandB step
-        single: True if this tensor is only logged once (component activations)
-    """
-    try:
-        if isinstance(data, dict):
-            # Handle dict of tensors
-            for key, tensor in data.items():
-                full_name: str = f"{name}.{key}"
-                _log_one(run, tensor, full_name, step, single=single)
-        else:
-            # Handle single tensor
-            _log_one(run, data, name, step, single=single)
-    except Exception as e:
-        warnings.warn(f"Failed to log tensor {name}: {e}")  # noqa: B028
-        dbg_tensor(data)
-        raise e
-
-
 def _create_histogram(
     info: dict[str, Any], tensor: Tensor, name: str, logy: bool = True
 ) -> plt.Figure:
@@ -192,6 +161,37 @@ def _create_histogram_wandb(tensor: Tensor, name: str) -> go.Figure:  # pyright:
     return fig
 
 
+def wandb_log_tensor(
+    run: wandb.sdk.wandb_run.Run,
+    data: Tensor | dict[str, Tensor],
+    name: str,
+    step: int,
+    single: bool = False,
+) -> None:
+    """Log tensor(s) with stats to WandB as metrics and histograms.
+
+    Args:
+        run: Current WandB run (None if WandB disabled)
+        data: Either a Tensor or dict[str, Tensor]
+        name: Name for logging
+        step: WandB step
+        single: True if this tensor is only logged once (component activations)
+    """
+    try:
+        if isinstance(data, dict):
+            # Handle dict of tensors
+            for key, tensor in data.items():
+                full_name: str = f"{name}.{key}"
+                _log_one(run, tensor, full_name, step, single=single)
+        else:
+            # Handle single tensor
+            _log_one(run, data, name, step, single=single)
+    except Exception as e:
+        warnings.warn(f"Failed to log tensor {name}: {e}")  # noqa: B028
+        dbg_tensor(data)
+        raise e
+
+
 def _log_one(
     run: wandb.sdk.wandb_run.Run,
     tensor_: Tensor,
diff --git a/spd/models/component_model.py b/spd/models/component_model.py
index 72c117a..12fbd6f 100644
--- a/spd/models/component_model.py
+++ b/spd/models/component_model.py
@@ -68,7 +68,7 @@ class SPDRunInfo(RunInfo[Config]):
 
         return cls(checkpoint_path=comp_model_path, config=config)
 
-# TODO encapsulate Gates in a separate class (containing sigmoid type and sampling mode)
+
 class ComponentModel(LoadableModule):
     """Wrapper around an arbitrary model for running SPD.
 
diff --git a/tests/clustering/math/test_compute_rank.py b/tests/clustering/math/test_compute_rank.py
new file mode 100644
index 0000000..d538992
--- /dev/null
+++ b/tests/clustering/math/test_compute_rank.py
@@ -0,0 +1,246 @@
+import numpy as np
+import pytest
+
+from spd.clustering.math.compute_rank import compute_rank_of_sum as rank_of_sum  # noqa: E402
+
+
+def random_orth(
+    d: int,
+    r: int,
+    rng: np.random.Generator,
+) -> np.ndarray:
+    """Generate a d x r column-orthonormal matrix."""
+    q, _ = np.linalg.qr(rng.standard_normal((d, r)), mode="reduced")
+    return q[:, :r]
+
+
+def brute_rank(m: np.ndarray, tol: float = 1.0e-12) -> int:
+    """
+    Ground-truth numerical rank via full SVD.
+
+    # Parameters:
+     - `m : np.ndarray`
+        matrix
+     - `tol : float`
+        singular values ≤ `tol` are treated as zero
+
+    # Returns:
+     - `int` : numerical rank
+    """
+    return int(np.sum(np.linalg.svd(m, compute_uv=False) > tol))
+
+
+@pytest.mark.parametrize("d,_r1,r2", [(8, 0, 0), (8, 0, 5), (8, 5, 0), (32, 3, 3)])
+def test_edge_zero_ranks(
+    d: int,
+    _r1: int,
+    r2: int,
+    rng: np.random.Generator | None = None,
+) -> None:
+    """
+    Edge-cases with zero-rank summands (all-zero matrices).
+    """
+    if rng is None:
+        rng = np.random.default_rng(0)
+
+    u1 = np.empty((d, 0))
+    v1 = np.empty((d, 0))
+    s1 = np.empty((0,))
+
+    u2 = random_orth(d, r2, rng)
+    v2 = random_orth(d, r2, rng)
+    s2 = rng.random(r2) + 0.1
+
+    p1 = np.zeros((d, d))
+    p2 = u2 @ np.diag(s2) @ v2.T
+
+    expected = brute_rank(p1 + p2)
+    got = rank_of_sum(u1, s1, v1, u2, s2, v2)
+
+    assert expected == got
+
+
+@pytest.mark.parametrize(
+    "d,r1,r2",
+    [
+        (32, 5, 4),
+        (64, 3, 3),
+        (50, 10, 1),
+        (40, 6, 6),
+        (128, 15, 15),
+    ],
+)
+def test_random_cases(
+    d: int,
+    r1: int,
+    r2: int,
+    rng: np.random.Generator | None = None,
+) -> None:
+    """
+    Random low-rank matrices with independent subspaces.
+    """
+    if rng is None:
+        rng = np.random.default_rng(42)
+
+    u1 = random_orth(d, r1, rng)
+    v1 = random_orth(d, r1, rng)
+    s1 = rng.random(r1) + 0.1
+
+    u2 = random_orth(d, r2, rng)
+    v2 = random_orth(d, r2, rng)
+    s2 = rng.random(r2) + 0.1
+
+    p1 = u1 @ np.diag(s1) @ v1.T
+    p2 = u2 @ np.diag(s2) @ v2.T
+
+    expected = brute_rank(p1 + p2)
+    got = rank_of_sum(u1, s1, v1, u2, s2, v2)
+
+    assert expected == got
+
+
+@pytest.mark.skip("rank computation is broken!")
+@pytest.mark.parametrize("overlap_dim", [1, 2, 3])
+def test_overlapping_subspaces(
+    overlap_dim: int,
+    d: int = 32,
+    r_base: int = 6,
+    rng: np.random.Generator | None = None,
+) -> None:
+    """
+    P1 and P2 share `overlap_dim` identical left singular vectors,
+    so rank(P1+P2) < rank(P1)+rank(P2).
+    """
+    if rng is None:
+        rng = np.random.default_rng(123)
+
+    # Build common orthonormal basis
+    u_common = random_orth(d, overlap_dim, rng)
+
+    # Unique parts
+    u1_unique = random_orth(d, r_base - overlap_dim, rng)
+    u2_unique = random_orth(d, r_base - overlap_dim, rng)
+
+    # Orthonormalise w.r.t common part
+    u1 = np.linalg.qr(np.concatenate((u_common, u1_unique), axis=1))[0]
+    u2 = np.linalg.qr(np.concatenate((u_common, u2_unique), axis=1))[0]
+
+    v1 = random_orth(d, r_base, rng)
+    v2 = random_orth(d, r_base, rng)
+
+    s1 = rng.random(r_base) + 0.1
+    s2 = rng.random(r_base) + 0.1
+
+    p1 = u1 @ np.diag(s1) @ v1.T
+    p2 = u2 @ np.diag(s2) @ v2.T
+
+    expected = brute_rank(p1 + p2)
+    got = rank_of_sum(u1, s1, v1, u2, s2, v2)
+
+    assert expected == got
+    assert expected < (2 * r_base)  # sanity: reduced rank due to overlap
+
+
+def test_small_singular_values(
+    d: int = 20,
+    r1: int = 4,
+    r2: int = 4,
+    tiny: float = 1.0e-14,
+    rng: np.random.Generator | None = None,
+) -> None:
+    """
+    Verify tolerance handling: one matrix has almost-zero singular values.
+    """
+    if rng is None:
+        rng = np.random.default_rng(999)
+
+    u1 = random_orth(d, r1, rng)
+    v1 = random_orth(d, r1, rng)
+    s1 = np.full(r1, tiny)
+
+    u2 = random_orth(d, r2, rng)
+    v2 = random_orth(d, r2, rng)
+    s2 = rng.random(r2) + 0.1
+
+    p1 = u1 @ np.diag(s1) @ v1.T
+    p2 = u2 @ np.diag(s2) @ v2.T
+
+    expected = brute_rank(p1 + p2)
+    got = rank_of_sum(u1, s1, v1, u2, s2, v2)
+
+    assert expected == got
+
+
+@pytest.mark.parametrize("seed", list(range(10)))
+def test_many_random_seeds(
+    seed: int,
+    d: int = 48,
+    r1: int = 5,
+    r2: int = 7,
+) -> None:
+    """
+    Monte-carlo sweep across seeds to catch hidden edge cases.
+    """
+    rng = np.random.default_rng(seed)
+    u1 = random_orth(d, r1, rng)
+    v1 = random_orth(d, r1, rng)
+    s1 = rng.random(r1) + 0.1
+
+    u2 = random_orth(d, r2, rng)
+    v2 = random_orth(d, r2, rng)
+    s2 = rng.random(r2) + 0.1
+
+    expected = brute_rank(u1 @ np.diag(s1) @ v1.T + u2 @ np.diag(s2) @ v2.T)
+    got = rank_of_sum(u1, s1, v1, u2, s2, v2)
+
+    assert expected == got
+
+
+def test_argument_order_symmetry(
+    d: int = 40,
+    r1: int = 6,
+    r2: int = 4,
+    rng: np.random.Generator | None = None,
+) -> None:
+    """
+    rank(P1+P2) must be symmetric w.r.t argument order.
+    """
+    if rng is None:
+        rng = np.random.default_rng(2025)
+
+    u1 = random_orth(d, r1, rng)
+    v1 = random_orth(d, r1, rng)
+    s1 = rng.random(r1) + 0.1
+
+    u2 = random_orth(d, r2, rng)
+    v2 = random_orth(d, r2, rng)
+    s2 = rng.random(r2) + 0.1
+
+    rank12 = rank_of_sum(u1, s1, v1, u2, s2, v2)
+    rank21 = rank_of_sum(u2, s2, v2, u1, s1, v1)
+
+    assert rank12 == rank21
+
+
+def test_large_dimension_performance(
+    d: int = 256,
+    r1: int = 20,
+    r2: int = 25,
+    rng: np.random.Generator | None = None,
+) -> None:
+    """
+    Sanity test on a moderately large `d`; ensures code runs in <~0.1 s.
+    """
+    if rng is None:
+        rng = np.random.default_rng(7)
+
+    u1 = random_orth(d, r1, rng)
+    v1 = random_orth(d, r1, rng)
+    s1 = rng.random(r1) + 0.1
+
+    u2 = random_orth(d, r2, rng)
+    v2 = random_orth(d, r2, rng)
+    s2 = rng.random(r2) + 0.1
+
+    got = rank_of_sum(u1, s1, v1, u2, s2, v2)
+    assert got <= (r1 + r2)  # trivial upper bound
diff --git a/tests/clustering/test_wandb_integration.py b/tests/clustering/test_wandb_integration.py
index 04d4c34..e4bde8e 100644
--- a/tests/clustering/test_wandb_integration.py
+++ b/tests/clustering/test_wandb_integration.py
@@ -7,7 +7,7 @@ from unittest.mock import Mock, patch
 import torch
 
 from spd.clustering.merge_history import MergeHistory
-from spd.clustering.scripts.s2_clustering import save_group_idxs_artifact
+from spd.clustering.scripts.s2_run_clustering import save_group_idxs_artifact
 from spd.clustering.scripts.s3_normalize_histories import load_merge_histories_from_wandb
 
 




wandb_project: spd
dtype: bfloat16
batch_size: 64
num_iterations: 100_000
warmup_iters: 600
learning_rate: 3e-4
learning_rate_decay_frac: 0.1
weight_decay: 0.1
grad_clip: 1.0
val_loss_every: 1000
val_max_steps: 20
sample_every: 1000
intermediate_checkpoints: false

model:
  model_type: LlamaSimpleMLP
  attn_bias: false
  block_size: 512
  flash_attention: false
  mlp_bias: false
  n_ctx: 512
  n_embd: 2048
  n_head: 16
  n_intermediate: 8192  # 2048 * 4
  n_key_value_heads: 16
  n_layer: 2
  rms_norm_eps: 1.0e-06
  rotary_adjacent_pairs: false
  rotary_base: 10000
  rotary_dim: 341       # 2048 // 6
  use_grouped_query_attention: true
  vocab_size: 50277

train_dataset_config:
  name: danbraunai/pile-uncopyrighted-tok-shuffled
  is_tokenized: true
  hf_tokenizer_path: EleutherAI/gpt-neox-20b
  split: train
  streaming: false
  n_ctx: 513  # model n_ctx + 1 for next-token label indexing
  seed: 0
  column_name: input_ids

val_dataset_config:
  name: danbraunai/pile-uncopyrighted-tok-shuffled
  is_tokenized: true
  hf_tokenizer_path: EleutherAI/gpt-neox-20b
  split: val
  streaming: false
  n_ctx: 513  # model n_ctx + 1 for next-token label indexing
  seed: 0
  column_name: input_ids

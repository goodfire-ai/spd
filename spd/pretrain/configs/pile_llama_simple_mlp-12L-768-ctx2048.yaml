wandb_project: spd
dtype: float32
batch_size: 256
num_iterations: 100_000
warmup_iters: 600
learning_rate: 1e-4
learning_rate_decay_frac: 0.1
weight_decay: 0.1
grad_clip: 1.0
val_loss_every: 1000
val_max_steps: 20
sample_every: 1000
intermediate_checkpoints: false

model:
  model_type: LlamaSimpleMLP
  block_size: 2048
  vocab_size: 50257  # GPT-2 tokenizer native vocab size
  n_layer: 12
  n_head: 12
  n_embd: 768
  n_intermediate: 3072  # 768 * 4
  rotary_dim: 128       # 768 // 6
  n_ctx: 2048
  n_key_value_heads: 12
  flash_attention: false

train_dataset_config:
  name: monology/pile-uncopyrighted
  is_tokenized: false
  hf_tokenizer_path: gpt2
  split: train[:10000000]  # Dataset has 177M examples
  streaming: false
  n_ctx: 2048
  seed: 0
  column_name: text

val_dataset_config:
  name: monology/pile-uncopyrighted
  is_tokenized: false
  hf_tokenizer_path: gpt2
  split: train[-100000:]
  streaming: false
  n_ctx: 2048
  seed: 0
  column_name: text

# Mem experiment config for SPD decomposition
# --- WandB ---
wandb_project: spd
wandb_run_name: null
wandb_run_name_prefix: ""

# --- General ---
seed: 0
C: 64
n_mask_samples: 1
ci_fn_type: "mlp"
ci_fn_hidden_dims: [16]
sigmoid_type: "leaky_hard"
target_module_patterns:
  - "embed"
  - "block.attn.q_proj"
  - "block.attn.k_proj"
  - "block.attn.v_proj"
  - "block.attn.o_proj"
  - "block.mlp.gate_proj"
  - "block.mlp.up_proj"
  - "block.mlp.down_proj"
  - "unembed"
identity_module_patterns: null
use_delta_component: true

# --- Loss config ---
loss_metric_configs:
- coeff: 1e-3
  classname: ImportanceMinimalityLoss
  pnorm: 2.0
  p_anneal_start_frac: 0.0
  p_anneal_final_p: 0.5
  p_anneal_end_frac: 1.0
  eps: 1.0e-12
- coeff: 0.5
  classname: StochasticReconSubsetLoss
- coeff: 0.5
  init: random
  step_size: 1.0
  n_steps: 2
  mask_scope: shared_across_batch
  classname: PGDReconSubsetLoss
- coeff: 1.0
  classname: FaithfulnessLoss
output_loss_type: kl

# --- Training ---
batch_size: 64
eval_batch_size: 64
steps: 10_000
lr: 1e-4
lr_schedule: constant
lr_warmup_pct: 0.0

# --- Faithfulness Warmup ---
faithfulness_warmup_steps: 100
faithfulness_warmup_lr: 0.01
faithfulness_warmup_weight_decay: 0.1

# --- Logging & Saving ---
train_log_freq: 100
eval_freq: 500
n_eval_steps: 50
slow_eval_freq: 2_500
slow_eval_on_first_step: true
save_freq: null
ci_alive_threshold: 0.01
n_examples_until_dead: 1000
eval_metric_configs:
  - classname: "CIHistograms"
    n_batches_accum: 5
  - classname: "ComponentActivationDensity"
  - classname: "CI_L0"
    groups: null
  - classname: "CIMeanPerComponent"

# --- Pretrained model info ---
pretrained_model_class: "spd.experiments.mem.models.MemTransformer"
pretrained_model_path: "wandb:goodfire/spd/runs/6anqjsv1"  # Set this to the path of a trained mem model

# --- Task Specific ---
task_config:
  task_name: mem


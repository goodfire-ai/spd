# Config for tinystories

# --- WandB ---
wandb_project: spd
# wandb_project: null
wandb_run_name: null
wandb_run_name_prefix: ""

# --- General ---
seed: 0
C: 100
n_mask_samples: 1
gate_type: "vector_mlp"
gate_hidden_dims: [8]
sigmoid_type: "leaky_hard"
target_module_patterns: ["transformer.h.*.attn.c_attn", "transformer.h.*.attn.c_proj"]

# --- Loss Coefficients ---
faithfulness_coeff: 1.0
recon_coeff: null
stochastic_recon_coeff: null
recon_layerwise_coeff: null
stochastic_recon_layerwise_coeff: null
importance_minimality_coeff: 1e-6
schatten_coeff: null
# embedding_recon_coeff: 1
embedding_recon_coeff: null
is_embed_unembed_recon: false
pnorm: 2.0
output_loss_type: kl

# --- Training ---
batch_size: 1
steps: 50_000
lr: 1e-4
lr_schedule: constant
lr_warmup_pct: 0.01
lr_exponential_halflife: null
n_eval_steps: 100

# --- Logging & Saving ---
image_freq: 2000
image_on_first_step: true
print_freq: 1000
save_freq: null
n_examples_until_dead: 8_192_000  # print_freq * batch_size * max_seq_len = 1000 * 4 * 2048
figures_fns:
  - name: "ci_histograms"
  - name: "mean_component_activation_counts"
metrics_fns:
  - name: "ci_l0"
  - name: "lm_kl"
  - name: "lm_ce_losses"


# --- Pretrained model info ---
pretrained_model_class: transformers.GPT2LMHeadModel
pretrained_model_name_hf: openai-community/gpt2
pretrained_model_output_attr: logits
tokenizer_name: openai-community/gpt2

# --- Task Specific ---
task_config:
  task_name: lm
  max_seq_len: 512
  buffer_size: 1000
  dataset_name: "roneneldan/TinyStories"
  column_name: "text"
  train_data_split: "train[:100000]"
  eval_data_split: "validation[:10000]"
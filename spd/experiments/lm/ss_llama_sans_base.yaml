# --- WandB ---
wandb_project: spd
wandb_run_name: null
wandb_run_name_prefix: ""

# --- General ---
seed: 0
C: 4000
n_mask_samples: 1
gate_type: "vector_mlp"
gate_hidden_dims: [12]
sigmoid_type: "leaky_hard"
target_module_patterns: ["model.layers.*.mlp.gate_proj", "model.layers.*.mlp.down_proj", "model.layers.*.mlp.up_proj", "model.layers.*.self_attn.q_proj", "model.layers.*.self_attn.k_proj", "model.layers.*.self_attn.v_proj", "model.layers.*.self_attn.o_proj"]
sampling: "binomial"

# --- Loss Coefficients ---
faithfulness_coeff: 0
recon_coeff: null
stochastic_recon_coeff: 0.2 
recon_layerwise_coeff: null
stochastic_recon_layerwise_coeff: 2
stochastic_recon_subset_coeff: null  # New loss - actively train on this

importance_minimality_coeff: 0.0003
l0_balancing_coeff: 0  # Encourage uniform sparsity across layers
l0_balancing_groups:  # Define groups for balancing (one per layer)
  - ["model.layers.0.*"]
  - ["model.layers.1.*"]
  - ["model.layers.2.*"]
  - ["model.layers.3.*"]
schatten_coeff: null
out_recon_coeff: null
embedding_recon_coeff: null
is_embed_unembed_recon: false
pnorm: 2.0
p_anneal_start_frac: 0.0
p_anneal_final_p: 0.2
p_anneal_end_frac: 0.75
output_loss_type: kl

# --- Training ---
batch_size: 16
eval_batch_size: 16
steps: 300000
lr: 0.0005
lr_schedule: cosine
lr_warmup_pct: 0.0
lr_exponential_halflife: null
gradient_accumulation_steps: 2

# --- Logging & Saving ---
train_log_freq: 100
eval_freq: 1000
slow_eval_freq: 5000
slow_eval_on_first_step: true
n_eval_steps: 5
save_freq: null
ci_alive_threshold: 0.0
n_examples_until_dead: 1368400
eval_metrics:
  - classname: "CIHistograms"
    extra_init_kwargs:
      n_batches_accum: 5
  - classname: "ComponentActivationDensity"
    extra_init_kwargs: {}
  - classname: "CI_L0"
    extra_init_kwargs:
      groups:
        total: ["*"]  # Total L0 across all modules
        layer_0: ["model.layers.0.*"]
        layer_1: ["model.layers.1.*"]
        layer_2: ["model.layers.2.*"]
        layer_3: ["model.layers.3.*"]
  - classname: "CEandKLLosses"
    extra_init_kwargs:
      rounding_threshold: 0.0
  - classname: "StochasticReconLayerwiseLoss"
    extra_init_kwargs: {}
  - classname: "SubsetReconstructionLoss"
    extra_init_kwargs:
      n_mask_samples: 1
      include_patterns:
        all: ["*"]
        layer_0_only: ["model.layers.0.*"]
        layer_1_only: ["model.layers.1.*"]
        layer_2_only: ["model.layers.2.*"]
        layer_3_only: ["model.layers.3.*"]
        mlp_only: ["*.mlp.*"]
        attention_only: ["*.self_attn.*"]
      exclude_patterns:
        all_but_layer_0: ["model.layers.0.*"]
        all_but_layer_1: ["model.layers.1.*"]
        all_but_layer_2: ["model.layers.2.*"]
        all_but_layer_3: ["model.layers.3.*"]

# --- Pretrained model info ---
pretrained_model_class: transformers.LlamaForCausalLM
pretrained_model_name: SimpleStories/SimpleStories-1.25M
pretrained_model_path: null
pretrained_model_output_attr: logits
tokenizer_name: SimpleStories/SimpleStories-1.25M

# --- Task Specific ---
task_config:
  task_name: lm 
  max_seq_len: 512
  buffer_size: 1000
  dataset_name: "SimpleStories/SimpleStories"
  column_name: "story"
  train_data_split: "train"
  eval_data_split: "test"
  shuffle_each_epoch: true
  is_tokenized: false
  streaming: false


# Config details for the target model taken from https://github.com/danbraunai/simple_stories_train/blob/main/simple_stories_train/models/model_configs.py#L54
  # "1.25M": LlamaConfig(
  #     block_size=512,
  #     vocab_size=4096,
  #     n_layer=4,
  #     n_head=4,
  #     n_embd=128,
  #     n_intermediate=128 * 4 * 2 // 3 = 341,
  #     rotary_dim=128 // 4 = 32,
  #     n_ctx=512,
  #     n_key_value_heads=2,
  #     flash_attention=True,
  # ),
# Config for tinystories

# --- WandB ---
wandb_project: spd
# wandb_project: null
wandb_run_name: null
wandb_run_name_prefix: ""

# --- General ---
seed: 0
C: 100
n_mask_samples: 1
gate_type: "vector_mlp"
gate_hidden_dims: [8]
sigmoid_type: "leaky_hard"
target_module_patterns: ["transformer.h.3.mlp.c_fc"]
sampling: "continuous"
use_delta_component: true

loss_metric_configs:
  - classname: "ImportanceMinimalityLoss"
    coeff: 0.0003
    pnorm: 2.0
    p_anneal_start_frac: 0.0
    p_anneal_final_p: 0.3
    p_anneal_end_frac: 1.0
  - classname: "StochasticReconLayerwiseLoss"
    coeff: 2.0
output_loss_type: kl

# --- Training ---
batch_size: 4
eval_batch_size: 4
steps: 50_000
lr: 1e-4
lr_schedule: constant
lr_warmup_pct: 0.01
lr_exponential_halflife: null

# --- Logging & Saving ---
train_log_freq: 50
eval_freq: 200
n_eval_steps: 10
slow_eval_freq: 2000
slow_eval_on_first_step: true
save_freq: null
n_examples_until_dead: 8_192_000
eval_metric_configs:
  - classname: "CIHistograms"
    n_batches_accum: null
  - classname: "ComponentActivationDensity"
  - classname: "CI_L0"
    groups: null
  - classname: "CEandKLLosses"
    rounding_threshold: 0.1
  - classname: "CIMeanPerComponent"

# --- Pretrained model info ---
pretrained_model_class: transformers.AutoModelForCausalLM
pretrained_model_name: roneneldan/TinyStories-1M
pretrained_model_output_attr: logits
tokenizer_name: EleutherAI/gpt-neo-125M

# --- Task Specific ---
task_config:
  task_name: lm
  max_seq_len: 2048
  dataset_name: "roneneldan/TinyStories"
  column_name: "text"
  train_data_split: "train"
  eval_data_split: "validation"
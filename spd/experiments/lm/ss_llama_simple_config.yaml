# --- WandB ---
wandb_project: null
wandb_run_name: null
wandb_run_name_prefix: ""

# --- General ---
seed: 0
C: 5000
n_mask_samples: 1
ci_fn_type: "shared_mlp"
ci_fn_hidden_dims: [1000]
sigmoid_type: "leaky_hard"
target_module_patterns:
  [
    "h.*.mlp.gate_proj",
    "h.*.mlp.up_proj",
    "h.*.mlp.down_proj",
    "h.*.attn.q_proj",
    "h.*.attn.k_proj",
    "h.*.attn.v_proj",
    "h.*.attn.o_proj",
  ]
identity_module_patterns: null
sampling: "continuous"
use_delta_component: true
loss_metric_configs:
  - classname: "StochasticArbHiddenActsReconLoss"
    coeff: 1.0
    post_target_module_patterns:
      - "model.embed_tokens"
  - classname: "PGDArbHiddenActsReconLoss"
    coeff: 0.1
    post_target_module_path: "model.embed_tokens"
    init: "random"
    step_size: 0.2
    n_steps: 2
    mask_scope: "shared_across_batch"
  - classname: "ImportanceMinimalityLoss"
    coeff: 0.004
    pnorm: 2.0
    p_anneal_start_frac: 0.0
    p_anneal_final_p: 0.3
    p_anneal_end_frac: 1.0

  - classname: "StochasticReconSubsetLoss"
    coeff: 1.0

output_loss_type: kl

# --- Training ---
batch_size: 256
eval_batch_size: 64
steps: 20000
lr: 0.0005
lr_schedule: cosine
steps: 200000
batch_size: 384 # / 4 (DDP) == 96
gradient_accumulation_steps: 1
eval_batch_size: 128
n_eval_steps: 5
eval_freq: 125
slow_eval_freq: 2000
slow_eval_on_first_step: true

# --- Faithfulness Warmup ---
faithfulness_warmup_steps: 200
faithfulness_warmup_lr: 0.01
faithfulness_warmup_weight_decay: 0.1

# --- Logging & Saving ---
train_log_freq: 25
save_freq: null
ci_alive_threshold: 0.0
n_examples_until_dead: 1368400
eval_metric_configs:
  - classname: "FaithfulnessLoss"
  - classname: "CIHistograms"
    n_batches_accum: 5
  - classname: "ComponentActivationDensity"
  - classname: "CI_L0"
    groups:
      total: ["*"] # Sum of all L0 values
      layer_0: ["h.0.*"]
      layer_1: ["h.1.*"]
      layer_2: ["h.2.*"]
      layer_3: ["h.3.*"]
  - classname: "CEandKLLosses"
    rounding_threshold: 0.0
  - classname: "CI_L0"
  - classname: "CIMeanPerComponent"
  - classname: "StochasticReconSubsetCEAndKL"
    include_patterns:
      layer_0_only: ["h.0.*"]
      layer_1_only: ["h.1.*"]
      layer_2_only: ["h.2.*"]
      layer_3_only: ["h.3.*"]
      mlp_only: ["*.mlp.*"]
      attention_only: ["*.attn.*"]
    exclude_patterns:
      all_but_layer_0: ["h.0.*"]
      all_but_layer_1: ["h.1.*"]
      all_but_layer_2: ["h.2.*"]
      all_but_layer_3: ["h.3.*"]
  - classname: "StochasticHiddenActsReconLoss"

# --- Pretrained model info ---
pretrained_model_class: simple_stories_train.models.llama_simple.LlamaSimple
pretrained_model_name: wandb:goodfire/spd/runs/erq48r3w # 100k steps 4019 tok
pretrained_model_output_attr: idx_0
tokenizer_name: SimpleStories/test-SimpleStories-gpt2-1.25M # 4019 tok: TODO: Load from wandb instead

# --- Task Specific ---
task_config:
  task_name: lm
  max_seq_len: 512
  buffer_size: 1000
  dataset_name: "SimpleStories/SimpleStories"
  column_name: "story"
  train_data_split: "train"
  eval_data_split: "test"
  shuffle_each_epoch: true
  is_tokenized: false
  streaming: false
# Config details for the target model taken from https://github.com/danbraunai/simple_stories_train/blob/main/simple_stories_train/models/model_configs.py#L54
# "llama_simple-1.25M": LlamaSimpleConfig(
#     block_size=512,
#     vocab_size=4019,
#     n_layer=4,
#     n_head=4,
#     n_embd=128,
#     n_intermediate=128 * 4 * 2 // 3 = 341,
#     rotary_dim=128 // 4 = 32,
#     n_ctx=512,
#     n_key_value_heads=2,
#     flash_attention=True,
# ),

# --- WandB ---
wandb_project: spd
wandb_run_name: null
wandb_run_name_prefix: ""

# --- General ---
seed: 0
n_mask_samples: 1
ci_fn_type: "vector_mlp"
ci_fn_hidden_dims: [12]
sigmoid_type: "leaky_hard"
module_info:
  - module_pattern: "h.*.mlp.c_fc"
    C: 2000
  - module_pattern: "h.*.mlp.c_proj"
    C: 2000
  - module_pattern: "h.*.attn.k_proj"
    C: 2000
  - module_pattern: "h.*.attn.v_proj"
    C: 2000
  - module_pattern: "h.*.attn.c_proj"
    C: 2000
identity_module_info: null
sampling: "continuous"
use_delta_component: true

loss_metric_configs:
  - classname: "ImportanceMinimalityLoss"
    coeff: 0.0003
    pnorm: 2.0
    beta: 0
    p_anneal_start_frac: 0.0
    p_anneal_final_p: 0.3
    p_anneal_end_frac: 1.0
  - classname: "StochasticReconLayerwiseLoss"
    coeff: 2.0
  - classname: "StochasticReconLoss"
    coeff: 0.2

# --- Training ---
batch_size: 48
eval_batch_size: 48
steps: 50000
lr_schedule:
  start_val: 5e-4
  fn_type: cosine
  warmup_pct: 0.0
  final_val_frac: 0.1
gradient_accumulation_steps: 1

# --- Faithfulness Warmup ---
faithfulness_warmup_steps: 200
faithfulness_warmup_lr: 0.01
faithfulness_warmup_weight_decay: 0.1

# --- Logging & Saving ---
train_log_freq: 200
eval_freq: 1000
slow_eval_freq: 5000
slow_eval_on_first_step: true
n_eval_steps: 5
save_freq: null
ci_alive_threshold: 0.0
eval_metric_configs:
  - classname: "CIHistograms"
    n_batches_accum: 5
  - classname: "ComponentActivationDensity"
  - classname: "CI_L0"
    groups:
      total: ["*"]  # Sum of all L0 values
      layer_0: ["h.0.*"]
      layer_1: ["h.1.*"]
      layer_2: ["h.2.*"]
      layer_3: ["h.3.*"]
  - classname: "CEandKLLosses"
    rounding_threshold: 0.0
  - classname: "CIMeanPerComponent"
  - classname: "StochasticReconSubsetCEAndKL"
    include_patterns:
      layer_0_only: ["h.0.*"]
      layer_1_only: ["h.1.*"]
      layer_2_only: ["h.2.*"]
      layer_3_only: ["h.3.*"]
      mlp_only: ["*.mlp.*"]
      attention_only: ["*.self_attn.*"]
    exclude_patterns:
      all_but_layer_0: ["h.0.*"]
      all_but_layer_1: ["h.1.*"]
      all_but_layer_2: ["h.2.*"]
      all_but_layer_3: ["h.3.*"]
  - classname: "StochasticHiddenActsReconLoss"

# --- Pretrained model info ---
pretrained_model_class: spd.pretrain.models.gpt2_simple.GPT2Simple
pretrained_model_name: wandb:goodfire/spd/runs/xi36b9az # No ln
output_extract: 0
tokenizer_name: SimpleStories/test-SimpleStories-gpt2-1.25M # We'll load this from wandb in future

# --- Task Specific ---
task_config:
  task_name: lm 
  max_seq_len: 512
  dataset_name: "SimpleStories/SimpleStories"
  column_name: "story"
  train_data_split: "train"
  eval_data_split: "test"

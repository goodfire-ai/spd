C: 1000
batch_size: 256
ci_alive_threshold: 0.0
ci_masked_recon_subset_coeff: null
ci_recon_coeff: null
ci_recon_layerwise_coeff: null
dist_backend: null
eval_batch_size: 64
eval_freq: 1000
eval_metrics:
- classname: CIHistograms
  extra_init_kwargs:
    n_batches_accum: 5
- classname: ComponentActivationDensity
  extra_init_kwargs: {}
- classname: CI_L0
  extra_init_kwargs:
    groups:
      layer_0:
      - model.layers.0.*
      # layer_1:
      # - model.layers.1.*
      # layer_2:
      # - model.layers.2.*
      # layer_3:
      # - model.layers.3.*
      # total:
      # - '*'
- classname: CEandKLLosses
  extra_init_kwargs:
    rounding_threshold: 0.0
- classname: CIMeanPerComponent
  extra_init_kwargs: {}
- classname: SubsetReconstructionLoss
  extra_init_kwargs:
    exclude_patterns:
      # all_but_layer_0:
      # - model.layers.0.*
      # all_but_layer_1:
      # - model.layers.1.*
      # all_but_layer_2:
      # - model.layers.2.*
      # all_but_layer_3:
      # - model.layers.3.*
    include_patterns:
      # attention_only:
      # - '*.self_attn.*'
      # key_only:
      # - '*.self_attn.k_proj'
      layer_0_only:
      - model.layers.0.*
      # layer_1_only:
      # - model.layers.1.*
      # layer_2_only:
      # - model.layers.2.*
      # layer_3_only:
      # - model.layers.3.*
      # mlp_only:
      # - '*.mlp.*'
      # output_only:
      # - '*.self_attn.o_proj'
      # query_only:
      # - '*.self_attn.q_proj'
      # value_only:
      # - '*.self_attn.v_proj'
    n_mask_samples: 1
- classname: FaithfulnessLoss
  extra_init_kwargs: {}
faithfulness_coeff: null
gate_hidden_dims:
- 1000
gate_type: shared_mlp
gradient_accumulation_steps: 1
identity_module_patterns:
# - model.layers.0.mlp.gate_proj
# - model.layers.0.mlp.down_proj
importance_minimality_coeff: 0.003
lr: 0.0005
lr_exponential_halflife: null
lr_schedule: cosine
lr_warmup_pct: 0.0
n_eval_steps: 5
n_examples_until_dead: 1368400
n_mask_samples: 1
out_dir: null
output_loss_type: kl
# p_anneal_end_frac: 0.1
# p_anneal_final_p: 0.9
# p_anneal_start_frac: 0.02
pnorm: 2.0
pgd_mask_enabled: true
pgd_mask_steps: 2
pgd_mask_step_size: 0.5
pretrained_model_class: transformers.LlamaForCausalLM
pretrained_model_name: SimpleStories/SimpleStories-1.25M
pretrained_model_output_attr: logits
pretrained_model_path: null
sampling: continuous
save_freq: null
seed: 0
sigmoid_type: leaky_hard
slow_eval_freq: 2000
slow_eval_on_first_step: true
steps: 50000
stochastic_recon_coeff: 0.1
stochastic_recon_layerwise_coeff: 1.0
stochastic_recon_subset_coeff: null
target_module_patterns:
- model.layers.0.mlp.gate_proj
- model.layers.0.mlp.down_proj
- model.layers.0.mlp.up_proj
- model.layers.0.self_attn.q_proj
- model.layers.0.self_attn.k_proj
- model.layers.0.self_attn.v_proj
- model.layers.0.self_attn.o_proj
task_config:
  buffer_size: 1000
  column_name: story
  dataset_name: SimpleStories/SimpleStories
  eval_data_split: test
  is_tokenized: false
  max_seq_len: 512
  shuffle_each_epoch: true
  streaming: false
  task_name: lm
  train_data_split: train
tokenizer_name: SimpleStories/SimpleStories-1.25M
train_log_freq: 200
use_delta_component: true
wandb_project: spd
wandb_run_name: null
wandb_run_name_prefix: ''
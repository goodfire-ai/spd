# --- WandB ---
wandb_project: oli-spd
# wandb_project: null
wandb_run_name: null
wandb_run_name_prefix: ""

# --- General ---
seed: 0
C: 4096
n_mask_samples: 1
n_ci_mlp_neurons: 16
gate_type: resid_mlp
sigmoid_type: leaky_hard
target_module_patterns: [
  # "model.layers.0.self_attn.q_proj",
  # "model.layers.0.self_attn.k_proj",
  # "model.layers.0.self_attn.v_proj",
  # "model.layers.0.self_attn.o_proj",

  # "model.layers.1.self_attn.q_proj",
  # "model.layers.1.self_attn.k_proj",
  # "model.layers.1.self_attn.v_proj",
  # "model.layers.1.self_attn.o_proj",

  "model.layers.2.self_attn.q_proj",
  "model.layers.2.self_attn.k_proj",
  "model.layers.2.self_attn.v_proj",
  "model.layers.2.self_attn.o_proj",

  # "model.layers.3.self_attn.q_proj",
  # "model.layers.3.self_attn.k_proj",
  # "model.layers.3.self_attn.v_proj",
  # "model.layers.3.self_attn.o_proj",
]

# --- Loss Coefficients ---
faithfulness_coeff: 1.0
recon_coeff: 99999
recon_layerwise_coeff: null
stochastic_recon_coeff: 99999
stochastic_recon_layerwise_coeff: null
stochastic_ablation_coeff: null
importance_minimality_coeff: 99999
schatten_coeff: null
embedding_recon_coeff: null
is_embed_unembed_recon: false
pnorm: anneal-1-2
output_loss_type: kl

# --- Training ---
batch_size: 16
gradient_accumulation_steps: 2
steps: 100_000
lr: 6e-5
lr_schedule: cosine
lr_warmup_pct: 0.01
lr_exponential_halflife: null
n_eval_steps: 100

# --- Logging & Saving ---
image_freq: 2000
image_on_first_step: true
print_freq: 20
save_freq: 5000
log_ce_losses: true

# --- Pretrained model info ---
pretrained_model_class: transformers.LlamaForCausalLM
pretrained_model_name_hf: SimpleStories/SimpleStories-1.25M
pretrained_model_output_attr: logits

# --- Task Specific ---
task_config:
  task_name: lm 
  max_seq_len: 512
  buffer_size: 1000
  dataset_name: "SimpleStories/SimpleStories"
  column_name: "story"
  train_data_split: "train"
  eval_data_split: "test"


# Config details for the target model taken from https://github.com/danbraunai/simple_stories_train/blob/main/simple_stories_train/models/model_configs.py#L54
  # "1.25M": LlamaConfig(
  #     block_size=512,
  #     vocab_size=4096,
  #     n_layer=4,
  #     n_head=4,
  #     n_embd=128,
  #     n_intermediate=128 * 4 * 2 // 3 = 341,
  #     rotary_dim=128 // 4 = 32,
  #     n_ctx=512,
  #     n_key_value_heads=2,
  #     flash_attention=True,
  # ),
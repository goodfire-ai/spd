"""Harvester for collecting component statistics in a single pass.

All accumulator state lives as tensors on `device` (GPU during harvesting, CPU during merge).
"""

from collections.abc import Iterator
from pathlib import Path
from typing import Any

import torch
import tqdm
from einops import einsum, rearrange, reduce, repeat
from jaxtyping import Float, Int
from torch import Tensor

from spd.harvest.reservoir import WINDOW_PAD_SENTINEL, ActivationExamplesReservoir
from spd.harvest.sampling import sample_at_most_n_per_group, top_k_pmi
from spd.harvest.schemas import ActivationExample, ComponentData, ComponentTokenPMI
from spd.log import logger


def extract_firing_windows(
    batch: Int[Tensor, "B S"],
    activation: Float[Tensor, "B S C"],
    component_acts: Float[Tensor, "B S C"],
    batch_idx: Int[Tensor, " n_firings"],
    seq_idx: Int[Tensor, " n_firings"],
    comp_idx: Int[Tensor, " n_firings"],
    context_tokens_per_side: int,
) -> tuple[
    Int[Tensor, "n_firings window_size"],
    Float[Tensor, "n_firings window_size"],
    Float[Tensor, "n_firings window_size"],
]:
    """Extract context windows around firing positions.

    For each firing at (batch_idx[i], seq_idx[i], comp_idx[i]), extracts a window
    of 2*context_tokens_per_side+1 tokens centered on seq_idx[i]. Positions outside
    sequence bounds are padded with WINDOW_PAD_SENTINEL / 0.0.

    Returns (token_windows, activation_windows, act_windows), each [N, W].
    """
    seq_len = batch.shape[1]
    offsets = torch.arange(
        -context_tokens_per_side, context_tokens_per_side + 1, device=batch.device
    )
    window_size = offsets.shape[0]
    assert window_size == 2 * context_tokens_per_side + 1

    window_positions: Int[Tensor, "n_firings window_size"]
    window_positions = seq_idx.unsqueeze(1) + offsets.unsqueeze(0)

    in_bounds = (window_positions >= 0) & (window_positions < seq_len)
    clamped = window_positions.clamp(0, seq_len - 1)

    batch_idx_rep = repeat(batch_idx, "n_firings -> n_firings window_size", window_size=window_size)
    c_idx_rep = repeat(comp_idx, "n_firings -> n_firings window_size", window_size=window_size)

    token_windows = batch[batch_idx_rep, clamped]
    token_windows[~in_bounds] = WINDOW_PAD_SENTINEL

    activation_windows = activation[batch_idx_rep, clamped, c_idx_rep]
    activation_windows[~in_bounds] = 0.0

    act_windows = component_acts[batch_idx_rep, clamped, c_idx_rep]
    act_windows[~in_bounds] = 0.0

    return token_windows, activation_windows, act_windows


class Harvester:
    """Accumulates component statistics in a single pass over data.

    All mutable state is stored as tensors on `device`. Workers on GPU accumulate
    into GPU tensors; the merge job reconstructs on CPU.
    """

    def __init__(
        self,
        layer_names: list[str],
        c_per_layer: dict[str, int],
        vocab_size: int,
        max_examples_per_component: int,
        context_tokens_per_side: int,
        max_examples_per_batch_per_component: int,
        device: torch.device,
        activation_threshold: float | None = None,
        ci_threshold: float | None = None,
        component_keys: list[str] | None = None,
    ):
        threshold = activation_threshold if activation_threshold is not None else ci_threshold
        assert threshold is not None, "activation_threshold (or legacy ci_threshold) is required"
        self.layer_names = layer_names
        self.c_per_layer = c_per_layer
        self.vocab_size = vocab_size
        self.activation_threshold = threshold
        self.max_examples_per_component = max_examples_per_component
        self.context_tokens_per_side = context_tokens_per_side
        self.max_examples_per_batch_per_component = max_examples_per_batch_per_component
        self.device = device

        self.layer_offsets: dict[str, int] = {}
        offset = 0
        for layer in layer_names:
            self.layer_offsets[layer] = offset
            offset += c_per_layer[layer]

        n_components = sum(c_per_layer[layer] for layer in layer_names)
        if component_keys is None:
            self.component_keys = [
                f"{layer}:{comp_idx}"
                for layer in layer_names
                for comp_idx in range(c_per_layer[layer])
            ]
        else:
            assert len(component_keys) == n_components
            self.component_keys = component_keys

        self._flat_component_meta = [
            _component_meta_from_key(key, default_idx=flat_idx)
            for flat_idx, key in enumerate(self.component_keys)
        ]
        window_size = 2 * context_tokens_per_side + 1

        # Per-component firing stats
        self.firing_counts = torch.zeros(n_components, device=device)
        self.activation_sums = torch.zeros(n_components, device=device)
        self.cooccurrence_counts: Float[Tensor, "C C"] = torch.zeros(
            n_components, n_components, device=device, dtype=torch.float32
        )

        # Per-(component, token) stats for PMI computation
        #   input: hard token counts at positions where component fires
        #   output: predicted probability mass at positions where component fires
        self.input_cooccurrence: Int[Tensor, "C vocab"] = torch.zeros(
            n_components, vocab_size, device=device, dtype=torch.long
        )
        self.input_marginals: Int[Tensor, " vocab"] = torch.zeros(
            vocab_size, device=device, dtype=torch.long
        )
        self.output_cooccurrence: Float[Tensor, "C vocab"] = torch.zeros(
            n_components, vocab_size, device=device
        )
        self.output_marginals: Float[Tensor, " vocab"] = torch.zeros(vocab_size, device=device)

        self.reservoir = ActivationExamplesReservoir.create(
            n_components, max_examples_per_component, window_size, device
        )
        self.total_tokens_processed = 0

    # -- Batch processing --------------------------------------------------

    def process_batch(
        self,
        batch: Int[Tensor, "B S"],
        activation: Float[Tensor, "B S C"],
        output_probs: Float[Tensor, "B S V"],
        component_acts: Float[Tensor, "B S C"],
    ) -> None:
        self.total_tokens_processed += batch.numel()

        firing = (activation > self.activation_threshold).float()
        firing_flat = rearrange(firing, "b s c -> (b s) c")
        tokens_flat = rearrange(batch, "b s -> (b s)")
        probs_flat = rearrange(output_probs, "b s v -> (b s) v")

        self.firing_counts += reduce(firing, "b s c -> c", "sum")
        self.activation_sums += reduce(activation, "b s c -> c", "sum")
        self.cooccurrence_counts += einsum(firing_flat, firing_flat, "S c1, S c2 -> c1 c2")
        self._accumulate_token_stats(tokens_flat, probs_flat, firing_flat)
        self._collect_activation_examples(batch, activation, component_acts)

    def _accumulate_token_stats(
        self,
        tokens_flat: Int[Tensor, " S"],
        probs_flat: Float[Tensor, "S vocab"],
        firing_flat: Float[Tensor, "S C"],
    ) -> None:
        n_components = firing_flat.shape[1]
        token_indices = repeat(tokens_flat, "S -> c S", c=n_components)

        # use scatter_add for inputs because inputs are one-hot / token indices
        self.input_cooccurrence.scatter_add_(
            dim=1, index=token_indices, src=rearrange(firing_flat, "S c -> c S").long()
        )
        self.input_marginals.scatter_add_(
            dim=0,
            index=tokens_flat,
            src=torch.ones(tokens_flat.shape[0], device=self.device, dtype=torch.long),
        )

        # however, for outputs we need to accumulate probability mass over vocab
        self.output_cooccurrence += einsum(firing_flat, probs_flat, "S c, S v -> c v")
        self.output_marginals += reduce(probs_flat, "S v -> v", "sum")

    def _collect_activation_examples(
        self,
        batch: Int[Tensor, "B S"],
        activation: Float[Tensor, "B S C"],
        component_acts: Float[Tensor, "B S C"],
    ) -> None:
        batch_idx, seq_idx, comp_idx = torch.where(activation > self.activation_threshold)
        if len(batch_idx) == 0:
            return

        keep = sample_at_most_n_per_group(comp_idx, self.max_examples_per_batch_per_component)
        batch_idx, seq_idx, comp_idx = batch_idx[keep], seq_idx[keep], comp_idx[keep]

        token_w, activation_w, act_w = extract_firing_windows(
            batch,
            activation,
            component_acts,
            batch_idx,
            seq_idx,
            comp_idx,
            self.context_tokens_per_side,
        )
        self.reservoir.add(comp_idx, token_w, activation_w, act_w)

    def save(self, path: Path) -> None:
        data: dict[str, object] = {
            "layer_names": self.layer_names,
            "c_per_layer": self.c_per_layer,
            "component_keys": self.component_keys,
            "vocab_size": self.vocab_size,
            "activation_threshold": self.activation_threshold,
            "ci_threshold": self.activation_threshold,
            "max_examples_per_component": self.max_examples_per_component,
            "context_tokens_per_side": self.context_tokens_per_side,
            "max_examples_per_batch_per_component": self.max_examples_per_batch_per_component,
            "total_tokens_processed": self.total_tokens_processed,
            "reservoir": self.reservoir.state_dict(),
            "firing_counts": self.firing_counts.cpu(),
            "activation_sums": self.activation_sums.cpu(),
            "ci_sums": self.activation_sums.cpu(),
            "cooccurrence_counts": self.cooccurrence_counts.cpu(),
            "input_cooccurrence": self.input_cooccurrence.cpu(),
            "input_marginals": self.input_marginals.cpu(),
            "output_cooccurrence": self.output_cooccurrence.cpu(),
            "output_marginals": self.output_marginals.cpu(),
        }
        torch.save(data, path)

    @staticmethod
    def load(path: Path, device: torch.device) -> "Harvester":
        d: dict[str, Any] = torch.load(path, weights_only=False)
        activation_threshold = d.get("activation_threshold", d.get("ci_threshold"))
        assert activation_threshold is not None
        h = Harvester(
            layer_names=d["layer_names"],
            c_per_layer=d["c_per_layer"],
            vocab_size=d["vocab_size"],
            activation_threshold=activation_threshold,
            max_examples_per_component=d["max_examples_per_component"],
            context_tokens_per_side=d["context_tokens_per_side"],
            max_examples_per_batch_per_component=d.get("max_examples_per_batch_per_component", 5),
            device=device,
            component_keys=d.get("component_keys"),
        )
        h.total_tokens_processed = d["total_tokens_processed"]
        h.firing_counts = d["firing_counts"].to(device)
        activation_sums = d.get("activation_sums", d.get("ci_sums"))
        assert activation_sums is not None
        h.activation_sums = activation_sums.to(device)
        h.cooccurrence_counts = d["cooccurrence_counts"].to(device)
        h.input_cooccurrence = d["input_cooccurrence"].to(device)
        h.input_marginals = d["input_marginals"].to(device)
        h.output_cooccurrence = d["output_cooccurrence"].to(device)
        h.output_marginals = d["output_marginals"].to(device)
        h.reservoir = ActivationExamplesReservoir.from_state_dict(d["reservoir"], device)
        return h

    def merge(self, other: "Harvester") -> None:
        assert other.layer_names == self.layer_names
        assert other.c_per_layer == self.c_per_layer
        assert other.component_keys == self.component_keys
        assert other.vocab_size == self.vocab_size
        assert other.activation_threshold == self.activation_threshold

        self.firing_counts += other.firing_counts
        self.activation_sums += other.activation_sums
        self.cooccurrence_counts += other.cooccurrence_counts
        self.input_cooccurrence += other.input_cooccurrence
        self.input_marginals += other.input_marginals
        self.output_cooccurrence += other.output_cooccurrence
        self.output_marginals += other.output_marginals
        self.total_tokens_processed += other.total_tokens_processed

        self.reservoir.merge(other.reservoir)

    # -- Result building ---------------------------------------------------

    def build_results(self, pmi_top_k_tokens: int) -> Iterator[ComponentData]:
        """Yield ComponentData objects one at a time (constant memory)."""
        logger.info("  Moving tensors to CPU...")
        mean_activation = (self.activation_sums / self.total_tokens_processed).cpu()
        firing_counts = self.firing_counts.cpu()
        input_cooccurrence = self.input_cooccurrence.cpu()
        input_marginals = self.input_marginals.cpu()
        output_cooccurrence = self.output_cooccurrence.cpu()
        output_marginals = self.output_marginals.cpu()

        reservoir_cpu = self.reservoir.to(torch.device("cpu"))

        _log_base_rate_summary(firing_counts, input_marginals)

        n_total = len(self.component_keys)
        logger.info(
            f"  Computing stats for {n_total} components across {len(self.layer_names)} layers..."
        )
        for flat_idx in tqdm.tqdm(range(n_total), desc="Building components"):
            n_firings = float(firing_counts[flat_idx])
            if n_firings == 0:
                continue

            layer_name, comp_idx, component_key = self._flat_component_meta[flat_idx]
            examples = [
                ActivationExample(
                    token_ids=toks.tolist(),
                    activation_values=activation_vals.tolist(),
                    component_acts=acts.tolist(),
                )
                for toks, activation_vals, acts in reservoir_cpu.examples(flat_idx)
            ]

            yield ComponentData(
                component_key=component_key,
                layer=layer_name,
                component_idx=comp_idx,
                mean_activation=float(mean_activation[flat_idx]),
                activation_examples=examples,
                input_token_pmi=_compute_token_pmi(
                    input_cooccurrence[flat_idx],
                    input_marginals,
                    n_firings,
                    self.total_tokens_processed,
                    pmi_top_k_tokens,
                ),
                output_token_pmi=_compute_token_pmi(
                    output_cooccurrence[flat_idx],
                    output_marginals,
                    n_firings,
                    self.total_tokens_processed,
                    pmi_top_k_tokens,
                ),
            )

    @property
    def ci_threshold(self) -> float:
        """Backward-compatible alias for activation_threshold."""
        return self.activation_threshold

    @ci_threshold.setter
    def ci_threshold(self, value: float) -> None:
        self.activation_threshold = value

    @property
    def ci_sums(self) -> Tensor:
        """Backward-compatible alias for activation_sums."""
        return self.activation_sums

    @ci_sums.setter
    def ci_sums(self, value: Tensor) -> None:
        self.activation_sums = value


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------


def _log_base_rate_summary(firing_counts: Tensor, input_marginals: Tensor) -> None:
    active_counts = firing_counts[firing_counts > 0]
    if len(active_counts) == 0:
        logger.info("  WARNING: No components fired above threshold!")
        return

    sorted_counts = active_counts.sort().values
    n_active = len(active_counts)
    logger.info("\n  === Base Rate Summary ===")
    logger.info(f"  Components with firings: {n_active} / {len(firing_counts)}")
    logger.info(
        f"  Firing counts - min: {int(sorted_counts[0])}, "
        f"median: {int(sorted_counts[n_active // 2])}, "
        f"max: {int(sorted_counts[-1])}"
    )

    LOW_FIRING_THRESHOLD = 100
    n_sparse = int((active_counts < LOW_FIRING_THRESHOLD).sum())
    if n_sparse > 0:
        logger.info(
            f"  WARNING: {n_sparse} components have <{LOW_FIRING_THRESHOLD} firings "
            f"(stats may be noisy)"
        )

    active_tokens = input_marginals[input_marginals > 0]
    sorted_token_counts = active_tokens.sort().values
    n_tokens = len(active_tokens)
    logger.info(
        f"  Tokens seen: {n_tokens} unique, "
        f"occurrences - min: {int(sorted_token_counts[0])}, "
        f"median: {int(sorted_token_counts[n_tokens // 2])}, "
        f"max: {int(sorted_token_counts[-1])}"
    )

    RARE_TOKEN_THRESHOLD = 10
    n_rare = int((active_tokens < RARE_TOKEN_THRESHOLD).sum())
    if n_rare > 0:
        logger.info(
            f"  Note: {n_rare} tokens have <{RARE_TOKEN_THRESHOLD} occurrences "
            f"(high precision/recall with these may be spurious)"
        )
    logger.info("")


def _compute_token_pmi(
    token_mass_for_component: Tensor,
    token_mass_totals: Tensor,
    component_firing_count: float,
    total_tokens: int,
    top_k: int,
) -> ComponentTokenPMI:
    top, bottom = top_k_pmi(
        cooccurrence_counts=token_mass_for_component,
        marginal_counts=token_mass_totals,
        target_count=component_firing_count,
        total_count=total_tokens,
        top_k=top_k,
    )
    return ComponentTokenPMI(top=top, bottom=bottom)


def _component_meta_from_key(key: str, default_idx: int) -> tuple[str, int, str]:
    """Parse component key into (layer, component_idx, key) with fallback."""
    if ":" in key:
        layer, idx_str = key.rsplit(":", 1)
        if idx_str.isdigit():
            return layer, int(idx_str), key
    return key, default_idx, key

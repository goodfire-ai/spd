"""Generate a PDF report showcasing autointerp results for a run.

Usage:
    python -m spd.autointerp.scripts.generate_report <run_id>
"""

import json
import random
from pathlib import Path
from typing import Any

import markdown
import numpy as np
from weasyprint import HTML

from spd.settings import SPD_OUT_DIR


def generate_report(run_id: str, output_path: Path | None = None) -> Path:
    random.seed(42)

    autointerp_dir = SPD_OUT_DIR / "autointerp" / run_id

    # Load latest interpretation results
    results_files = sorted(autointerp_dir.glob("results_*.jsonl"))
    assert results_files, f"No interpretation results found in {autointerp_dir}"
    interp_results: list[dict[str, str]] = []
    with open(results_files[-1]) as f:
        for line in f:
            interp_results.append(json.loads(line))

    high = [r for r in interp_results if r["confidence"] == "high"]
    med = [r for r in interp_results if r["confidence"] == "medium"]
    low = [r for r in interp_results if r["confidence"] == "low"]

    # Try loading intruder eval results
    intruder_dir = autointerp_dir / "eval" / "intruder"
    if not intruder_dir.exists():
        intruder_dir = autointerp_dir / "scoring" / "intruder"
    intruder_results: list[dict[str, Any]] = []
    intruder_files = sorted(intruder_dir.glob("results_*.jsonl")) if intruder_dir.exists() else []
    if intruder_files:
        with open(intruder_files[-1]) as f:
            for line in f:
                intruder_results.append(json.loads(line))

    # Get a full prompt example
    prompt_example = random.choice(high) if high else random.choice(interp_results)

    # Build markdown
    md = f"""
# SPD Autointerp Report

**Run:** `{run_id}`
**Components interpreted:** {len(interp_results):,}

---

## Interpretation Confidence Breakdown

| Confidence | Count | Percentage |
|---|---|---|
| High | {len(high):,} | {len(high) / len(interp_results) * 100:.0f}% |
| Medium | {len(med):,} | {len(med) / len(interp_results) * 100:.0f}% |
| Low | {len(low):,} | {len(low) / len(interp_results) * 100:.0f}% |

---

## High-Confidence Examples

"""
    for r in random.sample(high, min(8, len(high))):
        md += f"**`{r['component_key']}`** -- {r['label']}\n\n"
        md += f"> {r['reasoning']}\n\n"

    md += "## Medium-Confidence Examples\n\n"
    for r in random.sample(med, min(4, len(med))):
        md += f"**`{r['component_key']}`** -- {r['label']}\n\n"
        md += f"> {r['reasoning']}\n\n"

    md += "## Low-Confidence Examples\n\n"
    for r in random.sample(low, min(3, len(low))):
        md += f"**`{r['component_key']}`** -- {r['label']}\n\n"
        md += f"> {r['reasoning']}\n\n"

    md += f"""---

## Example: Full Prompt and Response

**Component:** `{prompt_example["component_key"]}`

### Prompt (truncated)

```
{prompt_example["prompt"][:3500]}
...
```

### Response

```json
{{
  "label": "{prompt_example["label"]}",
  "confidence": "{prompt_example["confidence"]}",
  "reasoning": "{prompt_example["reasoning"]}"
}}
```

"""

    # Intruder eval section
    if intruder_results:
        scores = [float(r["score"]) for r in intruder_results if int(r["n_trials"]) > 0]
        md += f"""---

## Intruder Detection Eval

Tests component coherence: 4 real examples + 1 intruder from a different component.
Random baseline = 20%.

**{len(scores)} components scored**

| Metric | Value |
|---|---|
| Mean accuracy | {np.mean(scores) * 100:.1f}% |
| Median accuracy | {np.median(scores) * 100:.1f}% |
| Random baseline | 20.0% |

### Score Distribution

| Coherence Level | Count |
|---|---|
"""
        bins = [
            (0, 0.2, "Incoherent (<20%)"),
            (0.2, 0.4, "Weak (20-40%)"),
            (0.4, 0.6, "Moderate (40-60%)"),
            (0.6, 0.8, "Good (60-80%)"),
            (0.8, 1.01, "Excellent (80-100%)"),
        ]
        for lo, hi, label in bins:
            count = sum(1 for s in scores if lo <= s < hi)
            md += f"| {label} | {count} |\n"

    md += "\n\n---\n\n*Generated by `spd.autointerp.scripts.generate_report`*\n"

    # Convert to PDF
    html_body = markdown.markdown(md, extensions=["tables", "fenced_code"])
    html_full = f"""<!DOCTYPE html>
<html><head><style>
  @page {{ margin: 2cm; size: A4; }}
  body {{ font-family: -apple-system, 'Segoe UI', Helvetica, Arial, sans-serif;
         font-size: 11pt; line-height: 1.5; color: #1a1a1a; }}
  h1 {{ font-size: 22pt; border-bottom: 2px solid #333; padding-bottom: 8px; margin-top: 0; }}
  h2 {{ font-size: 16pt; color: #2c3e50; border-bottom: 1px solid #ddd; padding-bottom: 4px; }}
  h3 {{ font-size: 13pt; color: #34495e; }}
  table {{ border-collapse: collapse; width: 100%; margin: 12px 0; font-size: 10pt; }}
  th, td {{ border: 1px solid #ddd; padding: 6px 10px; text-align: left; }}
  th {{ background: #f5f5f5; font-weight: 600; }}
  tr:nth-child(even) {{ background: #fafafa; }}
  code {{ background: #f4f4f4; padding: 1px 4px; border-radius: 3px; font-size: 9.5pt; }}
  pre {{ background: #f8f8f8; padding: 12px; border-radius: 4px; overflow-x: auto;
         font-size: 8.5pt; line-height: 1.4; border: 1px solid #e0e0e0; }}
  pre code {{ background: none; padding: 0; }}
  blockquote {{ border-left: 3px solid #3498db; margin: 8px 0; padding: 4px 12px;
               color: #555; font-size: 10pt; background: #f9fbfd; }}
  hr {{ border: none; border-top: 1px solid #ccc; margin: 20px 0; }}
</style></head><body>{html_body}</body></html>"""

    if output_path is None:
        output_path = Path(f"autointerp_report_{run_id}.pdf")
    HTML(string=html_full).write_pdf(str(output_path))
    print(f"Report written to {output_path}")
    return output_path


if __name__ == "__main__":
    import fire

    fire.Fire(generate_report)

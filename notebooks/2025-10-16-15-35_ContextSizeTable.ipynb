{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e0a50fa",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/polished-lake/home/braun/spd/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pydantic\")\n",
    "\n",
    "from spd.data import DatasetConfig, create_data_loader\n",
    "from spd.experiments.lm.configs import LMTaskConfig\n",
    "from spd.models.component_model import ComponentModel, SPDRunInfo\n",
    "from spd.utils.distributed_utils import get_device\n",
    "from spd.utils.general_utils import replace_pydantic_model, set_seed\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "set_seed(0)\n",
    "device = get_device()\n",
    "\n",
    "# Load model\n",
    "run_info = SPDRunInfo.from_path(\"wandb:goodfire/spd/runs/9d313yrl\")\n",
    "config = run_info.config\n",
    "model = ComponentModel.from_run_info(run_info)\n",
    "model.to(device)\n",
    "model.target_model.requires_grad_(False)\n",
    "model.eval()\n",
    "\n",
    "print(\"Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c22b5afb",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function defined\n"
     ]
    }
   ],
   "source": [
    "def evaluate_loss_with_position_tracking(n_ctx, n_batches=10, batch_size=32):\n",
    "    set_seed(0)\n",
    "    \n",
    "    task_config = replace_pydantic_model(\n",
    "        config.task_config, \n",
    "        {\"max_seq_len\": n_ctx, \"train_data_split\": \"train[:5000]\"}\n",
    "    )\n",
    "    \n",
    "    data_config = DatasetConfig(\n",
    "        name=task_config.dataset_name,\n",
    "        hf_tokenizer_path=config.tokenizer_name,\n",
    "        split=task_config.train_data_split,\n",
    "        n_ctx=task_config.max_seq_len,\n",
    "        is_tokenized=task_config.is_tokenized,\n",
    "        streaming=task_config.streaming,\n",
    "        column_name=task_config.column_name,\n",
    "        shuffle_each_epoch=task_config.shuffle_each_epoch,\n",
    "        seed=0,\n",
    "    )\n",
    "    \n",
    "    data_loader, _tokenizer = create_data_loader(\n",
    "        dataset_config=data_config,\n",
    "        batch_size=batch_size,\n",
    "        buffer_size=task_config.buffer_size,\n",
    "        global_seed=0,\n",
    "        ddp_rank=0,\n",
    "        ddp_world_size=1,\n",
    "    )\n",
    "    \n",
    "    position_losses = torch.zeros(n_ctx - 1, device=device)\n",
    "    position_counts = torch.zeros(n_ctx - 1, device=device)\n",
    "    batch_losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            if i >= n_batches:\n",
    "                break\n",
    "            \n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            output = model.target_model(input_ids)\n",
    "            \n",
    "            if hasattr(output, 'logits'):\n",
    "                logits = output.logits\n",
    "            else:\n",
    "                logits = output\n",
    "            \n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = input_ids[:, 1:].contiguous()\n",
    "            \n",
    "            loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "            per_token_loss = loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                shift_labels.view(-1)\n",
    "            )\n",
    "            \n",
    "            per_token_loss = per_token_loss.view(shift_logits.shape[0], shift_logits.shape[1])\n",
    "            position_losses[:n_ctx-1] += per_token_loss.sum(dim=0)\n",
    "            position_counts[:n_ctx-1] += per_token_loss.shape[0]\n",
    "            batch_losses.append(per_token_loss.mean().item())\n",
    "    \n",
    "    avg_position_losses = (position_losses / position_counts.clamp(min=1)).cpu().numpy()\n",
    "    \n",
    "    return {\n",
    "        'mean_loss': np.mean(batch_losses),\n",
    "        'std_loss': np.std(batch_losses),\n",
    "        'min_loss': np.min(batch_losses),\n",
    "        'max_loss': np.max(batch_losses),\n",
    "        'losses_per_position': avg_position_losses,\n",
    "        'num_predictions': n_ctx - 1,\n",
    "        'input_seq_len': n_ctx\n",
    "    }\n",
    "\n",
    "print(\"Function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99144176",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [],
   "source": [
    "# Run analysis for all context sizes\n",
    "context_sizes = [8, 32, 128, 505, 511, 512, 513, 515, 550, 800]\n",
    "results = {}\n",
    "\n",
    "for n_ctx in context_sizes:\n",
    "    result = evaluate_loss_with_position_tracking(n_ctx, n_batches=10, batch_size=32)\n",
    "    results[n_ctx] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4ea1217",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "COMPREHENSIVE SUMMARY: Target Model Loss vs Context Size (With Position Tracking)\n",
      "==========================================================================================\n",
      " input_seq_len predicts_positions  num_predictions  mean_loss  std_loss  min_loss  max_loss\n",
      "             8                1-7                7   3.432429  0.170315  3.158330  3.762946\n",
      "            32               1-31               31   2.735031  0.084271  2.597227  2.897082\n",
      "           128              1-127              127   2.451201  0.043478  2.362967  2.514810\n",
      "           505              1-504              504   2.326909  0.026657  2.290959  2.374750\n",
      "           511              1-510              510   2.336479  0.046317  2.275845  2.423364\n",
      "           512              1-511              511   2.325966  0.042399  2.248553  2.391512\n",
      "           513              1-512              512   2.342385  0.030618  2.265809  2.392439\n",
      "           515              1-514              514   2.338477  0.019998  2.309671  2.381780\n",
      "           550              1-549              549   2.345897  0.050434  2.281770  2.445718\n",
      "           800              1-799              799   2.910516  0.036985  2.859410  2.965678\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create the comprehensive summary table\n",
    "summary_data = []\n",
    "for n_ctx, result in results.items():\n",
    "    summary_data.append({\n",
    "        'input_seq_len': n_ctx,\n",
    "        'predicts_positions': f\"1-{n_ctx-1}\",\n",
    "        'num_predictions': n_ctx - 1,\n",
    "        'mean_loss': result['mean_loss'],\n",
    "        'std_loss': result['std_loss'],\n",
    "        'min_loss': result['min_loss'],\n",
    "        'max_loss': result['max_loss']\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(summary_data)\n",
    "df = df.sort_values('input_seq_len')\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"COMPREHENSIVE SUMMARY: Target Model Loss vs Context Size (With Position Tracking)\")\n",
    "print(\"=\" * 90)\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\" * 90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scribe: 2025-10-16-15-35_ContextSizeTable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

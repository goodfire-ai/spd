========================================
Eval Debug Tests - Mon Feb  9 19:40:19 UTC 2026
Host: h200-reserved-145-003
GPUs: NVIDIA H200 x 8
========================================

========================================
Starting test: 00_no_eval_metrics
Time: Mon Feb  9 19:40:19 UTC 2026
Config: /mnt/polished-lake/home/braun/spd/eval_debug/configs/00_no_eval_metrics.yaml
========================================

*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=0, local_rank=0, device=device(type='cuda', index=0)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=3, local_rank=3, device=device(type='cuda', index=3)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=2, local_rank=2, device=device(type='cuda', index=2)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=1, local_rank=1, device=device(type='cuda', index=1)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=6, local_rank=6, device=device(type='cuda', index=6)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=7, local_rank=7, device=device(type='cuda', index=7)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=5, local_rank=5, device=device(type='cuda', index=5)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=4, local_rank=4, device=device(type='cuda', index=4)
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 34745
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 34745
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 34745
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 34745
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 34745
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 34745
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 34745
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 34745
Distributed state: DistributedState(rank=4, world_size=8, local_rank=4, backend='nccl')
Distributed state: DistributedState(rank=6, world_size=8, local_rank=6, backend='nccl')
Distributed state: DistributedState(rank=2, world_size=8, local_rank=2, backend='nccl')
Distributed state: DistributedState(rank=5, world_size=8, local_rank=5, backend='nccl')
Distributed state: DistributedState(rank=7, world_size=8, local_rank=7, backend='nccl')
Distributed state: DistributedState(rank=1, world_size=8, local_rank=1, backend='nccl')
Distributed state: DistributedState(rank=3, world_size=8, local_rank=3, backend='nccl')
Distributed state: DistributedState(rank=0, world_size=8, local_rank=0, backend='nccl')
Using current branch: dev (uncommitted changes, no commit hash)
Run ID: s-46da508f
Output directory: /mnt/polished-lake/artifacts/mechanisms/spd/spd/s-46da508f
wandb_project=None wandb_run_name=None wandb_run_name_prefix='' seed=0 autocast_bf16=True n_mask_samples=1 ci_config=GlobalCiConfig(mode='global', fn_type='global_shared_transformer', hidden_dims=None, reader_hidden_dims=None, d_resid_ci_fn=None, block_groups=None, transition_attn_config=None, transition_hidden_dim=None, simple_transformer_ci_cfg=GlobalSharedTransformerCiConfig(d_model=1024, n_blocks=8, mlp_hidden_dim=[8192], attn_config=AttnConfig(n_heads=16, max_len=512, rope_base=10000.0))) sampling='continuous' sigmoid_type='leaky_hard' module_info=[ModulePatternInfoConfig(module_pattern='h.*.mlp.c_fc', C=4096), ModulePatternInfoConfig(module_pattern='h.*.mlp.down_proj', C=3072), ModulePatternInfoConfig(module_pattern='h.*.attn.q_proj', C=512), ModulePatternInfoConfig(module_pattern='h.*.attn.k_proj', C=256), ModulePatternInfoConfig(module_pattern='h.*.attn.v_proj', C=1024), ModulePatternInfoConfig(module_pattern='h.*.attn.o_proj', C=1024)] identity_module_info=None use_delta_component=True loss_metric_configs=[ImportanceMinimalityLossConfig(coeff=0.0005, classname='ImportanceMinimalityLoss', pnorm=2.0, beta=0.1, p_anneal_start_frac=0.0, p_anneal_final_p=0.4, p_anneal_end_frac=1.0, eps=1e-12), StochasticReconSubsetLossConfig(coeff=0.5, classname='StochasticReconSubsetLoss', routing=UniformKSubsetRoutingConfig(type='uniform_k_subset')), PersistentPGDReconSubsetLossConfig(coeff=0.5, classname='PersistentPGDReconSubsetLoss', optimizer=AdamPGDConfig(type='adam', lr=0.1, beta1=0.9, beta2=0.99, eps=1e-08), scope=BatchInvariantScope(type='batch_invariant', n_masks=8), routing=UniformKSubsetRoutingConfig(type='uniform_k_subset')), FaithfulnessLossConfig(coeff=1000000.0, classname='FaithfulnessLoss')] output_loss_type='kl' lr_schedule=ScheduleConfig(start_val=0.0001, warmup_pct=0.0, final_val_frac=0.1, fn_type='cosine') steps=5 batch_size=64 gradient_accumulation_steps=1 grad_clip_norm_components=0.01 grad_clip_norm_ci_fns=None faithfulness_warmup_steps=400 faithfulness_warmup_lr=0.001 faithfulness_warmup_weight_decay=0.0 train_log_freq=200 eval_freq=1000 eval_batch_size=256 slow_eval_freq=10000 n_eval_steps=1 slow_eval_on_first_step=True save_freq=None eval_metric_configs=[] ci_alive_threshold=0.0 pretrained_model_class='spd.pretrain.models.llama_simple_mlp.LlamaSimpleMLP' pretrained_model_path=None pretrained_model_name='wandb:goodfire/spd/t-32d1bb3b' pretrained_model_output_attr='idx_0' tokenizer_name='EleutherAI/gpt-neox-20b' task_config=LMTaskConfig(task_name='lm', max_seq_len=512, buffer_size=1000, dataset_name='danbraunai/pile-uncopyrighted-tok', column_name='input_ids', train_data_split='train', eval_data_split='val', shuffle_each_epoch=True, is_tokenized=True, streaming=True)
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading dataset...
Starting optimization...
Train+eval logs saved to directory: /mnt/polished-lake/artifacts/mechanisms/spd/spd/s-46da508f
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
  0% 0/6 [00:00<?, ?it/s]                         --- Step 0 ---
  0% 0/6 [00:01<?, ?it/s]                         LR: 0.000100
  0% 0/6 [00:01<?, ?it/s]                         train/loss/ImportanceMinimalityLoss: 36238.726562500000000
  0% 0/6 [00:01<?, ?it/s]                         train/loss/StochasticReconSubsetLoss: 2.963917255401611
  0% 0/6 [00:01<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss: 4.808569431304932
  0% 0/6 [00:01<?, ?it/s]                         train/loss/FaithfulnessLoss: 0.000040523682401
  0% 0/6 [00:01<?, ?it/s]                         train/loss/total: 62.529289245605469
  0% 0/6 [00:01<?, ?it/s]                         train/l0/h.0.attn.k_proj: 130.421875000000000
  0% 0/6 [00:01<?, ?it/s]                         train/l0/h.0.attn.o_proj: 507.907653808593750
  0% 0/6 [00:01<?, ?it/s]                         train/l0/h.0.attn.q_proj: 258.663482666015625
  0% 0/6 [00:01<?, ?it/s]                         train/l0/h.0.attn.v_proj: 505.962799072265625
  0% 0/6 [00:01<?, ?it/s]                         train/l0/h.0.mlp.c_fc: 2047.188476562500000
  0% 0/6 [00:01<?, ?it/s]                         train/l0/h.0.mlp.down_proj: 1536.454833984375000
  0% 0/6 [00:01<?, ?it/s]                         train/l0/h.1.attn.k_proj: 127.785308837890625
  0% 0/6 [00:01<?, ?it/s]                         train/l0/h.1.attn.o_proj: 511.397613525390625
  0% 0/6 [00:01<?, ?it/s]                         train/l0/h.1.attn.q_proj: 267.274322509765625
  0% 0/6 [00:01<?, ?it/s]                         train/l0/h.1.attn.v_proj: 508.254882812500000
  0% 0/6 [00:01<?, ?it/s]                         train/l0/h.1.mlp.c_fc: 2051.299804687500000
  0% 0/6 [00:01<?, ?it/s]                         train/l0/h.1.mlp.down_proj: 1535.686523437500000
  0% 0/6 [00:01<?, ?it/s]                         train/l0/h.2.attn.k_proj: 132.868255615234375
  0% 0/6 [00:01<?, ?it/s]                         train/l0/h.2.attn.o_proj: 513.306762695312500
  0% 0/6 [00:01<?, ?it/s]                         train/l0/h.2.attn.q_proj: 243.575714111328125
  0% 0/6 [00:01<?, ?it/s]                         train/l0/h.2.attn.v_proj: 510.212829589843750
  0% 0/6 [00:01<?, ?it/s]                         train/l0/h.2.mlp.c_fc: 2057.168945312500000
  0% 0/6 [00:01<?, ?it/s]                         train/l0/h.2.mlp.down_proj: 1548.709106445312500
  0% 0/6 [00:01<?, ?it/s]                         train/l0/h.3.attn.k_proj: 130.091400146484375
  0% 0/6 [00:01<?, ?it/s]                         train/l0/h.3.attn.o_proj: 516.913940429687500
  0% 0/6 [00:01<?, ?it/s]                         train/l0/h.3.attn.q_proj: 259.873718261718750
  0% 0/6 [00:01<?, ?it/s]                         train/l0/h.3.attn.v_proj: 517.953979492187500
  0% 0/6 [00:01<?, ?it/s]                         train/l0/h.3.mlp.c_fc: 2040.286743164062500
  0% 0/6 [00:01<?, ?it/s]                         train/l0/h.3.mlp.down_proj: 1540.889404296875000
  0% 0/6 [00:01<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.mlp.c_fc: 0.097107894718647
  0% 0/6 [00:01<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.mlp.c_fc: 0.097029037773609
  0% 0/6 [00:01<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.mlp.c_fc: 0.096681430935860
  0% 0/6 [00:01<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.mlp.c_fc: 0.096843123435974
  0% 0/6 [00:01<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.mlp.down_proj: 0.096805348992348
  0% 0/6 [00:01<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.mlp.down_proj: 0.096881560981274
  0% 0/6 [00:01<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.mlp.down_proj: 0.096580810844898
  0% 0/6 [00:01<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.mlp.down_proj: 0.096301153302193
  0% 0/6 [00:01<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.attn.q_proj: 0.096449606120586
  0% 0/6 [00:01<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.attn.q_proj: 0.093979999423027
  0% 0/6 [00:01<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.attn.q_proj: 0.095499560236931
  0% 0/6 [00:01<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.attn.q_proj: 0.095262818038464
  0% 0/6 [00:01<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.attn.k_proj: 0.095392063260078
  0% 0/6 [00:01<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.attn.k_proj: 0.095359392464161
  0% 0/6 [00:01<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.attn.k_proj: 0.095940053462982
  0% 0/6 [00:01<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.attn.k_proj: 0.095890298485756
  0% 0/6 [00:01<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.attn.v_proj: 0.096610367298126
  0% 0/6 [00:01<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.attn.v_proj: 0.095834366977215
  0% 0/6 [00:01<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.attn.v_proj: 0.096549026668072
  0% 0/6 [00:01<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.attn.v_proj: 0.095930404961109
  0% 0/6 [00:01<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.attn.o_proj: 0.096996203064919
  0% 0/6 [00:01<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.attn.o_proj: 0.096441082656384
  0% 0/6 [00:01<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.attn.o_proj: 0.096836693584919
  0% 0/6 [00:01<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.attn.o_proj: 0.096477016806602
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.0.mlp.c_fc.V: 1.415825366973877
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.0.mlp.c_fc.U: 3.596897602081299
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.1.mlp.c_fc.V: 0.923466324806213
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.1.mlp.c_fc.U: 2.407195091247559
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.2.mlp.c_fc.V: 0.649173617362976
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.2.mlp.c_fc.U: 1.526712059974670
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.3.mlp.c_fc.V: 0.413524925708771
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.3.mlp.c_fc.U: 0.888019323348999
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.0.mlp.down_proj.V: 1.526773691177368
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.0.mlp.down_proj.U: 1.446317434310913
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.1.mlp.down_proj.V: 0.778590619564056
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.1.mlp.down_proj.U: 0.740494549274445
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.2.mlp.down_proj.V: 0.535984456539154
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.2.mlp.down_proj.U: 0.497253209352493
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.3.mlp.down_proj.V: 0.508944153785706
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.3.mlp.down_proj.U: 0.434355825185776
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.0.attn.q_proj.V: 0.260735183954239
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.0.attn.q_proj.U: 0.279456675052643
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.1.attn.q_proj.V: 0.225697919726372
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.1.attn.q_proj.U: 0.233237668871880
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.2.attn.q_proj.V: 0.184147357940674
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.2.attn.q_proj.U: 0.192132458090782
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.3.attn.q_proj.V: 0.184873670339584
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.3.attn.q_proj.U: 0.191540241241455
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.0.attn.k_proj.V: 0.261168509721756
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.0.attn.k_proj.U: 0.413004040718079
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.1.attn.k_proj.V: 0.230889737606049
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.1.attn.k_proj.U: 0.326825737953186
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.2.attn.k_proj.V: 0.204294517636299
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.2.attn.k_proj.U: 0.264139592647552
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.3.attn.k_proj.V: 0.184091627597809
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.3.attn.k_proj.U: 0.240563303232193
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.0.attn.v_proj.V: 0.861580789089203
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.0.attn.v_proj.U: 1.049399375915527
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.1.attn.v_proj.V: 0.969275414943695
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.1.attn.v_proj.U: 1.084967374801636
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.2.attn.v_proj.V: 0.757885396480560
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.2.attn.v_proj.U: 0.818872749805450
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.3.attn.v_proj.V: 0.442526072263718
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.3.attn.v_proj.U: 0.461256027221680
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.0.attn.o_proj.V: 0.995954155921936
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.0.attn.o_proj.U: 1.263409733772278
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.1.attn.o_proj.V: 0.467842549085617
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.1.attn.o_proj.U: 0.599913120269775
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.2.attn.o_proj.V: 0.459928691387177
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.2.attn.o_proj.U: 0.572499155998230
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.3.attn.o_proj.V: 0.329448997974396
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/components/h.3.attn.o_proj.U: 0.433251380920410
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._input_projector.W: 9.018929481506348
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._input_projector.b: 0.119211040437222
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._output_head.W: 2.372462272644043
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._output_head.b: 0.038158148527145
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.attn.q_proj.weight: 0.014386421069503
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.attn.k_proj.weight: 0.015054508112371
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.attn.v_proj.weight: 0.857673346996307
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.attn.out_proj.weight: 0.857142686843872
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.mlp.0.W: 0.820418953895569
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.mlp.0.b: 0.055864088237286
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.mlp.2.W: 5.222486495971680
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.mlp.2.b: 0.100813008844852
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.attn.q_proj.weight: 0.010587261058390
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.attn.k_proj.weight: 0.010920211672783
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.attn.v_proj.weight: 0.840065658092499
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.attn.out_proj.weight: 0.855740189552307
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.mlp.0.W: 0.814605116844177
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.mlp.0.b: 0.049388583749533
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.mlp.2.W: 4.870756626129150
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.mlp.2.b: 0.090884260833263
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.attn.q_proj.weight: 0.010306921787560
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.attn.k_proj.weight: 0.010416929610074
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.attn.v_proj.weight: 0.817939460277557
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.attn.out_proj.weight: 0.831115126609802
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.mlp.0.W: 0.789797067642212
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.mlp.0.b: 0.046137277036905
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.mlp.2.W: 4.554909229278564
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.mlp.2.b: 0.083077073097229
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.attn.q_proj.weight: 0.008916009217501
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.attn.k_proj.weight: 0.008408317342401
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.attn.v_proj.weight: 0.800790131092072
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.attn.out_proj.weight: 0.802278697490692
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.mlp.0.W: 0.773106217384338
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.mlp.0.b: 0.043396271765232
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.mlp.2.W: 4.283875465393066
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.mlp.2.b: 0.076526030898094
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.attn.q_proj.weight: 0.008459125645459
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.attn.k_proj.weight: 0.007927154190838
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.attn.v_proj.weight: 0.768382012844086
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.attn.out_proj.weight: 0.788564860820770
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.mlp.0.W: 0.755263268947601
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.mlp.0.b: 0.040811441838741
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.mlp.2.W: 4.096725463867188
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.mlp.2.b: 0.071230709552765
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.attn.q_proj.weight: 0.007026242557913
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.attn.k_proj.weight: 0.007082591764629
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.attn.v_proj.weight: 0.735487341880798
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.attn.out_proj.weight: 0.741966128349304
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.mlp.0.W: 0.733805596828461
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.mlp.0.b: 0.038626067340374
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.mlp.2.W: 3.874341249465942
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.mlp.2.b: 0.067627243697643
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.attn.q_proj.weight: 0.007889715954661
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.attn.k_proj.weight: 0.007805140223354
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.attn.v_proj.weight: 0.729653775691986
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.attn.out_proj.weight: 0.690813124179840
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.mlp.0.W: 0.703687131404877
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.mlp.0.b: 0.036594826728106
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.mlp.2.W: 3.713208913803101
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.mlp.2.b: 0.063591144979000
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.attn.q_proj.weight: 0.006506594363600
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.attn.k_proj.weight: 0.006776180118322
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.attn.v_proj.weight: 0.710435986518860
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.attn.out_proj.weight: 0.666987180709839
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.mlp.0.W: 0.676341176033020
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.mlp.0.b: 0.034438244998455
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.mlp.2.W: 3.572649955749512
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.mlp.2.b: 0.060629315674305
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/summary/components: 6.517097473144531
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/summary/ci_fns: 15.809143066406250
  0% 0/6 [00:01<?, ?it/s]                         train/grad_norms/summary/total: 17.099754333496094
  0% 0/6 [00:01<?, ?it/s]                         train/schedules/lr: 0.000100000000000
  0% 0/6 [00:01<?, ?it/s]                         eval/loss/ImportanceMinimalityLoss: 44855.765625
  0% 0/6 [00:03<?, ?it/s]                         eval/loss/StochasticReconSubsetLoss: 2.981088399887085
  0% 0/6 [00:03<?, ?it/s]                         eval/loss/PersistentPGDReconSubsetLoss: 3.5039939880371094
  0% 0/6 [00:03<?, ?it/s]                         eval/loss/FaithfulnessLoss: 4.053008524351753e-05
  0% 0/6 [00:03<?, ?it/s] 17% 1/6 [00:03<00:19,  3.94s/it] 33% 2/6 [00:04<00:07,  1.82s/it] 50% 3/6 [00:04<00:03,  1.12s/it] 67% 4/6 [00:04<00:01,  1.31it/s] 83% 5/6 [00:05<00:00,  1.75it/s]Saved model, optimizer, and out_dir to /mnt/polished-lake/artifacts/mechanisms/spd/spd/s-46da508f
100% 6/6 [00:07<00:00,  1.26s/it]100% 6/6 [00:07<00:00,  1.27s/it]
Finished training loop.
Optimization finished.

>>> 00_no_eval_metrics: 73s [OK] peak_gpu_mem=33481MiB


========================================
Starting test: 01_cheap_only
Time: Mon Feb  9 19:41:33 UTC 2026
Config: /mnt/polished-lake/home/braun/spd/eval_debug/configs/01_cheap_only.yaml
========================================

*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=0, local_rank=0, device=device(type='cuda', index=0)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=1, local_rank=1, device=device(type='cuda', index=1)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=3, local_rank=3, device=device(type='cuda', index=3)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=5, local_rank=5, device=device(type='cuda', index=5)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=7, local_rank=7, device=device(type='cuda', index=7)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=6, local_rank=6, device=device(type='cuda', index=6)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=2, local_rank=2, device=device(type='cuda', index=2)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=4, local_rank=4, device=device(type='cuda', index=4)
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 45581
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 45581
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 45581
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 45581
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 45581
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 45581
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 45581
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 45581
Distributed state: DistributedState(rank=7, world_size=8, local_rank=7, backend='nccl')
Distributed state: DistributedState(rank=1, world_size=8, local_rank=1, backend='nccl')
Distributed state: DistributedState(rank=5, world_size=8, local_rank=5, backend='nccl')
Distributed state: DistributedState(rank=2, world_size=8, local_rank=2, backend='nccl')
Distributed state: DistributedState(rank=3, world_size=8, local_rank=3, backend='nccl')
Distributed state: DistributedState(rank=0, world_size=8, local_rank=0, backend='nccl')
Distributed state: DistributedState(rank=6, world_size=8, local_rank=6, backend='nccl')
Distributed state: DistributedState(rank=4, world_size=8, local_rank=4, backend='nccl')
Using current branch: dev (uncommitted changes, no commit hash)
Run ID: s-d5e092f5
Output directory: /mnt/polished-lake/artifacts/mechanisms/spd/spd/s-d5e092f5
wandb_project=None wandb_run_name=None wandb_run_name_prefix='' seed=0 autocast_bf16=True n_mask_samples=1 ci_config=GlobalCiConfig(mode='global', fn_type='global_shared_transformer', hidden_dims=None, reader_hidden_dims=None, d_resid_ci_fn=None, block_groups=None, transition_attn_config=None, transition_hidden_dim=None, simple_transformer_ci_cfg=GlobalSharedTransformerCiConfig(d_model=1024, n_blocks=8, mlp_hidden_dim=[8192], attn_config=AttnConfig(n_heads=16, max_len=512, rope_base=10000.0))) sampling='continuous' sigmoid_type='leaky_hard' module_info=[ModulePatternInfoConfig(module_pattern='h.*.mlp.c_fc', C=4096), ModulePatternInfoConfig(module_pattern='h.*.mlp.down_proj', C=3072), ModulePatternInfoConfig(module_pattern='h.*.attn.q_proj', C=512), ModulePatternInfoConfig(module_pattern='h.*.attn.k_proj', C=256), ModulePatternInfoConfig(module_pattern='h.*.attn.v_proj', C=1024), ModulePatternInfoConfig(module_pattern='h.*.attn.o_proj', C=1024)] identity_module_info=None use_delta_component=True loss_metric_configs=[ImportanceMinimalityLossConfig(coeff=0.0005, classname='ImportanceMinimalityLoss', pnorm=2.0, beta=0.1, p_anneal_start_frac=0.0, p_anneal_final_p=0.4, p_anneal_end_frac=1.0, eps=1e-12), StochasticReconSubsetLossConfig(coeff=0.5, classname='StochasticReconSubsetLoss', routing=UniformKSubsetRoutingConfig(type='uniform_k_subset')), PersistentPGDReconSubsetLossConfig(coeff=0.5, classname='PersistentPGDReconSubsetLoss', optimizer=AdamPGDConfig(type='adam', lr=0.1, beta1=0.9, beta2=0.99, eps=1e-08), scope=BatchInvariantScope(type='batch_invariant', n_masks=8), routing=UniformKSubsetRoutingConfig(type='uniform_k_subset')), FaithfulnessLossConfig(coeff=1000000.0, classname='FaithfulnessLoss')] output_loss_type='kl' lr_schedule=ScheduleConfig(start_val=0.0001, warmup_pct=0.0, final_val_frac=0.1, fn_type='cosine') steps=5 batch_size=64 gradient_accumulation_steps=1 grad_clip_norm_components=0.01 grad_clip_norm_ci_fns=None faithfulness_warmup_steps=400 faithfulness_warmup_lr=0.001 faithfulness_warmup_weight_decay=0.0 train_log_freq=200 eval_freq=1000 eval_batch_size=256 slow_eval_freq=10000 n_eval_steps=1 slow_eval_on_first_step=True save_freq=None eval_metric_configs=[ComponentActivationDensityConfig(classname='ComponentActivationDensity'), CI_L0Config(classname='CI_L0', groups={'layer_0': ['h.0.*'], 'layer_1': ['h.1.*'], 'layer_2': ['h.2.*'], 'layer_3': ['h.3.*'], 'total': ['*']}), CIMeanPerComponentConfig(classname='CIMeanPerComponent')] ci_alive_threshold=0.0 pretrained_model_class='spd.pretrain.models.llama_simple_mlp.LlamaSimpleMLP' pretrained_model_path=None pretrained_model_name='wandb:goodfire/spd/t-32d1bb3b' pretrained_model_output_attr='idx_0' tokenizer_name='EleutherAI/gpt-neox-20b' task_config=LMTaskConfig(task_name='lm', max_seq_len=512, buffer_size=1000, dataset_name='danbraunai/pile-uncopyrighted-tok', column_name='input_ids', train_data_split='train', eval_data_split='val', shuffle_each_epoch=True, is_tokenized=True, streaming=True)
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading dataset...
Starting optimization...
Train+eval logs saved to directory: /mnt/polished-lake/artifacts/mechanisms/spd/spd/s-d5e092f5
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
  0% 0/6 [00:00<?, ?it/s]                         --- Step 0 ---
  0% 0/6 [00:00<?, ?it/s]                         LR: 0.000100
  0% 0/6 [00:00<?, ?it/s]                         train/loss/ImportanceMinimalityLoss: 36238.726562500000000
  0% 0/6 [00:00<?, ?it/s]                         train/loss/StochasticReconSubsetLoss: 2.963917255401611
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss: 4.808569431304932
  0% 0/6 [00:00<?, ?it/s]                         train/loss/FaithfulnessLoss: 0.000040523682401
  0% 0/6 [00:00<?, ?it/s]                         train/loss/total: 62.529289245605469
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.attn.k_proj: 130.421875000000000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.attn.o_proj: 507.907653808593750
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.attn.q_proj: 258.663482666015625
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.attn.v_proj: 505.962799072265625
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.mlp.c_fc: 2047.188476562500000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.mlp.down_proj: 1536.454833984375000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.attn.k_proj: 127.785308837890625
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.attn.o_proj: 511.397613525390625
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.attn.q_proj: 267.274322509765625
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.attn.v_proj: 508.254882812500000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.mlp.c_fc: 2051.299804687500000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.mlp.down_proj: 1535.686523437500000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.attn.k_proj: 132.868255615234375
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.attn.o_proj: 513.306762695312500
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.attn.q_proj: 243.575714111328125
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.attn.v_proj: 510.212829589843750
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.mlp.c_fc: 2057.168945312500000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.mlp.down_proj: 1548.709106445312500
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.attn.k_proj: 130.091400146484375
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.attn.o_proj: 516.913940429687500
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.attn.q_proj: 259.873718261718750
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.attn.v_proj: 517.953979492187500
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.mlp.c_fc: 2040.286743164062500
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.mlp.down_proj: 1540.889404296875000
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.mlp.c_fc: 0.097107894718647
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.mlp.c_fc: 0.097029037773609
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.mlp.c_fc: 0.096681430935860
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.mlp.c_fc: 0.096843123435974
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.mlp.down_proj: 0.096805348992348
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.mlp.down_proj: 0.096881560981274
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.mlp.down_proj: 0.096580810844898
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.mlp.down_proj: 0.096301153302193
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.attn.q_proj: 0.096449606120586
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.attn.q_proj: 0.093979999423027
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.attn.q_proj: 0.095499560236931
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.attn.q_proj: 0.095262818038464
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.attn.k_proj: 0.095392063260078
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.attn.k_proj: 0.095359392464161
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.attn.k_proj: 0.095940053462982
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.attn.k_proj: 0.095890298485756
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.attn.v_proj: 0.096610367298126
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.attn.v_proj: 0.095834366977215
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.attn.v_proj: 0.096549026668072
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.attn.v_proj: 0.095930404961109
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.attn.o_proj: 0.096996203064919
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.attn.o_proj: 0.096441082656384
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.attn.o_proj: 0.096836693584919
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.attn.o_proj: 0.096477016806602
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.mlp.c_fc.V: 1.415825366973877
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.mlp.c_fc.U: 3.596897602081299
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.mlp.c_fc.V: 0.923466324806213
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.mlp.c_fc.U: 2.407195091247559
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.mlp.c_fc.V: 0.649173617362976
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.mlp.c_fc.U: 1.526712059974670
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.mlp.c_fc.V: 0.413524925708771
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.mlp.c_fc.U: 0.888019323348999
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.mlp.down_proj.V: 1.526773691177368
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.mlp.down_proj.U: 1.446317434310913
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.mlp.down_proj.V: 0.778590619564056
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.mlp.down_proj.U: 0.740494549274445
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.mlp.down_proj.V: 0.535984456539154
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.mlp.down_proj.U: 0.497253209352493
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.mlp.down_proj.V: 0.508944153785706
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.mlp.down_proj.U: 0.434355825185776
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.q_proj.V: 0.260735183954239
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.q_proj.U: 0.279456675052643
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.q_proj.V: 0.225697919726372
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.q_proj.U: 0.233237668871880
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.q_proj.V: 0.184147357940674
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.q_proj.U: 0.192132458090782
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.q_proj.V: 0.184873670339584
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.q_proj.U: 0.191540241241455
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.k_proj.V: 0.261168509721756
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.k_proj.U: 0.413004040718079
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.k_proj.V: 0.230889737606049
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.k_proj.U: 0.326825737953186
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.k_proj.V: 0.204294517636299
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.k_proj.U: 0.264139592647552
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.k_proj.V: 0.184091627597809
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.k_proj.U: 0.240563303232193
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.v_proj.V: 0.861580789089203
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.v_proj.U: 1.049399375915527
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.v_proj.V: 0.969275414943695
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.v_proj.U: 1.084967374801636
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.v_proj.V: 0.757885396480560
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.v_proj.U: 0.818872749805450
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.v_proj.V: 0.442526072263718
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.v_proj.U: 0.461256027221680
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.o_proj.V: 0.995954155921936
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.o_proj.U: 1.263409733772278
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.o_proj.V: 0.467842549085617
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.o_proj.U: 0.599913120269775
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.o_proj.V: 0.459928691387177
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.o_proj.U: 0.572499155998230
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.o_proj.V: 0.329448997974396
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.o_proj.U: 0.433251380920410
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._input_projector.W: 9.018836021423340
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._input_projector.b: 0.119213268160820
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._output_head.W: 2.372462272644043
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._output_head.b: 0.038158148527145
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.attn.q_proj.weight: 0.014384986832738
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.attn.k_proj.weight: 0.015054211020470
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.attn.v_proj.weight: 0.857681453227997
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.attn.out_proj.weight: 0.857142627239227
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.mlp.0.W: 0.820401310920715
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.mlp.0.b: 0.055862292647362
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.mlp.2.W: 5.222383975982666
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.mlp.2.b: 0.100812211632729
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.attn.q_proj.weight: 0.010587576776743
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.attn.k_proj.weight: 0.010919852182269
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.attn.v_proj.weight: 0.840056478977203
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.attn.out_proj.weight: 0.855731964111328
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.mlp.0.W: 0.814612627029419
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.mlp.0.b: 0.049388989806175
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.mlp.2.W: 4.870746135711670
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.mlp.2.b: 0.090884059667587
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.attn.q_proj.weight: 0.010306893847883
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.attn.k_proj.weight: 0.010416800156236
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.attn.v_proj.weight: 0.817932605743408
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.attn.out_proj.weight: 0.831117212772369
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.mlp.0.W: 0.789804816246033
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.mlp.0.b: 0.046137757599354
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.mlp.2.W: 4.554945945739746
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.mlp.2.b: 0.083077117800713
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.attn.q_proj.weight: 0.008915972895920
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.attn.k_proj.weight: 0.008408558554947
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.attn.v_proj.weight: 0.800787508487701
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.attn.out_proj.weight: 0.802280008792877
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.mlp.0.W: 0.773108184337616
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.mlp.0.b: 0.043396323919296
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.mlp.2.W: 4.283857345581055
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.mlp.2.b: 0.076526187360287
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.attn.q_proj.weight: 0.008458892814815
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.attn.k_proj.weight: 0.007926870137453
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.attn.v_proj.weight: 0.768383383750916
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.attn.out_proj.weight: 0.788564920425415
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.mlp.0.W: 0.755263984203339
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.mlp.0.b: 0.040811464190483
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.mlp.2.W: 4.096724987030029
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.mlp.2.b: 0.071230776607990
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.attn.q_proj.weight: 0.007026236504316
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.attn.k_proj.weight: 0.007082591298968
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.attn.v_proj.weight: 0.735488474369049
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.attn.out_proj.weight: 0.741967618465424
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.mlp.0.W: 0.733805358409882
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.mlp.0.b: 0.038626044988632
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.mlp.2.W: 3.874340295791626
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.mlp.2.b: 0.067627243697643
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.attn.q_proj.weight: 0.007889730855823
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.attn.k_proj.weight: 0.007805150002241
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.attn.v_proj.weight: 0.729653656482697
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.attn.out_proj.weight: 0.690813183784485
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.mlp.0.W: 0.703687131404877
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.mlp.0.b: 0.036594826728106
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.mlp.2.W: 3.713208913803101
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.mlp.2.b: 0.063591144979000
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.attn.q_proj.weight: 0.006506592966616
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.attn.k_proj.weight: 0.006776180118322
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.attn.v_proj.weight: 0.710435986518860
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.attn.out_proj.weight: 0.666987180709839
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.mlp.0.W: 0.676341176033020
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.mlp.0.b: 0.034438244998455
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.mlp.2.W: 3.572649955749512
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.mlp.2.b: 0.060629315674305
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/summary/components: 6.517097473144531
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/summary/ci_fns: 15.809058189392090
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/summary/total: 17.099674224853516
  0% 0/6 [00:00<?, ?it/s]                         train/schedules/lr: 0.000100000000000
  0% 0/6 [00:00<?, ?it/s]                         eval/figures/component_activation_density: <PIL.Image.Image image mode=RGB size=1989x2990 at 0x71E8B860EEA0>
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/0.0_h.0.attn.k_proj: 130.44708251953125
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/0.0_h.0.attn.o_proj: 507.10870361328125
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/0.0_h.0.attn.q_proj: 258.8153076171875
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/0.0_h.0.attn.v_proj: 506.0845031738281
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/0.0_h.0.mlp.c_fc: 2047.969482421875
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/0.0_h.0.mlp.down_proj: 1537.3848876953125
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/0.0_h.1.attn.k_proj: 127.92353820800781
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/0.0_h.1.attn.o_proj: 511.77154541015625
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/0.0_h.1.attn.q_proj: 267.328857421875
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/0.0_h.1.attn.v_proj: 508.5190124511719
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/0.0_h.1.mlp.c_fc: 2053.302001953125
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/0.0_h.1.mlp.down_proj: 1536.3486328125
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/0.0_h.2.attn.k_proj: 132.82232666015625
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/0.0_h.2.attn.o_proj: 512.3269653320312
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/0.0_h.2.attn.q_proj: 243.21173095703125
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/0.0_h.2.attn.v_proj: 510.0046691894531
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/0.0_h.2.mlp.c_fc: 2057.486083984375
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/0.0_h.2.mlp.down_proj: 1549.276123046875
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/0.0_h.3.attn.k_proj: 129.75634765625
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/0.0_h.3.attn.o_proj: 516.0956420898438
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/0.0_h.3.attn.q_proj: 259.73065185546875
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/0.0_h.3.attn.v_proj: 518.4866333007812
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/0.0_h.3.mlp.c_fc: 2039.227783203125
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/0.0_h.3.mlp.down_proj: 1540.16943359375
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/0.0_layer_0: 4987.81005859375
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/0.0_total: 20001.59765625
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/0.0_layer_1: 5005.19384765625
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/0.0_layer_2: 5005.1279296875
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/0.0_layer_3: 5003.46630859375
  0% 0/6 [00:12<?, ?it/s]                         eval/l0/bar_chart: CustomChart(table=<wandb.sdk.data_types.table.Table object at 0x71e939599010>, spec=CustomChartSpec(spec_name='wandb/bar/v0', fields={'label': 'layer', 'value': 'l0'}, string_fields={'title': 'L0_0.0'}, key='', panel_type='Vega2', split_table=False))
  0% 0/6 [00:12<?, ?it/s]                         eval/figures/ci_mean_per_component: <PIL.Image.Image image mode=RGB size=6380x3579 at 0x71E8B87917B0>
  0% 0/6 [00:12<?, ?it/s]                         eval/figures/ci_mean_per_component_log: <PIL.Image.Image image mode=RGB size=6380x3579 at 0x71E93960F020>
  0% 0/6 [00:12<?, ?it/s]                         eval/loss/ImportanceMinimalityLoss: 44855.765625
  0% 0/6 [00:12<?, ?it/s]                         eval/loss/StochasticReconSubsetLoss: 2.981088399887085
  0% 0/6 [00:12<?, ?it/s]                         eval/loss/PersistentPGDReconSubsetLoss: 3.5039939880371094
  0% 0/6 [00:12<?, ?it/s]                         eval/loss/FaithfulnessLoss: 4.053008524351753e-05
  0% 0/6 [00:12<?, ?it/s]                         Saved figure figures/component_activation_density to /mnt/polished-lake/artifacts/mechanisms/spd/spd/s-d5e092f5/figures/figures_component_activation_density_0.png
  0% 0/6 [00:13<?, ?it/s]                         Saved custom chart data l0/bar_chart to /mnt/polished-lake/artifacts/mechanisms/spd/spd/s-d5e092f5/figures/l0_bar_chart_0.json
  0% 0/6 [00:13<?, ?it/s]                         Saved figure figures/ci_mean_per_component to /mnt/polished-lake/artifacts/mechanisms/spd/spd/s-d5e092f5/figures/figures_ci_mean_per_component_0.png
  0% 0/6 [00:13<?, ?it/s]                         Saved figure figures/ci_mean_per_component_log to /mnt/polished-lake/artifacts/mechanisms/spd/spd/s-d5e092f5/figures/figures_ci_mean_per_component_log_0.png
  0% 0/6 [00:14<?, ?it/s] 17% 1/6 [00:14<01:13, 14.64s/it] 33% 2/6 [00:14<00:24,  6.21s/it] 50% 3/6 [00:15<00:10,  3.47s/it] 67% 4/6 [00:15<00:04,  2.18s/it] 83% 5/6 [00:15<00:01,  1.47s/it]Saved model, optimizer, and out_dir to /mnt/polished-lake/artifacts/mechanisms/spd/spd/s-d5e092f5
100% 6/6 [00:17<00:00,  1.78s/it]100% 6/6 [00:17<00:00,  2.99s/it]
Finished training loop.
Optimization finished.

>>> 01_cheap_only: 60s [OK] peak_gpu_mem=54535MiB


========================================
Starting test: 02_cekl_only
Time: Mon Feb  9 19:42:34 UTC 2026
Config: /mnt/polished-lake/home/braun/spd/eval_debug/configs/02_cekl_only.yaml
========================================

*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=3, local_rank=3, device=device(type='cuda', index=3)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=1, local_rank=1, device=device(type='cuda', index=1)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=5, local_rank=5, device=device(type='cuda', index=5)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=6, local_rank=6, device=device(type='cuda', index=6)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=2, local_rank=2, device=device(type='cuda', index=2)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=4, local_rank=4, device=device(type='cuda', index=4)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=0, local_rank=0, device=device(type='cuda', index=0)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=7, local_rank=7, device=device(type='cuda', index=7)
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 38977
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 38977
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 38977
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 38977
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 38977
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 38977
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 38977
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 38977
Distributed state: DistributedState(rank=5, world_size=8, local_rank=5, backend='nccl')
Distributed state: DistributedState(rank=6, world_size=8, local_rank=6, backend='nccl')
Distributed state: DistributedState(rank=1, world_size=8, local_rank=1, backend='nccl')
Distributed state: DistributedState(rank=4, world_size=8, local_rank=4, backend='nccl')
Distributed state: DistributedState(rank=2, world_size=8, local_rank=2, backend='nccl')
Distributed state: DistributedState(rank=7, world_size=8, local_rank=7, backend='nccl')
Distributed state: DistributedState(rank=3, world_size=8, local_rank=3, backend='nccl')
Distributed state: DistributedState(rank=0, world_size=8, local_rank=0, backend='nccl')
Using current branch: dev (uncommitted changes, no commit hash)
Run ID: s-b9528869
Output directory: /mnt/polished-lake/artifacts/mechanisms/spd/spd/s-b9528869
wandb_project=None wandb_run_name=None wandb_run_name_prefix='' seed=0 autocast_bf16=True n_mask_samples=1 ci_config=GlobalCiConfig(mode='global', fn_type='global_shared_transformer', hidden_dims=None, reader_hidden_dims=None, d_resid_ci_fn=None, block_groups=None, transition_attn_config=None, transition_hidden_dim=None, simple_transformer_ci_cfg=GlobalSharedTransformerCiConfig(d_model=1024, n_blocks=8, mlp_hidden_dim=[8192], attn_config=AttnConfig(n_heads=16, max_len=512, rope_base=10000.0))) sampling='continuous' sigmoid_type='leaky_hard' module_info=[ModulePatternInfoConfig(module_pattern='h.*.mlp.c_fc', C=4096), ModulePatternInfoConfig(module_pattern='h.*.mlp.down_proj', C=3072), ModulePatternInfoConfig(module_pattern='h.*.attn.q_proj', C=512), ModulePatternInfoConfig(module_pattern='h.*.attn.k_proj', C=256), ModulePatternInfoConfig(module_pattern='h.*.attn.v_proj', C=1024), ModulePatternInfoConfig(module_pattern='h.*.attn.o_proj', C=1024)] identity_module_info=None use_delta_component=True loss_metric_configs=[ImportanceMinimalityLossConfig(coeff=0.0005, classname='ImportanceMinimalityLoss', pnorm=2.0, beta=0.1, p_anneal_start_frac=0.0, p_anneal_final_p=0.4, p_anneal_end_frac=1.0, eps=1e-12), StochasticReconSubsetLossConfig(coeff=0.5, classname='StochasticReconSubsetLoss', routing=UniformKSubsetRoutingConfig(type='uniform_k_subset')), PersistentPGDReconSubsetLossConfig(coeff=0.5, classname='PersistentPGDReconSubsetLoss', optimizer=AdamPGDConfig(type='adam', lr=0.1, beta1=0.9, beta2=0.99, eps=1e-08), scope=BatchInvariantScope(type='batch_invariant', n_masks=8), routing=UniformKSubsetRoutingConfig(type='uniform_k_subset')), FaithfulnessLossConfig(coeff=1000000.0, classname='FaithfulnessLoss')] output_loss_type='kl' lr_schedule=ScheduleConfig(start_val=0.0001, warmup_pct=0.0, final_val_frac=0.1, fn_type='cosine') steps=5 batch_size=64 gradient_accumulation_steps=1 grad_clip_norm_components=0.01 grad_clip_norm_ci_fns=None faithfulness_warmup_steps=400 faithfulness_warmup_lr=0.001 faithfulness_warmup_weight_decay=0.0 train_log_freq=200 eval_freq=1000 eval_batch_size=256 slow_eval_freq=10000 n_eval_steps=1 slow_eval_on_first_step=True save_freq=None eval_metric_configs=[CEandKLLossesConfig(classname='CEandKLLosses', rounding_threshold=0.0)] ci_alive_threshold=0.0 pretrained_model_class='spd.pretrain.models.llama_simple_mlp.LlamaSimpleMLP' pretrained_model_path=None pretrained_model_name='wandb:goodfire/spd/t-32d1bb3b' pretrained_model_output_attr='idx_0' tokenizer_name='EleutherAI/gpt-neox-20b' task_config=LMTaskConfig(task_name='lm', max_seq_len=512, buffer_size=1000, dataset_name='danbraunai/pile-uncopyrighted-tok', column_name='input_ids', train_data_split='train', eval_data_split='val', shuffle_each_epoch=True, is_tokenized=True, streaming=True)
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading dataset...
Starting optimization...
Train+eval logs saved to directory: /mnt/polished-lake/artifacts/mechanisms/spd/spd/s-b9528869
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
  0% 0/6 [00:00<?, ?it/s]                         --- Step 0 ---
  0% 0/6 [00:00<?, ?it/s]                         LR: 0.000100
  0% 0/6 [00:00<?, ?it/s]                         train/loss/ImportanceMinimalityLoss: 36238.726562500000000
  0% 0/6 [00:00<?, ?it/s]                         train/loss/StochasticReconSubsetLoss: 2.963917255401611
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss: 4.808569431304932
  0% 0/6 [00:00<?, ?it/s]                         train/loss/FaithfulnessLoss: 0.000040523682401
  0% 0/6 [00:00<?, ?it/s]                         train/loss/total: 62.529289245605469
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.attn.k_proj: 130.421875000000000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.attn.o_proj: 507.907653808593750
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.attn.q_proj: 258.663482666015625
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.attn.v_proj: 505.962799072265625
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.mlp.c_fc: 2047.188476562500000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.mlp.down_proj: 1536.454833984375000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.attn.k_proj: 127.785308837890625
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.attn.o_proj: 511.397613525390625
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.attn.q_proj: 267.274322509765625
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.attn.v_proj: 508.254882812500000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.mlp.c_fc: 2051.299804687500000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.mlp.down_proj: 1535.686523437500000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.attn.k_proj: 132.868255615234375
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.attn.o_proj: 513.306762695312500
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.attn.q_proj: 243.575714111328125
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.attn.v_proj: 510.212829589843750
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.mlp.c_fc: 2057.168945312500000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.mlp.down_proj: 1548.709106445312500
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.attn.k_proj: 130.091400146484375
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.attn.o_proj: 516.913940429687500
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.attn.q_proj: 259.873718261718750
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.attn.v_proj: 517.953979492187500
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.mlp.c_fc: 2040.286743164062500
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.mlp.down_proj: 1540.889404296875000
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.mlp.c_fc: 0.097107894718647
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.mlp.c_fc: 0.097029037773609
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.mlp.c_fc: 0.096681430935860
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.mlp.c_fc: 0.096843123435974
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.mlp.down_proj: 0.096805348992348
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.mlp.down_proj: 0.096881560981274
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.mlp.down_proj: 0.096580810844898
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.mlp.down_proj: 0.096301153302193
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.attn.q_proj: 0.096449606120586
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.attn.q_proj: 0.093979999423027
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.attn.q_proj: 0.095499560236931
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.attn.q_proj: 0.095262818038464
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.attn.k_proj: 0.095392063260078
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.attn.k_proj: 0.095359392464161
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.attn.k_proj: 0.095940053462982
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.attn.k_proj: 0.095890298485756
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.attn.v_proj: 0.096610367298126
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.attn.v_proj: 0.095834366977215
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.attn.v_proj: 0.096549026668072
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.attn.v_proj: 0.095930404961109
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.attn.o_proj: 0.096996203064919
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.attn.o_proj: 0.096441082656384
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.attn.o_proj: 0.096836693584919
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.attn.o_proj: 0.096477016806602
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.mlp.c_fc.V: 1.415825366973877
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.mlp.c_fc.U: 3.596897602081299
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.mlp.c_fc.V: 0.923466324806213
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.mlp.c_fc.U: 2.407195091247559
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.mlp.c_fc.V: 0.649173617362976
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.mlp.c_fc.U: 1.526712059974670
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.mlp.c_fc.V: 0.413524925708771
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.mlp.c_fc.U: 0.888019323348999
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.mlp.down_proj.V: 1.526773691177368
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.mlp.down_proj.U: 1.446317434310913
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.mlp.down_proj.V: 0.778590619564056
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.mlp.down_proj.U: 0.740494549274445
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.mlp.down_proj.V: 0.535984456539154
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.mlp.down_proj.U: 0.497253209352493
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.mlp.down_proj.V: 0.508944153785706
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.mlp.down_proj.U: 0.434355825185776
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.q_proj.V: 0.260735183954239
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.q_proj.U: 0.279456675052643
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.q_proj.V: 0.225697919726372
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.q_proj.U: 0.233237668871880
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.q_proj.V: 0.184147357940674
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.q_proj.U: 0.192132458090782
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.q_proj.V: 0.184873670339584
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.q_proj.U: 0.191540241241455
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.k_proj.V: 0.261168509721756
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.k_proj.U: 0.413004040718079
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.k_proj.V: 0.230889737606049
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.k_proj.U: 0.326825737953186
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.k_proj.V: 0.204294517636299
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.k_proj.U: 0.264139592647552
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.k_proj.V: 0.184091627597809
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.k_proj.U: 0.240563303232193
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.v_proj.V: 0.861580789089203
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.v_proj.U: 1.049399375915527
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.v_proj.V: 0.969275414943695
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.v_proj.U: 1.084967374801636
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.v_proj.V: 0.757885396480560
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.v_proj.U: 0.818872749805450
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.v_proj.V: 0.442526072263718
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.v_proj.U: 0.461256027221680
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.o_proj.V: 0.995954155921936
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.o_proj.U: 1.263409733772278
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.o_proj.V: 0.467842549085617
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.o_proj.U: 0.599913120269775
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.o_proj.V: 0.459928691387177
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.o_proj.U: 0.572499155998230
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.o_proj.V: 0.329448997974396
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.o_proj.U: 0.433251380920410
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._input_projector.W: 9.019086837768555
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._input_projector.b: 0.119214855134487
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._output_head.W: 2.372462272644043
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._output_head.b: 0.038158148527145
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.attn.q_proj.weight: 0.014386352151632
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.attn.k_proj.weight: 0.015054504387081
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.attn.v_proj.weight: 0.857707023620605
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.attn.out_proj.weight: 0.857165873050690
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.mlp.0.W: 0.820408761501312
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.mlp.0.b: 0.055862981826067
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.mlp.2.W: 5.222399711608887
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.mlp.2.b: 0.100812986493111
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.attn.q_proj.weight: 0.010587119497359
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.attn.k_proj.weight: 0.010920067317784
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.attn.v_proj.weight: 0.840069711208344
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.attn.out_proj.weight: 0.855731308460236
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.mlp.0.W: 0.814617395401001
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.mlp.0.b: 0.049389250576496
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.mlp.2.W: 4.870814800262451
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.mlp.2.b: 0.090884484350681
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.attn.q_proj.weight: 0.010307545773685
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.attn.k_proj.weight: 0.010417084209621
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.attn.v_proj.weight: 0.817930877208710
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.attn.out_proj.weight: 0.831116199493408
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.mlp.0.W: 0.789801955223083
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.mlp.0.b: 0.046137608587742
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.mlp.2.W: 4.554945945739746
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.mlp.2.b: 0.083077207207680
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.attn.q_proj.weight: 0.008916218765080
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.attn.k_proj.weight: 0.008408281952143
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.attn.v_proj.weight: 0.800790429115295
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.attn.out_proj.weight: 0.802281737327576
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.mlp.0.W: 0.773106992244720
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.mlp.0.b: 0.043396301567554
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.mlp.2.W: 4.283865928649902
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.mlp.2.b: 0.076526157557964
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.attn.q_proj.weight: 0.008458609692752
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.attn.k_proj.weight: 0.007926718331873
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.attn.v_proj.weight: 0.768380999565125
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.attn.out_proj.weight: 0.788566887378693
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.mlp.0.W: 0.755263507366180
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.mlp.0.b: 0.040811415761709
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.mlp.2.W: 4.096730232238770
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.mlp.2.b: 0.071230739355087
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.attn.q_proj.weight: 0.007026496343315
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.attn.k_proj.weight: 0.007082628551871
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.attn.v_proj.weight: 0.735490202903748
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.attn.out_proj.weight: 0.741967797279358
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.mlp.0.W: 0.733805596828461
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.mlp.0.b: 0.038626056164503
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.mlp.2.W: 3.874341011047363
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.mlp.2.b: 0.067627243697643
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.attn.q_proj.weight: 0.007889725267887
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.attn.k_proj.weight: 0.007805156521499
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.attn.v_proj.weight: 0.729653894901276
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.attn.out_proj.weight: 0.690813124179840
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.mlp.0.W: 0.703687131404877
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.mlp.0.b: 0.036594826728106
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.mlp.2.W: 3.713208913803101
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.mlp.2.b: 0.063591144979000
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.attn.q_proj.weight: 0.006506593897939
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.attn.k_proj.weight: 0.006776180118322
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.attn.v_proj.weight: 0.710435986518860
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.attn.out_proj.weight: 0.666987180709839
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.mlp.0.W: 0.676341176033020
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.mlp.0.b: 0.034438244998455
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.mlp.2.W: 3.572649955749512
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.mlp.2.b: 0.060629315674305
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/summary/components: 6.517097473144531
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/summary/ci_fns: 15.809236526489258
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/summary/total: 17.099840164184570
  0% 0/6 [00:00<?, ?it/s]                         train/schedules/lr: 0.000100000000000
  0% 0/6 [00:00<?, ?it/s]                         eval/ce_kl/kl_ci_masked: 9.495499610900879
  0% 0/6 [00:02<?, ?it/s]                         eval/ce_kl/kl_unmasked: 0.8671587109565735
  0% 0/6 [00:02<?, ?it/s]                         eval/ce_kl/kl_stoch_masked: 4.900856971740723
  0% 0/6 [00:02<?, ?it/s]                         eval/ce_kl/kl_random_masked: 6.479849338531494
  0% 0/6 [00:02<?, ?it/s]                         eval/ce_kl/kl_rounded_masked: 9.209177017211914
  0% 0/6 [00:02<?, ?it/s]                         eval/ce_kl/kl_zero_masked: 67.24137878417969
  0% 0/6 [00:02<?, ?it/s]                         eval/ce_kl/ce_difference_ci_masked: 9.524429321289062
  0% 0/6 [00:02<?, ?it/s]                         eval/ce_kl/ce_difference_unmasked: 0.8797334432601929
  0% 0/6 [00:02<?, ?it/s]                         eval/ce_kl/ce_difference_stoch_masked: 4.9171037673950195
  0% 0/6 [00:02<?, ?it/s]                         eval/ce_kl/ce_difference_random_masked: 6.505438804626465
  0% 0/6 [00:02<?, ?it/s]                         eval/ce_kl/ce_difference_rounded_masked: 9.234663009643555
  0% 0/6 [00:02<?, ?it/s]                         eval/ce_kl/ce_unrecovered_ci_masked: 0.14157015085220337
  0% 0/6 [00:02<?, ?it/s]                         eval/ce_kl/ce_unrecovered_unmasked: 0.013070366345345974
  0% 0/6 [00:02<?, ?it/s]                         eval/ce_kl/ce_unrecovered_stoch_masked: 0.07307715713977814
  0% 0/6 [00:02<?, ?it/s]                         eval/ce_kl/ce_unrecovered_random_masked: 0.0966954380273819
  0% 0/6 [00:02<?, ?it/s]                         eval/ce_kl/ce_unrecovered_rounded_masked: 0.13726381957530975
  0% 0/6 [00:02<?, ?it/s]                         eval/loss/ImportanceMinimalityLoss: 44855.765625
  0% 0/6 [00:02<?, ?it/s]                         eval/loss/StochasticReconSubsetLoss: 3.002746820449829
  0% 0/6 [00:02<?, ?it/s]                         eval/loss/PersistentPGDReconSubsetLoss: 3.5409085750579834
  0% 0/6 [00:02<?, ?it/s]                         eval/loss/FaithfulnessLoss: 4.053008524351753e-05
  0% 0/6 [00:02<?, ?it/s] 17% 1/6 [00:03<00:16,  3.39s/it] 33% 2/6 [00:03<00:06,  1.62s/it] 50% 3/6 [00:03<00:02,  1.02it/s] 67% 4/6 [00:04<00:01,  1.48it/s] 83% 5/6 [00:04<00:00,  1.97it/s]Saved model, optimizer, and out_dir to /mnt/polished-lake/artifacts/mechanisms/spd/spd/s-b9528869
100% 6/6 [00:07<00:00,  1.23s/it]100% 6/6 [00:07<00:00,  1.18s/it]
Finished training loop.
Optimization finished.

>>> 02_cekl_only: 52s [OK] peak_gpu_mem=32721MiB


========================================
Starting test: 03_stoch_hidden_only
Time: Mon Feb  9 19:43:26 UTC 2026
Config: /mnt/polished-lake/home/braun/spd/eval_debug/configs/03_stoch_hidden_only.yaml
========================================

*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=2, local_rank=2, device=device(type='cuda', index=2)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=3, local_rank=3, device=device(type='cuda', index=3)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=4, local_rank=4, device=device(type='cuda', index=4)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=5, local_rank=5, device=device(type='cuda', index=5)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=0, local_rank=0, device=device(type='cuda', index=0)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=7, local_rank=7, device=device(type='cuda', index=7)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=1, local_rank=1, device=device(type='cuda', index=1)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=6, local_rank=6, device=device(type='cuda', index=6)
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 33463
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 33463
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 33463
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 33463
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 33463
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 33463
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 33463
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 33463
Distributed state: DistributedState(rank=4, world_size=8, local_rank=4, backend='nccl')
Distributed state: DistributedState(rank=2, world_size=8, local_rank=2, backend='nccl')
Distributed state: DistributedState(rank=6, world_size=8, local_rank=6, backend='nccl')
Distributed state: DistributedState(rank=5, world_size=8, local_rank=5, backend='nccl')
Distributed state: DistributedState(rank=3, world_size=8, local_rank=3, backend='nccl')
Distributed state: DistributedState(rank=1, world_size=8, local_rank=1, backend='nccl')
Distributed state: DistributedState(rank=0, world_size=8, local_rank=0, backend='nccl')
Distributed state: DistributedState(rank=7, world_size=8, local_rank=7, backend='nccl')
Using current branch: dev (uncommitted changes, no commit hash)
Run ID: s-47fc7e84
Output directory: /mnt/polished-lake/artifacts/mechanisms/spd/spd/s-47fc7e84
wandb_project=None wandb_run_name=None wandb_run_name_prefix='' seed=0 autocast_bf16=True n_mask_samples=1 ci_config=GlobalCiConfig(mode='global', fn_type='global_shared_transformer', hidden_dims=None, reader_hidden_dims=None, d_resid_ci_fn=None, block_groups=None, transition_attn_config=None, transition_hidden_dim=None, simple_transformer_ci_cfg=GlobalSharedTransformerCiConfig(d_model=1024, n_blocks=8, mlp_hidden_dim=[8192], attn_config=AttnConfig(n_heads=16, max_len=512, rope_base=10000.0))) sampling='continuous' sigmoid_type='leaky_hard' module_info=[ModulePatternInfoConfig(module_pattern='h.*.mlp.c_fc', C=4096), ModulePatternInfoConfig(module_pattern='h.*.mlp.down_proj', C=3072), ModulePatternInfoConfig(module_pattern='h.*.attn.q_proj', C=512), ModulePatternInfoConfig(module_pattern='h.*.attn.k_proj', C=256), ModulePatternInfoConfig(module_pattern='h.*.attn.v_proj', C=1024), ModulePatternInfoConfig(module_pattern='h.*.attn.o_proj', C=1024)] identity_module_info=None use_delta_component=True loss_metric_configs=[ImportanceMinimalityLossConfig(coeff=0.0005, classname='ImportanceMinimalityLoss', pnorm=2.0, beta=0.1, p_anneal_start_frac=0.0, p_anneal_final_p=0.4, p_anneal_end_frac=1.0, eps=1e-12), StochasticReconSubsetLossConfig(coeff=0.5, classname='StochasticReconSubsetLoss', routing=UniformKSubsetRoutingConfig(type='uniform_k_subset')), PersistentPGDReconSubsetLossConfig(coeff=0.5, classname='PersistentPGDReconSubsetLoss', optimizer=AdamPGDConfig(type='adam', lr=0.1, beta1=0.9, beta2=0.99, eps=1e-08), scope=BatchInvariantScope(type='batch_invariant', n_masks=8), routing=UniformKSubsetRoutingConfig(type='uniform_k_subset')), FaithfulnessLossConfig(coeff=1000000.0, classname='FaithfulnessLoss')] output_loss_type='kl' lr_schedule=ScheduleConfig(start_val=0.0001, warmup_pct=0.0, final_val_frac=0.1, fn_type='cosine') steps=5 batch_size=64 gradient_accumulation_steps=1 grad_clip_norm_components=0.01 grad_clip_norm_ci_fns=None faithfulness_warmup_steps=400 faithfulness_warmup_lr=0.001 faithfulness_warmup_weight_decay=0.0 train_log_freq=200 eval_freq=1000 eval_batch_size=256 slow_eval_freq=10000 n_eval_steps=1 slow_eval_on_first_step=True save_freq=None eval_metric_configs=[StochasticHiddenActsReconLossConfig(coeff=None, classname='StochasticHiddenActsReconLoss')] ci_alive_threshold=0.0 pretrained_model_class='spd.pretrain.models.llama_simple_mlp.LlamaSimpleMLP' pretrained_model_path=None pretrained_model_name='wandb:goodfire/spd/t-32d1bb3b' pretrained_model_output_attr='idx_0' tokenizer_name='EleutherAI/gpt-neox-20b' task_config=LMTaskConfig(task_name='lm', max_seq_len=512, buffer_size=1000, dataset_name='danbraunai/pile-uncopyrighted-tok', column_name='input_ids', train_data_split='train', eval_data_split='val', shuffle_each_epoch=True, is_tokenized=True, streaming=True)
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading dataset...
Starting optimization...
Train+eval logs saved to directory: /mnt/polished-lake/artifacts/mechanisms/spd/spd/s-47fc7e84
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
  0% 0/6 [00:00<?, ?it/s]                         --- Step 0 ---
  0% 0/6 [00:00<?, ?it/s]                         LR: 0.000100
  0% 0/6 [00:00<?, ?it/s]                         train/loss/ImportanceMinimalityLoss: 36238.726562500000000
  0% 0/6 [00:00<?, ?it/s]                         train/loss/StochasticReconSubsetLoss: 2.963917255401611
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss: 4.808569431304932
  0% 0/6 [00:00<?, ?it/s]                         train/loss/FaithfulnessLoss: 0.000040523682401
  0% 0/6 [00:00<?, ?it/s]                         train/loss/total: 62.529289245605469
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.attn.k_proj: 130.421875000000000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.attn.o_proj: 507.907653808593750
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.attn.q_proj: 258.663482666015625
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.attn.v_proj: 505.962799072265625
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.mlp.c_fc: 2047.188476562500000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.mlp.down_proj: 1536.454833984375000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.attn.k_proj: 127.785308837890625
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.attn.o_proj: 511.397613525390625
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.attn.q_proj: 267.274322509765625
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.attn.v_proj: 508.254882812500000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.mlp.c_fc: 2051.299804687500000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.mlp.down_proj: 1535.686523437500000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.attn.k_proj: 132.868255615234375
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.attn.o_proj: 513.306762695312500
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.attn.q_proj: 243.575714111328125
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.attn.v_proj: 510.212829589843750
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.mlp.c_fc: 2057.168945312500000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.mlp.down_proj: 1548.709106445312500
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.attn.k_proj: 130.091400146484375
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.attn.o_proj: 516.913940429687500
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.attn.q_proj: 259.873718261718750
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.attn.v_proj: 517.953979492187500
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.mlp.c_fc: 2040.286743164062500
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.mlp.down_proj: 1540.889404296875000
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.mlp.c_fc: 0.097107894718647
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.mlp.c_fc: 0.097029037773609
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.mlp.c_fc: 0.096681430935860
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.mlp.c_fc: 0.096843123435974
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.mlp.down_proj: 0.096805348992348
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.mlp.down_proj: 0.096881560981274
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.mlp.down_proj: 0.096580810844898
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.mlp.down_proj: 0.096301153302193
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.attn.q_proj: 0.096449606120586
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.attn.q_proj: 0.093979999423027
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.attn.q_proj: 0.095499560236931
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.attn.q_proj: 0.095262818038464
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.attn.k_proj: 0.095392063260078
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.attn.k_proj: 0.095359392464161
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.attn.k_proj: 0.095940053462982
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.attn.k_proj: 0.095890298485756
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.attn.v_proj: 0.096610367298126
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.attn.v_proj: 0.095834366977215
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.attn.v_proj: 0.096549026668072
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.attn.v_proj: 0.095930404961109
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.attn.o_proj: 0.096996203064919
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.attn.o_proj: 0.096441082656384
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.attn.o_proj: 0.096836693584919
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.attn.o_proj: 0.096477016806602
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.mlp.c_fc.V: 1.415825366973877
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.mlp.c_fc.U: 3.596897602081299
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.mlp.c_fc.V: 0.923466324806213
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.mlp.c_fc.U: 2.407195091247559
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.mlp.c_fc.V: 0.649173617362976
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.mlp.c_fc.U: 1.526712059974670
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.mlp.c_fc.V: 0.413524925708771
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.mlp.c_fc.U: 0.888019323348999
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.mlp.down_proj.V: 1.526773691177368
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.mlp.down_proj.U: 1.446317434310913
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.mlp.down_proj.V: 0.778590619564056
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.mlp.down_proj.U: 0.740494549274445
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.mlp.down_proj.V: 0.535984456539154
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.mlp.down_proj.U: 0.497253209352493
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.mlp.down_proj.V: 0.508944153785706
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.mlp.down_proj.U: 0.434355825185776
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.q_proj.V: 0.260735183954239
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.q_proj.U: 0.279456675052643
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.q_proj.V: 0.225697919726372
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.q_proj.U: 0.233237668871880
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.q_proj.V: 0.184147357940674
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.q_proj.U: 0.192132458090782
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.q_proj.V: 0.184873670339584
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.q_proj.U: 0.191540241241455
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.k_proj.V: 0.261168509721756
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.k_proj.U: 0.413004040718079
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.k_proj.V: 0.230889737606049
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.k_proj.U: 0.326825737953186
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.k_proj.V: 0.204294517636299
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.k_proj.U: 0.264139592647552
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.k_proj.V: 0.184091627597809
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.k_proj.U: 0.240563303232193
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.v_proj.V: 0.861580789089203
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.v_proj.U: 1.049399375915527
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.v_proj.V: 0.969275414943695
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.v_proj.U: 1.084967374801636
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.v_proj.V: 0.757885396480560
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.v_proj.U: 0.818872749805450
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.v_proj.V: 0.442526072263718
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.v_proj.U: 0.461256027221680
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.o_proj.V: 0.995954155921936
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.o_proj.U: 1.263409733772278
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.o_proj.V: 0.467842549085617
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.o_proj.U: 0.599913120269775
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.o_proj.V: 0.459928691387177
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.o_proj.U: 0.572499155998230
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.o_proj.V: 0.329448997974396
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.o_proj.U: 0.433251380920410
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._input_projector.W: 9.018902778625488
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._input_projector.b: 0.119212992489338
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._output_head.W: 2.372462272644043
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._output_head.b: 0.038158148527145
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.attn.q_proj.weight: 0.014386396855116
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.attn.k_proj.weight: 0.015055296942592
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.attn.v_proj.weight: 0.857685804367065
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.attn.out_proj.weight: 0.857149600982666
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.mlp.0.W: 0.820410013198853
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.mlp.0.b: 0.055863313376904
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.mlp.2.W: 5.222441196441650
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.mlp.2.b: 0.100813314318657
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.attn.q_proj.weight: 0.010587044060230
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.attn.k_proj.weight: 0.010919576510787
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.attn.v_proj.weight: 0.840037763118744
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.attn.out_proj.weight: 0.855726003646851
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.mlp.0.W: 0.814619660377502
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.mlp.0.b: 0.049389250576496
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.mlp.2.W: 4.870819091796875
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.mlp.2.b: 0.090884886682034
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.attn.q_proj.weight: 0.010307810269296
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.attn.k_proj.weight: 0.010417417623103
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.attn.v_proj.weight: 0.817951202392578
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.attn.out_proj.weight: 0.831127941608429
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.mlp.0.W: 0.789794325828552
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.mlp.0.b: 0.046137064695358
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.mlp.2.W: 4.554907798767090
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.mlp.2.b: 0.083076760172844
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.attn.q_proj.weight: 0.008915673941374
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.attn.k_proj.weight: 0.008408128283918
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.attn.v_proj.weight: 0.800791680812836
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.attn.out_proj.weight: 0.802277982234955
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.mlp.0.W: 0.773103594779968
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.mlp.0.b: 0.043396133929491
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.mlp.2.W: 4.283856391906738
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.mlp.2.b: 0.076525963842869
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.attn.q_proj.weight: 0.008458827622235
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.attn.k_proj.weight: 0.007926682010293
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.attn.v_proj.weight: 0.768379449844360
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.attn.out_proj.weight: 0.788563966751099
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.mlp.0.W: 0.755264699459076
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.mlp.0.b: 0.040811516344547
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.mlp.2.W: 4.096726894378662
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.mlp.2.b: 0.071230709552765
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.attn.q_proj.weight: 0.007026406936347
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.attn.k_proj.weight: 0.007082501426339
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.attn.v_proj.weight: 0.735486984252930
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.attn.out_proj.weight: 0.741966962814331
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.mlp.0.W: 0.733805537223816
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.mlp.0.b: 0.038626082241535
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.mlp.2.W: 3.874341964721680
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.mlp.2.b: 0.067627243697643
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.attn.q_proj.weight: 0.007889721542597
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.attn.k_proj.weight: 0.007805148139596
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.attn.v_proj.weight: 0.729653835296631
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.attn.out_proj.weight: 0.690813183784485
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.mlp.0.W: 0.703687131404877
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.mlp.0.b: 0.036594826728106
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.mlp.2.W: 3.713208913803101
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.mlp.2.b: 0.063591144979000
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.attn.q_proj.weight: 0.006506593897939
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.attn.k_proj.weight: 0.006776180118322
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.attn.v_proj.weight: 0.710435986518860
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.attn.out_proj.weight: 0.666987180709839
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.mlp.0.W: 0.676341176033020
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.mlp.0.b: 0.034438244998455
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.mlp.2.W: 3.572649955749512
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.mlp.2.b: 0.060629315674305
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/summary/components: 6.517097473144531
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/summary/ci_fns: 15.809127807617188
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/summary/total: 17.099739074707031
  0% 0/6 [00:00<?, ?it/s]                         train/schedules/lr: 0.000100000000000
  0% 0/6 [00:00<?, ?it/s]                         eval/loss/StochasticHiddenActsReconLoss: 0.5621165633201599
  0% 0/6 [00:02<?, ?it/s]                         eval/loss/ImportanceMinimalityLoss: 44855.765625
  0% 0/6 [00:02<?, ?it/s]                         eval/loss/StochasticReconSubsetLoss: 3.000047206878662
  0% 0/6 [00:02<?, ?it/s]                         eval/loss/PersistentPGDReconSubsetLoss: 3.530320882797241
  0% 0/6 [00:02<?, ?it/s]                         eval/loss/FaithfulnessLoss: 4.053008524351753e-05
  0% 0/6 [00:02<?, ?it/s] 17% 1/6 [00:02<00:14,  2.96s/it] 33% 2/6 [00:03<00:05,  1.39s/it] 50% 3/6 [00:03<00:02,  1.13it/s] 67% 4/6 [00:03<00:01,  1.61it/s] 83% 5/6 [00:03<00:00,  2.11it/s]Saved model, optimizer, and out_dir to /mnt/polished-lake/artifacts/mechanisms/spd/spd/s-47fc7e84
100% 6/6 [00:06<00:00,  1.19s/it]100% 6/6 [00:06<00:00,  1.09s/it]
Finished training loop.
Optimization finished.

>>> 03_stoch_hidden_only: 50s [OK] peak_gpu_mem=32721MiB


========================================
Starting test: 04_pgd_only
Time: Mon Feb  9 19:44:17 UTC 2026
Config: /mnt/polished-lake/home/braun/spd/eval_debug/configs/04_pgd_only.yaml
========================================

*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=5, local_rank=5, device=device(type='cuda', index=5)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=0, local_rank=0, device=device(type='cuda', index=0)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=1, local_rank=1, device=device(type='cuda', index=1)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=7, local_rank=7, device=device(type='cuda', index=7)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=4, local_rank=4, device=device(type='cuda', index=4)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=2, local_rank=2, device=device(type='cuda', index=2)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=3, local_rank=3, device=device(type='cuda', index=3)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=6, local_rank=6, device=device(type='cuda', index=6)
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 37393
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 37393
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 37393
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 37393
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 37393
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 37393
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 37393
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 37393
Distributed state: DistributedState(rank=5, world_size=8, local_rank=5, backend='nccl')
Distributed state: DistributedState(rank=6, world_size=8, local_rank=6, backend='nccl')
Distributed state: DistributedState(rank=7, world_size=8, local_rank=7, backend='nccl')
Distributed state: DistributedState(rank=0, world_size=8, local_rank=0, backend='nccl')
Distributed state: DistributedState(rank=3, world_size=8, local_rank=3, backend='nccl')
Distributed state: DistributedState(rank=4, world_size=8, local_rank=4, backend='nccl')
Distributed state: DistributedState(rank=2, world_size=8, local_rank=2, backend='nccl')
Distributed state: DistributedState(rank=1, world_size=8, local_rank=1, backend='nccl')
Using current branch: dev (uncommitted changes, no commit hash)
Run ID: s-b3bbe4aa
Output directory: /mnt/polished-lake/artifacts/mechanisms/spd/spd/s-b3bbe4aa
wandb_project=None wandb_run_name=None wandb_run_name_prefix='' seed=0 autocast_bf16=True n_mask_samples=1 ci_config=GlobalCiConfig(mode='global', fn_type='global_shared_transformer', hidden_dims=None, reader_hidden_dims=None, d_resid_ci_fn=None, block_groups=None, transition_attn_config=None, transition_hidden_dim=None, simple_transformer_ci_cfg=GlobalSharedTransformerCiConfig(d_model=1024, n_blocks=8, mlp_hidden_dim=[8192], attn_config=AttnConfig(n_heads=16, max_len=512, rope_base=10000.0))) sampling='continuous' sigmoid_type='leaky_hard' module_info=[ModulePatternInfoConfig(module_pattern='h.*.mlp.c_fc', C=4096), ModulePatternInfoConfig(module_pattern='h.*.mlp.down_proj', C=3072), ModulePatternInfoConfig(module_pattern='h.*.attn.q_proj', C=512), ModulePatternInfoConfig(module_pattern='h.*.attn.k_proj', C=256), ModulePatternInfoConfig(module_pattern='h.*.attn.v_proj', C=1024), ModulePatternInfoConfig(module_pattern='h.*.attn.o_proj', C=1024)] identity_module_info=None use_delta_component=True loss_metric_configs=[ImportanceMinimalityLossConfig(coeff=0.0005, classname='ImportanceMinimalityLoss', pnorm=2.0, beta=0.1, p_anneal_start_frac=0.0, p_anneal_final_p=0.4, p_anneal_end_frac=1.0, eps=1e-12), StochasticReconSubsetLossConfig(coeff=0.5, classname='StochasticReconSubsetLoss', routing=UniformKSubsetRoutingConfig(type='uniform_k_subset')), PersistentPGDReconSubsetLossConfig(coeff=0.5, classname='PersistentPGDReconSubsetLoss', optimizer=AdamPGDConfig(type='adam', lr=0.1, beta1=0.9, beta2=0.99, eps=1e-08), scope=BatchInvariantScope(type='batch_invariant', n_masks=8), routing=UniformKSubsetRoutingConfig(type='uniform_k_subset')), FaithfulnessLossConfig(coeff=1000000.0, classname='FaithfulnessLoss')] output_loss_type='kl' lr_schedule=ScheduleConfig(start_val=0.0001, warmup_pct=0.0, final_val_frac=0.1, fn_type='cosine') steps=5 batch_size=64 gradient_accumulation_steps=1 grad_clip_norm_components=0.01 grad_clip_norm_ci_fns=None faithfulness_warmup_steps=400 faithfulness_warmup_lr=0.001 faithfulness_warmup_weight_decay=0.0 train_log_freq=200 eval_freq=1000 eval_batch_size=256 slow_eval_freq=10000 n_eval_steps=1 slow_eval_on_first_step=True save_freq=None eval_metric_configs=[PGDReconLossConfig(coeff=None, init='random', step_size=0.1, n_steps=20, mask_scope='shared_across_batch', classname='PGDReconLoss')] ci_alive_threshold=0.0 pretrained_model_class='spd.pretrain.models.llama_simple_mlp.LlamaSimpleMLP' pretrained_model_path=None pretrained_model_name='wandb:goodfire/spd/t-32d1bb3b' pretrained_model_output_attr='idx_0' tokenizer_name='EleutherAI/gpt-neox-20b' task_config=LMTaskConfig(task_name='lm', max_seq_len=512, buffer_size=1000, dataset_name='danbraunai/pile-uncopyrighted-tok', column_name='input_ids', train_data_split='train', eval_data_split='val', shuffle_each_epoch=True, is_tokenized=True, streaming=True)
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading dataset...
Starting optimization...
Train+eval logs saved to directory: /mnt/polished-lake/artifacts/mechanisms/spd/spd/s-b3bbe4aa
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
  0% 0/6 [00:00<?, ?it/s]                         --- Step 0 ---
  0% 0/6 [00:00<?, ?it/s]                         LR: 0.000100
  0% 0/6 [00:00<?, ?it/s]                         train/loss/ImportanceMinimalityLoss: 36238.726562500000000
  0% 0/6 [00:00<?, ?it/s]                         train/loss/StochasticReconSubsetLoss: 2.963917255401611
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss: 4.808569431304932
  0% 0/6 [00:00<?, ?it/s]                         train/loss/FaithfulnessLoss: 0.000040523682401
  0% 0/6 [00:00<?, ?it/s]                         train/loss/total: 62.529289245605469
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.attn.k_proj: 130.421875000000000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.attn.o_proj: 507.907653808593750
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.attn.q_proj: 258.663482666015625
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.attn.v_proj: 505.962799072265625
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.mlp.c_fc: 2047.188476562500000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.mlp.down_proj: 1536.454833984375000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.attn.k_proj: 127.785308837890625
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.attn.o_proj: 511.397613525390625
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.attn.q_proj: 267.274322509765625
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.attn.v_proj: 508.254882812500000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.mlp.c_fc: 2051.299804687500000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.mlp.down_proj: 1535.686523437500000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.attn.k_proj: 132.868255615234375
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.attn.o_proj: 513.306762695312500
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.attn.q_proj: 243.575714111328125
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.attn.v_proj: 510.212829589843750
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.mlp.c_fc: 2057.168945312500000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.mlp.down_proj: 1548.709106445312500
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.attn.k_proj: 130.091400146484375
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.attn.o_proj: 516.913940429687500
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.attn.q_proj: 259.873718261718750
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.attn.v_proj: 517.953979492187500
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.mlp.c_fc: 2040.286743164062500
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.mlp.down_proj: 1540.889404296875000
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.mlp.c_fc: 0.097107894718647
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.mlp.c_fc: 0.097029037773609
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.mlp.c_fc: 0.096681430935860
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.mlp.c_fc: 0.096843123435974
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.mlp.down_proj: 0.096805348992348
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.mlp.down_proj: 0.096881560981274
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.mlp.down_proj: 0.096580810844898
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.mlp.down_proj: 0.096301153302193
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.attn.q_proj: 0.096449606120586
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.attn.q_proj: 0.093979999423027
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.attn.q_proj: 0.095499560236931
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.attn.q_proj: 0.095262818038464
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.attn.k_proj: 0.095392063260078
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.attn.k_proj: 0.095359392464161
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.attn.k_proj: 0.095940053462982
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.attn.k_proj: 0.095890298485756
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.attn.v_proj: 0.096610367298126
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.attn.v_proj: 0.095834366977215
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.attn.v_proj: 0.096549026668072
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.attn.v_proj: 0.095930404961109
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.attn.o_proj: 0.096996203064919
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.attn.o_proj: 0.096441082656384
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.attn.o_proj: 0.096836693584919
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.attn.o_proj: 0.096477016806602
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.mlp.c_fc.V: 1.415825366973877
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.mlp.c_fc.U: 3.596897602081299
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.mlp.c_fc.V: 0.923466324806213
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.mlp.c_fc.U: 2.407195091247559
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.mlp.c_fc.V: 0.649173617362976
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.mlp.c_fc.U: 1.526712059974670
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.mlp.c_fc.V: 0.413524925708771
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.mlp.c_fc.U: 0.888019323348999
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.mlp.down_proj.V: 1.526773691177368
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.mlp.down_proj.U: 1.446317434310913
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.mlp.down_proj.V: 0.778590619564056
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.mlp.down_proj.U: 0.740494549274445
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.mlp.down_proj.V: 0.535984456539154
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.mlp.down_proj.U: 0.497253209352493
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.mlp.down_proj.V: 0.508944153785706
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.mlp.down_proj.U: 0.434355825185776
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.q_proj.V: 0.260735183954239
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.q_proj.U: 0.279456675052643
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.q_proj.V: 0.225697919726372
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.q_proj.U: 0.233237668871880
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.q_proj.V: 0.184147357940674
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.q_proj.U: 0.192132458090782
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.q_proj.V: 0.184873670339584
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.q_proj.U: 0.191540241241455
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.k_proj.V: 0.261168509721756
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.k_proj.U: 0.413004040718079
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.k_proj.V: 0.230889737606049
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.k_proj.U: 0.326825737953186
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.k_proj.V: 0.204294517636299
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.k_proj.U: 0.264139592647552
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.k_proj.V: 0.184091627597809
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.k_proj.U: 0.240563303232193
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.v_proj.V: 0.861580789089203
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.v_proj.U: 1.049399375915527
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.v_proj.V: 0.969275414943695
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.v_proj.U: 1.084967374801636
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.v_proj.V: 0.757885396480560
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.v_proj.U: 0.818872749805450
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.v_proj.V: 0.442526072263718
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.v_proj.U: 0.461256027221680
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.o_proj.V: 0.995954155921936
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.o_proj.U: 1.263409733772278
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.o_proj.V: 0.467842549085617
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.o_proj.U: 0.599913120269775
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.o_proj.V: 0.459928691387177
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.o_proj.U: 0.572499155998230
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.o_proj.V: 0.329448997974396
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.o_proj.U: 0.433251380920410
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._input_projector.W: 9.019034385681152
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._input_projector.b: 0.119214452803135
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._output_head.W: 2.372462272644043
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._output_head.b: 0.038158148527145
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.attn.q_proj.weight: 0.014386134222150
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.attn.k_proj.weight: 0.015055373311043
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.attn.v_proj.weight: 0.857696712017059
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.attn.out_proj.weight: 0.857157528400421
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.mlp.0.W: 0.820401370525360
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.mlp.0.b: 0.055862925946712
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.mlp.2.W: 5.222419261932373
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.mlp.2.b: 0.100812651216984
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.attn.q_proj.weight: 0.010587256401777
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.attn.k_proj.weight: 0.010919827036560
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.attn.v_proj.weight: 0.840049266815186
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.attn.out_proj.weight: 0.855723381042480
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.mlp.0.W: 0.814602851867676
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.mlp.0.b: 0.049388367682695
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.mlp.2.W: 4.870763778686523
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.mlp.2.b: 0.090884685516357
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.attn.q_proj.weight: 0.010307282209396
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.attn.k_proj.weight: 0.010417091660202
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.attn.v_proj.weight: 0.817941367626190
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.attn.out_proj.weight: 0.831118583679199
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.mlp.0.W: 0.789799213409424
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.mlp.0.b: 0.046137258410454
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.mlp.2.W: 4.554944038391113
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.mlp.2.b: 0.083076924085617
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.attn.q_proj.weight: 0.008915360085666
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.attn.k_proj.weight: 0.008408028632402
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.attn.v_proj.weight: 0.800793170928955
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.attn.out_proj.weight: 0.802283406257629
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.mlp.0.W: 0.773101210594177
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.mlp.0.b: 0.043396033346653
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.mlp.2.W: 4.283849716186523
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.mlp.2.b: 0.076526202261448
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.attn.q_proj.weight: 0.008458891883492
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.attn.k_proj.weight: 0.007926657795906
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.attn.v_proj.weight: 0.768385231494904
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.attn.out_proj.weight: 0.788568675518036
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.mlp.0.W: 0.755264759063721
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.mlp.0.b: 0.040811493992805
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.mlp.2.W: 4.096728801727295
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.mlp.2.b: 0.071230791509151
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.attn.q_proj.weight: 0.007026274688542
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.attn.k_proj.weight: 0.007082642056048
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.attn.v_proj.weight: 0.735490977764130
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.attn.out_proj.weight: 0.741967976093292
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.mlp.0.W: 0.733805418014526
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.mlp.0.b: 0.038626041263342
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.mlp.2.W: 3.874340534210205
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.mlp.2.b: 0.067627243697643
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.attn.q_proj.weight: 0.007889731787145
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.attn.k_proj.weight: 0.007805154658854
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.attn.v_proj.weight: 0.729653894901276
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.attn.out_proj.weight: 0.690813183784485
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.mlp.0.W: 0.703687131404877
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.mlp.0.b: 0.036594826728106
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.mlp.2.W: 3.713208913803101
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.mlp.2.b: 0.063591144979000
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.attn.q_proj.weight: 0.006506593432277
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.attn.k_proj.weight: 0.006776180118322
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.attn.v_proj.weight: 0.710435986518860
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.attn.out_proj.weight: 0.666987180709839
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.mlp.0.W: 0.676341176033020
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.mlp.0.b: 0.034438244998455
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.mlp.2.W: 3.572649955749512
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.mlp.2.b: 0.060629315674305
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/summary/components: 6.517097473144531
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/summary/ci_fns: 15.809186935424805
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/summary/total: 17.099794387817383
  0% 0/6 [00:00<?, ?it/s]                         train/schedules/lr: 0.000100000000000
  0% 0/6 [00:00<?, ?it/s]                         eval/loss/PGDReconLoss: 88.2333755493164
  0% 0/6 [00:04<?, ?it/s]                         eval/loss/ImportanceMinimalityLoss: 44855.765625
  0% 0/6 [00:04<?, ?it/s]                         eval/loss/StochasticReconSubsetLoss: 3.009490966796875
  0% 0/6 [00:04<?, ?it/s]                         eval/loss/PersistentPGDReconSubsetLoss: 3.521451950073242
  0% 0/6 [00:04<?, ?it/s]                         eval/loss/FaithfulnessLoss: 4.053008524351753e-05
  0% 0/6 [00:04<?, ?it/s] 17% 1/6 [00:05<00:27,  5.52s/it] 33% 2/6 [00:05<00:09,  2.44s/it] 50% 3/6 [00:06<00:04,  1.42s/it] 67% 4/6 [00:06<00:01,  1.06it/s] 83% 5/6 [00:06<00:00,  1.47it/s]Saved model, optimizer, and out_dir to /mnt/polished-lake/artifacts/mechanisms/spd/spd/s-b3bbe4aa
100% 6/6 [00:09<00:00,  1.35s/it]100% 6/6 [00:09<00:00,  1.51s/it]
Finished training loop.
Optimization finished.

>>> 04_pgd_only: 59s [OK] peak_gpu_mem=55461MiB


========================================
Starting test: 05_all_eval_metrics
Time: Mon Feb  9 19:45:17 UTC 2026
Config: /mnt/polished-lake/home/braun/spd/eval_debug/configs/05_all_eval_metrics.yaml
========================================

*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=3, local_rank=3, device=device(type='cuda', index=3)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=0, local_rank=0, device=device(type='cuda', index=0)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=6, local_rank=6, device=device(type='cuda', index=6)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=1, local_rank=1, device=device(type='cuda', index=1)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=4, local_rank=4, device=device(type='cuda', index=4)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=7, local_rank=7, device=device(type='cuda', index=7)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=2, local_rank=2, device=device(type='cuda', index=2)
init_distributed: using backend='nccl'
init_distributed: world_size=8, rank=5, local_rank=5, device=device(type='cuda', index=5)
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 43895
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 43895
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 43895
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 43895
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 43895
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 43895
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 43895
init_distributed: MASTER_ADDR: h200-reserved-145-003.tenant-ac-goodfre-compute.tenant-ac-goodfre.svc.cluster.local, MASTER_PORT: 43895
Distributed state: DistributedState(rank=1, world_size=8, local_rank=1, backend='nccl')
Distributed state: DistributedState(rank=4, world_size=8, local_rank=4, backend='nccl')
Distributed state: DistributedState(rank=3, world_size=8, local_rank=3, backend='nccl')
Distributed state: DistributedState(rank=5, world_size=8, local_rank=5, backend='nccl')
Distributed state: DistributedState(rank=0, world_size=8, local_rank=0, backend='nccl')
Distributed state: DistributedState(rank=6, world_size=8, local_rank=6, backend='nccl')
Distributed state: DistributedState(rank=2, world_size=8, local_rank=2, backend='nccl')
Distributed state: DistributedState(rank=7, world_size=8, local_rank=7, backend='nccl')
Using current branch: dev (uncommitted changes, no commit hash)
Run ID: s-6c2818c0
Output directory: /mnt/polished-lake/artifacts/mechanisms/spd/spd/s-6c2818c0
wandb_project=None wandb_run_name=None wandb_run_name_prefix='' seed=0 autocast_bf16=True n_mask_samples=1 ci_config=GlobalCiConfig(mode='global', fn_type='global_shared_transformer', hidden_dims=None, reader_hidden_dims=None, d_resid_ci_fn=None, block_groups=None, transition_attn_config=None, transition_hidden_dim=None, simple_transformer_ci_cfg=GlobalSharedTransformerCiConfig(d_model=1024, n_blocks=8, mlp_hidden_dim=[8192], attn_config=AttnConfig(n_heads=16, max_len=512, rope_base=10000.0))) sampling='continuous' sigmoid_type='leaky_hard' module_info=[ModulePatternInfoConfig(module_pattern='h.*.mlp.c_fc', C=4096), ModulePatternInfoConfig(module_pattern='h.*.mlp.down_proj', C=3072), ModulePatternInfoConfig(module_pattern='h.*.attn.q_proj', C=512), ModulePatternInfoConfig(module_pattern='h.*.attn.k_proj', C=256), ModulePatternInfoConfig(module_pattern='h.*.attn.v_proj', C=1024), ModulePatternInfoConfig(module_pattern='h.*.attn.o_proj', C=1024)] identity_module_info=None use_delta_component=True loss_metric_configs=[ImportanceMinimalityLossConfig(coeff=0.0005, classname='ImportanceMinimalityLoss', pnorm=2.0, beta=0.1, p_anneal_start_frac=0.0, p_anneal_final_p=0.4, p_anneal_end_frac=1.0, eps=1e-12), StochasticReconSubsetLossConfig(coeff=0.5, classname='StochasticReconSubsetLoss', routing=UniformKSubsetRoutingConfig(type='uniform_k_subset')), PersistentPGDReconSubsetLossConfig(coeff=0.5, classname='PersistentPGDReconSubsetLoss', optimizer=AdamPGDConfig(type='adam', lr=0.1, beta1=0.9, beta2=0.99, eps=1e-08), scope=BatchInvariantScope(type='batch_invariant', n_masks=8), routing=UniformKSubsetRoutingConfig(type='uniform_k_subset')), FaithfulnessLossConfig(coeff=1000000.0, classname='FaithfulnessLoss')] output_loss_type='kl' lr_schedule=ScheduleConfig(start_val=0.0001, warmup_pct=0.0, final_val_frac=0.1, fn_type='cosine') steps=5 batch_size=64 gradient_accumulation_steps=1 grad_clip_norm_components=0.01 grad_clip_norm_ci_fns=None faithfulness_warmup_steps=400 faithfulness_warmup_lr=0.001 faithfulness_warmup_weight_decay=0.0 train_log_freq=200 eval_freq=1000 eval_batch_size=256 slow_eval_freq=10000 n_eval_steps=1 slow_eval_on_first_step=True save_freq=None eval_metric_configs=[ComponentActivationDensityConfig(classname='ComponentActivationDensity'), CI_L0Config(classname='CI_L0', groups={'layer_0': ['h.0.*'], 'layer_1': ['h.1.*'], 'layer_2': ['h.2.*'], 'layer_3': ['h.3.*'], 'total': ['*']}), CEandKLLossesConfig(classname='CEandKLLosses', rounding_threshold=0.0), CIMeanPerComponentConfig(classname='CIMeanPerComponent'), StochasticHiddenActsReconLossConfig(coeff=None, classname='StochasticHiddenActsReconLoss'), PGDReconLossConfig(coeff=None, init='random', step_size=0.1, n_steps=20, mask_scope='shared_across_batch', classname='PGDReconLoss')] ci_alive_threshold=0.0 pretrained_model_class='spd.pretrain.models.llama_simple_mlp.LlamaSimpleMLP' pretrained_model_path=None pretrained_model_name='wandb:goodfire/spd/t-32d1bb3b' pretrained_model_output_attr='idx_0' tokenizer_name='EleutherAI/gpt-neox-20b' task_config=LMTaskConfig(task_name='lm', max_seq_len=512, buffer_size=1000, dataset_name='danbraunai/pile-uncopyrighted-tok', column_name='input_ids', train_data_split='train', eval_data_split='val', shuffle_each_epoch=True, is_tokenized=True, streaming=True)
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: dan-braun (goodfire) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading dataset...
Starting optimization...
Train+eval logs saved to directory: /mnt/polished-lake/artifacts/mechanisms/spd/spd/s-6c2818c0
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Starting faithfulness warmup phase...
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 1 / 400; Faithfulness loss: 0.001661496
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 101 / 400; Faithfulness loss: 0.000194557
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 201 / 400; Faithfulness loss: 0.000089181
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 301 / 400; Faithfulness loss: 0.000055473
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
Faithfulness warmup step 400 / 400; Faithfulness loss: 0.000040628
  0% 0/6 [00:00<?, ?it/s]                         --- Step 0 ---
  0% 0/6 [00:00<?, ?it/s]                         LR: 0.000100
  0% 0/6 [00:00<?, ?it/s]                         train/loss/ImportanceMinimalityLoss: 36238.726562500000000
  0% 0/6 [00:00<?, ?it/s]                         train/loss/StochasticReconSubsetLoss: 2.963917255401611
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss: 4.808569431304932
  0% 0/6 [00:00<?, ?it/s]                         train/loss/FaithfulnessLoss: 0.000040523682401
  0% 0/6 [00:00<?, ?it/s]                         train/loss/total: 62.529289245605469
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.attn.k_proj: 130.421875000000000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.attn.o_proj: 507.907653808593750
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.attn.q_proj: 258.663482666015625
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.attn.v_proj: 505.962799072265625
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.mlp.c_fc: 2047.188476562500000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.0.mlp.down_proj: 1536.454833984375000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.attn.k_proj: 127.785308837890625
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.attn.o_proj: 511.397613525390625
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.attn.q_proj: 267.274322509765625
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.attn.v_proj: 508.254882812500000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.mlp.c_fc: 2051.299804687500000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.1.mlp.down_proj: 1535.686523437500000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.attn.k_proj: 132.868255615234375
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.attn.o_proj: 513.306762695312500
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.attn.q_proj: 243.575714111328125
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.attn.v_proj: 510.212829589843750
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.mlp.c_fc: 2057.168945312500000
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.2.mlp.down_proj: 1548.709106445312500
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.attn.k_proj: 130.091400146484375
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.attn.o_proj: 516.913940429687500
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.attn.q_proj: 259.873718261718750
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.attn.v_proj: 517.953979492187500
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.mlp.c_fc: 2040.286743164062500
  0% 0/6 [00:00<?, ?it/s]                         train/l0/h.3.mlp.down_proj: 1540.889404296875000
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.mlp.c_fc: 0.097107894718647
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.mlp.c_fc: 0.097029037773609
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.mlp.c_fc: 0.096681430935860
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.mlp.c_fc: 0.096843123435974
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.mlp.down_proj: 0.096805348992348
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.mlp.down_proj: 0.096881560981274
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.mlp.down_proj: 0.096580810844898
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.mlp.down_proj: 0.096301153302193
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.attn.q_proj: 0.096449606120586
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.attn.q_proj: 0.093979999423027
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.attn.q_proj: 0.095499560236931
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.attn.q_proj: 0.095262818038464
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.attn.k_proj: 0.095392063260078
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.attn.k_proj: 0.095359392464161
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.attn.k_proj: 0.095940053462982
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.attn.k_proj: 0.095890298485756
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.attn.v_proj: 0.096610367298126
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.attn.v_proj: 0.095834366977215
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.attn.v_proj: 0.096549026668072
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.attn.v_proj: 0.095930404961109
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.0.attn.o_proj: 0.096996203064919
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.1.attn.o_proj: 0.096441082656384
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.2.attn.o_proj: 0.096836693584919
  0% 0/6 [00:00<?, ?it/s]                         train/loss/PersistentPGDReconSubsetLoss/mean_abs_step/h.3.attn.o_proj: 0.096477016806602
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.mlp.c_fc.V: 1.415825366973877
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.mlp.c_fc.U: 3.596897602081299
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.mlp.c_fc.V: 0.923466324806213
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.mlp.c_fc.U: 2.407195091247559
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.mlp.c_fc.V: 0.649173617362976
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.mlp.c_fc.U: 1.526712059974670
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.mlp.c_fc.V: 0.413524925708771
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.mlp.c_fc.U: 0.888019323348999
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.mlp.down_proj.V: 1.526773691177368
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.mlp.down_proj.U: 1.446317434310913
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.mlp.down_proj.V: 0.778590619564056
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.mlp.down_proj.U: 0.740494549274445
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.mlp.down_proj.V: 0.535984456539154
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.mlp.down_proj.U: 0.497253209352493
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.mlp.down_proj.V: 0.508944153785706
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.mlp.down_proj.U: 0.434355825185776
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.q_proj.V: 0.260735183954239
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.q_proj.U: 0.279456675052643
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.q_proj.V: 0.225697919726372
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.q_proj.U: 0.233237668871880
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.q_proj.V: 0.184147357940674
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.q_proj.U: 0.192132458090782
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.q_proj.V: 0.184873670339584
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.q_proj.U: 0.191540241241455
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.k_proj.V: 0.261168509721756
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.k_proj.U: 0.413004040718079
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.k_proj.V: 0.230889737606049
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.k_proj.U: 0.326825737953186
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.k_proj.V: 0.204294517636299
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.k_proj.U: 0.264139592647552
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.k_proj.V: 0.184091627597809
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.k_proj.U: 0.240563303232193
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.v_proj.V: 0.861580789089203
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.v_proj.U: 1.049399375915527
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.v_proj.V: 0.969275414943695
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.v_proj.U: 1.084967374801636
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.v_proj.V: 0.757885396480560
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.v_proj.U: 0.818872749805450
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.v_proj.V: 0.442526072263718
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.v_proj.U: 0.461256027221680
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.o_proj.V: 0.995954155921936
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.0.attn.o_proj.U: 1.263409733772278
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.o_proj.V: 0.467842549085617
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.1.attn.o_proj.U: 0.599913120269775
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.o_proj.V: 0.459928691387177
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.2.attn.o_proj.U: 0.572499155998230
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.o_proj.V: 0.329448997974396
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/components/h.3.attn.o_proj.U: 0.433251380920410
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._input_projector.W: 9.018864631652832
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._input_projector.b: 0.119213633239269
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._output_head.W: 2.372462272644043
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._output_head.b: 0.038158148527145
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.attn.q_proj.weight: 0.014386381022632
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.attn.k_proj.weight: 0.015054779127240
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.attn.v_proj.weight: 0.857683539390564
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.attn.out_proj.weight: 0.857139825820923
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.mlp.0.W: 0.820408403873444
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.mlp.0.b: 0.055863261222839
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.mlp.2.W: 5.222475051879883
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.0.mlp.2.b: 0.100813619792461
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.attn.q_proj.weight: 0.010587137192488
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.attn.k_proj.weight: 0.010920027270913
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.attn.v_proj.weight: 0.840052485466003
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.attn.out_proj.weight: 0.855735659599304
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.mlp.0.W: 0.814608752727509
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.mlp.0.b: 0.049388617277145
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.mlp.2.W: 4.870766162872314
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.1.mlp.2.b: 0.090884506702423
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.attn.q_proj.weight: 0.010307569988072
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.attn.k_proj.weight: 0.010417462326586
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.attn.v_proj.weight: 0.817948222160339
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.attn.out_proj.weight: 0.831114053726196
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.mlp.0.W: 0.789796411991119
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.mlp.0.b: 0.046137224882841
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.mlp.2.W: 4.554920196533203
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.2.mlp.2.b: 0.083076745271683
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.attn.q_proj.weight: 0.008915863931179
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.attn.k_proj.weight: 0.008408299647272
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.attn.v_proj.weight: 0.800786077976227
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.attn.out_proj.weight: 0.802274048328400
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.mlp.0.W: 0.773098945617676
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.mlp.0.b: 0.043395802378654
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.mlp.2.W: 4.283839225769043
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.3.mlp.2.b: 0.076525911688805
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.attn.q_proj.weight: 0.008458628319204
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.attn.k_proj.weight: 0.007926503196359
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.attn.v_proj.weight: 0.768374741077423
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.attn.out_proj.weight: 0.788562178611755
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.mlp.0.W: 0.755264818668365
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.mlp.0.b: 0.040811486542225
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.mlp.2.W: 4.096728324890137
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.4.mlp.2.b: 0.071230649948120
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.attn.q_proj.weight: 0.007026254665107
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.attn.k_proj.weight: 0.007082557305694
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.attn.v_proj.weight: 0.735490083694458
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.attn.out_proj.weight: 0.741965591907501
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.mlp.0.W: 0.733805358409882
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.mlp.0.b: 0.038626048713923
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.mlp.2.W: 3.874341726303101
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.5.mlp.2.b: 0.067627243697643
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.attn.q_proj.weight: 0.007889724336565
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.attn.k_proj.weight: 0.007805151399225
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.attn.v_proj.weight: 0.729653894901276
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.attn.out_proj.weight: 0.690813183784485
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.mlp.0.W: 0.703687131404877
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.mlp.0.b: 0.036594826728106
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.mlp.2.W: 3.713208913803101
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.6.mlp.2.b: 0.063591144979000
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.attn.q_proj.weight: 0.006506593432277
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.attn.k_proj.weight: 0.006776180118322
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.attn.v_proj.weight: 0.710435986518860
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.attn.out_proj.weight: 0.666987180709839
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.mlp.0.W: 0.676341176033020
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.mlp.0.b: 0.034438244998455
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.mlp.2.W: 3.572649955749512
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/ci_fns/_global_ci_fn._blocks.7.mlp.2.b: 0.060629315674305
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/summary/components: 6.517097473144531
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/summary/ci_fns: 15.809098243713379
  0% 0/6 [00:00<?, ?it/s]                         train/grad_norms/summary/total: 17.099712371826172
  0% 0/6 [00:00<?, ?it/s]                         train/schedules/lr: 0.000100000000000
  0% 0/6 [00:00<?, ?it/s]                         eval/figures/component_activation_density: <PIL.Image.Image image mode=RGB size=1989x2990 at 0x7454F4129F30>
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/0.0_h.0.attn.k_proj: 130.44708251953125
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/0.0_h.0.attn.o_proj: 507.10870361328125
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/0.0_h.0.attn.q_proj: 258.8153076171875
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/0.0_h.0.attn.v_proj: 506.0845031738281
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/0.0_h.0.mlp.c_fc: 2047.969482421875
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/0.0_h.0.mlp.down_proj: 1537.3848876953125
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/0.0_h.1.attn.k_proj: 127.92353820800781
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/0.0_h.1.attn.o_proj: 511.77154541015625
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/0.0_h.1.attn.q_proj: 267.328857421875
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/0.0_h.1.attn.v_proj: 508.5190124511719
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/0.0_h.1.mlp.c_fc: 2053.302001953125
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/0.0_h.1.mlp.down_proj: 1536.3486328125
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/0.0_h.2.attn.k_proj: 132.82232666015625
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/0.0_h.2.attn.o_proj: 512.3269653320312
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/0.0_h.2.attn.q_proj: 243.21173095703125
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/0.0_h.2.attn.v_proj: 510.0046691894531
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/0.0_h.2.mlp.c_fc: 2057.486083984375
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/0.0_h.2.mlp.down_proj: 1549.276123046875
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/0.0_h.3.attn.k_proj: 129.75634765625
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/0.0_h.3.attn.o_proj: 516.0956420898438
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/0.0_h.3.attn.q_proj: 259.73065185546875
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/0.0_h.3.attn.v_proj: 518.4866333007812
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/0.0_h.3.mlp.c_fc: 2039.227783203125
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/0.0_h.3.mlp.down_proj: 1540.16943359375
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/0.0_layer_0: 4987.81005859375
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/0.0_total: 20001.59765625
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/0.0_layer_1: 5005.19384765625
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/0.0_layer_2: 5005.1279296875
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/0.0_layer_3: 5003.46630859375
  0% 0/6 [00:15<?, ?it/s]                         eval/l0/bar_chart: CustomChart(table=<wandb.sdk.data_types.table.Table object at 0x7455435e0c20>, spec=CustomChartSpec(spec_name='wandb/bar/v0', fields={'label': 'layer', 'value': 'l0'}, string_fields={'title': 'L0_0.0'}, key='', panel_type='Vega2', split_table=False))
  0% 0/6 [00:15<?, ?it/s]                         eval/ce_kl/kl_ci_masked: 9.495499610900879
  0% 0/6 [00:15<?, ?it/s]                         eval/ce_kl/kl_unmasked: 0.8671587109565735
  0% 0/6 [00:15<?, ?it/s]                         eval/ce_kl/kl_stoch_masked: 4.900856971740723
  0% 0/6 [00:15<?, ?it/s]                         eval/ce_kl/kl_random_masked: 6.479849338531494
  0% 0/6 [00:15<?, ?it/s]                         eval/ce_kl/kl_rounded_masked: 9.209177017211914
  0% 0/6 [00:15<?, ?it/s]                         eval/ce_kl/kl_zero_masked: 67.24137878417969
  0% 0/6 [00:15<?, ?it/s]                         eval/ce_kl/ce_difference_ci_masked: 9.524429321289062
  0% 0/6 [00:15<?, ?it/s]                         eval/ce_kl/ce_difference_unmasked: 0.8797334432601929
  0% 0/6 [00:15<?, ?it/s]                         eval/ce_kl/ce_difference_stoch_masked: 4.9171037673950195
  0% 0/6 [00:15<?, ?it/s]                         eval/ce_kl/ce_difference_random_masked: 6.505438804626465
  0% 0/6 [00:15<?, ?it/s]                         eval/ce_kl/ce_difference_rounded_masked: 9.234663009643555
  0% 0/6 [00:15<?, ?it/s]                         eval/ce_kl/ce_unrecovered_ci_masked: 0.14157015085220337
  0% 0/6 [00:15<?, ?it/s]                         eval/ce_kl/ce_unrecovered_unmasked: 0.013070366345345974
  0% 0/6 [00:15<?, ?it/s]                         eval/ce_kl/ce_unrecovered_stoch_masked: 0.07307715713977814
  0% 0/6 [00:15<?, ?it/s]                         eval/ce_kl/ce_unrecovered_random_masked: 0.0966954380273819
  0% 0/6 [00:15<?, ?it/s]                         eval/ce_kl/ce_unrecovered_rounded_masked: 0.13726381957530975
  0% 0/6 [00:15<?, ?it/s]                         eval/figures/ci_mean_per_component: <PIL.Image.Image image mode=RGB size=6380x3579 at 0x7454F40AF350>
  0% 0/6 [00:15<?, ?it/s]                         eval/figures/ci_mean_per_component_log: <PIL.Image.Image image mode=RGB size=6380x3579 at 0x74554352E120>
  0% 0/6 [00:15<?, ?it/s]                         eval/loss/StochasticHiddenActsReconLoss: 0.562035322189331
  0% 0/6 [00:15<?, ?it/s]                         eval/loss/PGDReconLoss: 89.03186798095703
  0% 0/6 [00:15<?, ?it/s]                         eval/loss/ImportanceMinimalityLoss: 44855.765625
  0% 0/6 [00:15<?, ?it/s]                         eval/loss/StochasticReconSubsetLoss: 3.0072948932647705
  0% 0/6 [00:15<?, ?it/s]                         eval/loss/PersistentPGDReconSubsetLoss: 3.5261943340301514
  0% 0/6 [00:15<?, ?it/s]                         eval/loss/FaithfulnessLoss: 4.053008524351753e-05
  0% 0/6 [00:15<?, ?it/s]                         Saved figure figures/component_activation_density to /mnt/polished-lake/artifacts/mechanisms/spd/spd/s-6c2818c0/figures/figures_component_activation_density_0.png
  0% 0/6 [00:16<?, ?it/s]                         Saved custom chart data l0/bar_chart to /mnt/polished-lake/artifacts/mechanisms/spd/spd/s-6c2818c0/figures/l0_bar_chart_0.json
  0% 0/6 [00:16<?, ?it/s]                         Saved figure figures/ci_mean_per_component to /mnt/polished-lake/artifacts/mechanisms/spd/spd/s-6c2818c0/figures/figures_ci_mean_per_component_0.png
  0% 0/6 [00:16<?, ?it/s]                         Saved figure figures/ci_mean_per_component_log to /mnt/polished-lake/artifacts/mechanisms/spd/spd/s-6c2818c0/figures/figures_ci_mean_per_component_log_0.png
  0% 0/6 [00:17<?, ?it/s] 17% 1/6 [00:17<01:28, 17.60s/it] 33% 2/6 [00:17<00:29,  7.42s/it] 50% 3/6 [00:18<00:12,  4.13s/it] 67% 4/6 [00:18<00:05,  2.59s/it] 83% 5/6 [00:18<00:01,  1.73s/it]Saved model, optimizer, and out_dir to /mnt/polished-lake/artifacts/mechanisms/spd/spd/s-6c2818c0
100% 6/6 [00:21<00:00,  2.08s/it]100% 6/6 [00:21<00:00,  3.55s/it]
Finished training loop.
Optimization finished.

>>> 05_all_eval_metrics: 65s [OK] peak_gpu_mem=65541MiB


========================================
SUMMARY
========================================
  00_no_eval_metrics                 73s  [OK]
  01_cheap_only                      60s  [OK]
  02_cekl_only                       52s  [OK]
  03_stoch_hidden_only               50s  [OK]
  04_pgd_only                        59s  [OK]
  05_all_eval_metrics                65s  [OK]
========================================
Done at Mon Feb  9 19:46:22 UTC 2026

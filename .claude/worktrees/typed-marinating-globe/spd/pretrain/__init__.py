"""Pretraining infrastructure for language models.

This module provides training scripts and model definitions for pretraining
language models that can later be decomposed using SPD.

Usage:
    # Submit training job to SLURM
    spd-pretrain --config_path spd/pretrain/configs/pile_llama_simple_mlp_4L.yaml

    # Run locally
    spd-pretrain --config_path ... --local

    # Multi-GPU training
    spd-pretrain --config_path ... --n_gpus 4

Available model types:
    - GPT2: Full GPT-2 implementation
    - GPT2Simple: Simplified GPT-2 (fewer optimizations)
    - Llama: Full Llama implementation
    - LlamaSimple: Simplified Llama (no QKV projection merging)
    - LlamaSimpleMLP: Llama with MLP-only architecture (for decomposition)

Output directory: SPD_OUT_DIR/target_models/
"""

from spd.pretrain.models import MODEL_CLASSES, ModelConfig
from spd.pretrain.run_info import PretrainRunInfo

__all__ = ["PretrainRunInfo", "MODEL_CLASSES", "ModelConfig"]

# Quick test of parallel LLM pipeline
config:
  model_name: "meta-llama/Llama-3.1-8B-Instruct"
  llm_model: "gpt-5.2"
  contrastive_tokens: ["Yes", "No"]
  tau: 0.02
  functional_split: true
  use_llm_split: true
  use_llm_reassignment: true
  parallel_llm: true
  output_dir: "outputs/reasoning_test/"

sequences:
  - prompt: "Alice is taller than Bob. Bob is taller than Carol. Is Alice taller than Carol? Yes or No."
  - prompt: "Dog A is faster than Dog B. Dog B is faster than Dog C. Is Dog C faster than Dog A? Yes or No."

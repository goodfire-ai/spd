"""Shared test fixtures for loss function tests."""

from typing import override

import torch
import torch.nn as nn
from jaxtyping import Float
from torch import Tensor

from spd.models.component_model import ComponentModel


class OneLayerLinearModel(nn.Module):
    """One-layer linear model for testing."""

    def __init__(self, d_in: int, d_out: int) -> None:
        super().__init__()
        self.fc = nn.Linear(d_in, d_out, bias=False)

    @override
    def forward(self, x: Tensor) -> Tensor:
        return self.fc(x)


class TwoLayerLinearModel(nn.Module):
    """Two-layer linear model for testing."""

    def __init__(self, d_in: int, d_hidden: int, d_out: int) -> None:
        super().__init__()
        self.fc1 = nn.Linear(d_in, d_hidden, bias=False)
        self.fc2 = nn.Linear(d_hidden, d_out, bias=False)

    @override
    def forward(self, x: Tensor) -> Tensor:
        x = self.fc1(x)
        x = self.fc2(x)
        return x


def make_one_layer_component_model(weight: Float[Tensor, " d_out d_in"]) -> ComponentModel:
    """Create a ComponentModel with a single linear layer for testing.

    Args:
        weight: Weight matrix for the linear layer

    Returns:
        ComponentModel wrapping the OneLayerLinearModel
    """
    d_out, d_in = weight.shape
    target = OneLayerLinearModel(d_in=d_in, d_out=d_out)
    with torch.no_grad():
        target.fc.weight.copy_(weight)
    target.requires_grad_(False)

    comp_model = ComponentModel(
        target_model=target,
        target_module_patterns=["fc"],
        C=1,
        ci_fn_hidden_dims=[2],
        ci_fn_type="mlp",
        pretrained_model_output_attr=None,
    )

    return comp_model


def make_two_layer_component_model(
    weight1: Float[Tensor, " d_hidden d_in"], weight2: Float[Tensor, " d_out d_hidden"]
) -> ComponentModel:
    """Create a ComponentModel with two linear layers for testing.

    Args:
        weight1: Weight matrix for the first linear layer
        weight2: Weight matrix for the second linear layer

    Returns:
        ComponentModel wrapping the TwoLayerLinearModel
    """
    d_hidden, d_in = weight1.shape
    d_out, d_hidden2 = weight2.shape
    assert d_hidden == d_hidden2, "Hidden dimensions must match"

    target = TwoLayerLinearModel(d_in=d_in, d_hidden=d_hidden, d_out=d_out)
    with torch.no_grad():
        target.fc1.weight.copy_(weight1)
        target.fc2.weight.copy_(weight2)
    target.requires_grad_(False)

    comp_model = ComponentModel(
        target_model=target,
        target_module_patterns=["fc1", "fc2"],
        C=1,
        ci_fn_hidden_dims=[2],
        ci_fn_type="mlp",
        pretrained_model_output_attr=None,
    )

    return comp_model
